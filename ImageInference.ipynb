{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "IYMwJOUjO7e3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **NEW**"
      ],
      "metadata": {
        "id": "4kF1dP1wrbsP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, os, cv2, numpy as np, matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "from scipy import ndimage\n",
        "from google.colab import files\n",
        "import time\n",
        "\n",
        "# Install required libraries\n",
        "try:\n",
        "    from segment_anything import sam_model_registry, SamPredictor\n",
        "except ModuleNotFoundError:\n",
        "    !pip install git+https://github.com/facebookresearch/segment-anything.git\n",
        "    from segment_anything import sam_model_registry, SamPredictor\n",
        "\n",
        "# Install YOLO if not available\n",
        "try:\n",
        "    from ultralytics import YOLO\n",
        "except ModuleNotFoundError:\n",
        "    !pip install ultralytics\n",
        "    from ultralytics import YOLO\n",
        "\n",
        "# Download SAM model if needed\n",
        "sam_checkpoint = \"sam_vit_h_4b8939.pth\"\n",
        "if not os.path.exists(sam_checkpoint):\n",
        "    !wget -O sam_vit_h_4b8939.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\n",
        "\n",
        "# Setup device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_models():\n",
        "    # Load SAM model\n",
        "    sam = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
        "    sam.to(device)\n",
        "    sam_predictor = SamPredictor(sam)\n",
        "\n",
        "    # Load MiDaS depth model\n",
        "    midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Large\")\n",
        "    midas.to(device)\n",
        "    midas.eval()\n",
        "\n",
        "    # Load YOLO model for object detection\n",
        "    yolo = YOLO(\"yolov8x.pt\")  # Load the largest YOLOv8 model\n",
        "\n",
        "    return sam_predictor, midas, yolo\n",
        "\n",
        "def preprocess_image(image_path):\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "    img_array = np.array(img)\n",
        "\n",
        "    # Resize keeping aspect ratio\n",
        "    width, height = img.size\n",
        "    ratio = min(520 / width, 520 / height)\n",
        "    new_size = (int(width * ratio), int(height * ratio))\n",
        "    resized_img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "    # Process for MiDaS\n",
        "    midas_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Resize((384, 384), antialias=True),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "    ])\n",
        "    midas_input = midas_transform(img).unsqueeze(0).to(device)\n",
        "\n",
        "    return {\n",
        "        'original': img, 'array': img_array,\n",
        "        'resized': resized_img, 'resized_array': np.array(resized_img),\n",
        "        'midas_input': midas_input\n",
        "    }\n",
        "\n",
        "def detect_sports_objects(yolo, img_data):\n",
        "    \"\"\"Detect sports-related objects using YOLO\"\"\"\n",
        "    results = yolo(img_data['resized_array'], conf=0.25)\n",
        "\n",
        "    # Extract detections\n",
        "    boxes = []\n",
        "    classes = []\n",
        "    scores = []\n",
        "    sports_classes = ['person', 'sports ball', 'tennis racket', 'baseball bat', 'baseball glove',\n",
        "                      'skateboard', 'surfboard', 'tennis ball', 'bottle', 'wine glass', 'cup',\n",
        "                      'frisbee', 'skis', 'snowboard', 'kite']\n",
        "\n",
        "    result = results[0]  # First image result\n",
        "\n",
        "    detections = {\n",
        "        'boxes': [],\n",
        "        'classes': [],\n",
        "        'scores': [],\n",
        "        'sports_objects': 0,\n",
        "        'athletes': 0\n",
        "    }\n",
        "\n",
        "    if hasattr(result, 'boxes') and len(result.boxes) > 0:\n",
        "        for box in result.boxes:\n",
        "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "            conf = float(box.conf[0])\n",
        "            cls = int(box.cls[0])\n",
        "            class_name = result.names[cls]\n",
        "\n",
        "            detections['boxes'].append([x1, y1, x2, y2])\n",
        "            detections['classes'].append(class_name)\n",
        "            detections['scores'].append(conf)\n",
        "\n",
        "            if class_name == 'person':\n",
        "                detections['athletes'] += 1\n",
        "            if class_name in sports_classes:\n",
        "                detections['sports_objects'] += 1\n",
        "\n",
        "    return detections\n",
        "\n",
        "def generate_depth_map(midas, img_data):\n",
        "    with torch.no_grad():\n",
        "        depth_map = midas(img_data['midas_input'])\n",
        "        depth_map = torch.nn.functional.interpolate(\n",
        "            depth_map.unsqueeze(1),\n",
        "            size=img_data['array'].shape[:2],\n",
        "            mode=\"bicubic\",\n",
        "            align_corners=False\n",
        "        ).squeeze().cpu().numpy()\n",
        "\n",
        "    normalized_depth = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min() + 1e-5)\n",
        "\n",
        "    depth_8bit = (normalized_depth * 255).astype(np.uint8)\n",
        "    _, depth_mask = cv2.threshold(depth_8bit, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
        "\n",
        "    kernel = np.ones((5, 5), np.uint8)\n",
        "    depth_mask = cv2.morphologyEx(depth_mask, cv2.MORPH_OPEN, kernel)\n",
        "\n",
        "    contours, _ = cv2.findContours(depth_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
        "    main_contour = max(contours, key=cv2.contourArea) if contours else None\n",
        "\n",
        "    return normalized_depth, depth_mask, main_contour\n",
        "\n",
        "def segment_with_sam(sam_predictor, img_data):\n",
        "    sam_predictor.set_image(img_data['resized_array'])\n",
        "\n",
        "    height, width = img_data['resized_array'].shape[:2]\n",
        "    points = []\n",
        "    grid_size = 3\n",
        "\n",
        "    for i in range(1, grid_size + 1):\n",
        "        for j in range(1, grid_size + 1):\n",
        "            points.append([width // (grid_size + 1) * i, height // (grid_size + 1) * j])\n",
        "    points.append([width // 2, height // 2])\n",
        "\n",
        "    input_points = np.array(points)\n",
        "    input_labels = np.ones(len(input_points), dtype=int)\n",
        "\n",
        "    masks, scores, _ = sam_predictor.predict(\n",
        "        point_coords=input_points,\n",
        "        point_labels=input_labels,\n",
        "        multimask_output=True\n",
        "    )\n",
        "\n",
        "    best_mask = masks[np.argmax(scores)].astype(np.uint8)\n",
        "    if np.mean(best_mask) > 0.5:\n",
        "        best_mask = 1 - best_mask\n",
        "\n",
        "    labeled, num = ndimage.label(best_mask)\n",
        "    if num > 1:\n",
        "        sizes = ndimage.sum(best_mask, labeled, range(1, num + 1))\n",
        "        best_mask = (labeled == np.argmax(sizes) + 1).astype(np.uint8)\n",
        "\n",
        "    color_mask = np.zeros((best_mask.shape[0], best_mask.shape[1], 3), dtype=np.uint8)\n",
        "    color_mask[best_mask == 1] = [0, 255, 0]\n",
        "\n",
        "    return best_mask, color_mask\n",
        "\n",
        "def analyze_sports_scene(detections, depth_map, img_data):\n",
        "    \"\"\"Analyze the sports scene based on detected objects and depth\"\"\"\n",
        "    height, width = depth_map.shape[:2]  # Lấy kích thước từ depth_map thay vì img_data\n",
        "\n",
        "    # Analyze player distribution\n",
        "    player_positions = []\n",
        "    for i, cls in enumerate(detections['classes']):\n",
        "        if cls == 'person':\n",
        "            x1, y1, x2, y2 = detections['boxes'][i]\n",
        "            # Chuyển đổi tọa độ hộp giới hạn để phù hợp với kích thước depth_map\n",
        "            x1 = int(x1 * depth_map.shape[1] / img_data['resized_array'].shape[1])\n",
        "            y1 = int(y1 * depth_map.shape[0] / img_data['resized_array'].shape[0])\n",
        "            x2 = int(x2 * depth_map.shape[1] / img_data['resized_array'].shape[1])\n",
        "            y2 = int(y2 * depth_map.shape[0] / img_data['resized_array'].shape[0])\n",
        "\n",
        "            center_x = (x1 + x2) / 2 / width\n",
        "            center_y = (y1 + y2) / 2 / height\n",
        "            player_positions.append((center_x, center_y))\n",
        "\n",
        "    # Khởi tạo player_dispersion với giá trị mặc định\n",
        "    player_dispersion = 0\n",
        "\n",
        "    # Calculate player dispersion (if multiple players)\n",
        "    if len(player_positions) > 1:\n",
        "        # Calculate average pairwise distance\n",
        "        total_distance = 0\n",
        "        count = 0\n",
        "        for i in range(len(player_positions)):\n",
        "            for j in range(i+1, len(player_positions)):\n",
        "                p1 = player_positions[i]\n",
        "                p2 = player_positions[j]\n",
        "                dist = np.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
        "                total_distance += dist\n",
        "                count += 1\n",
        "        if count > 0:\n",
        "            player_dispersion = total_distance / count\n",
        "\n",
        "    # Identify key subjects (based on size and position)\n",
        "    key_subjects = []\n",
        "    for i, box in enumerate(detections['boxes']):\n",
        "        x1, y1, x2, y2 = box\n",
        "        area = (x2-x1) * (y2-y1)\n",
        "        area_ratio = area / (img_data['resized_array'].shape[1] * img_data['resized_array'].shape[0])\n",
        "\n",
        "        # Chuyển đổi tọa độ để phù hợp với depth_map\n",
        "        depth_x1 = int(x1 * depth_map.shape[1] / img_data['resized_array'].shape[1])\n",
        "        depth_y1 = int(y1 * depth_map.shape[0] / img_data['resized_array'].shape[0])\n",
        "        depth_x2 = int(x2 * depth_map.shape[1] / img_data['resized_array'].shape[1])\n",
        "        depth_y2 = int(y2 * depth_map.shape[0] / img_data['resized_array'].shape[0])\n",
        "\n",
        "        # Đảm bảo các tọa độ nằm trong giới hạn của depth_map\n",
        "        depth_x1 = max(0, min(depth_x1, depth_map.shape[1]-1))\n",
        "        depth_y1 = max(0, min(depth_y1, depth_map.shape[0]-1))\n",
        "        depth_x2 = max(0, min(depth_x2, depth_map.shape[1]-1))\n",
        "        depth_y2 = max(0, min(depth_y2, depth_map.shape[0]-1))\n",
        "\n",
        "        # Tạo mask với kích thước của depth_map\n",
        "        mask = np.zeros((depth_map.shape[0], depth_map.shape[1]), dtype=np.uint8)\n",
        "        if depth_y2 > depth_y1 and depth_x2 > depth_x1:  # Đảm bảo hộp có kích thước hợp lệ\n",
        "            mask[depth_y1:depth_y2, depth_x1:depth_x2] = 1\n",
        "\n",
        "        # Tính toán độ sâu trung bình\n",
        "        obj_depth = np.mean(depth_map[mask > 0]) if np.sum(mask) > 0 else 0\n",
        "\n",
        "        subject_info = {\n",
        "            'class': detections['classes'][i],\n",
        "            'box': box,\n",
        "            'area_ratio': area_ratio,\n",
        "            'depth': obj_depth,\n",
        "            'position': ((x1+x2)/2/img_data['resized_array'].shape[1],\n",
        "                         (y1+y2)/2/img_data['resized_array'].shape[0])\n",
        "        }\n",
        "\n",
        "        # Add prominence score based on size and position\n",
        "        center_dist = np.sqrt((subject_info['position'][0]-0.5)**2 +\n",
        "                             (subject_info['position'][1]-0.5)**2)\n",
        "        subject_info['prominence'] = area_ratio * (1 - center_dist)\n",
        "\n",
        "        key_subjects.append(subject_info)\n",
        "\n",
        "    # Sort by prominence\n",
        "    key_subjects.sort(key=lambda x: x['prominence'], reverse=True)\n",
        "\n",
        "    return {\n",
        "        'player_count': detections['athletes'],\n",
        "        'player_positions': player_positions,\n",
        "        'player_dispersion': player_dispersion,\n",
        "        'key_subjects': key_subjects[:5] if key_subjects else []\n",
        "    }\n",
        "\n",
        "def analyze_action_quality(detections, img_data):\n",
        "    \"\"\"Phân tích chất lượng hành động trong ảnh thể thao với cách tiếp cận cải tiến\"\"\"\n",
        "    height, width = img_data['resized_array'].shape[:2]\n",
        "\n",
        "    # 1. Kiểm tra thiết bị thể thao (giữ nguyên như cũ)\n",
        "    has_equipment = False\n",
        "    equipment_types = []\n",
        "    for cls in detections['classes']:\n",
        "        if cls in ['sports ball', 'tennis racket', 'baseball bat', 'baseball glove',\n",
        "                  'skateboard', 'surfboard', 'tennis ball', 'frisbee', 'skis', 'snowboard']:\n",
        "            has_equipment = True\n",
        "            if cls not in equipment_types:\n",
        "                equipment_types.append(cls)\n",
        "\n",
        "    # 2. Phân tích tư thế cá nhân thay vì so sánh giữa nhiều người\n",
        "    action_posture_score = 0\n",
        "    dynamic_posture_count = 0\n",
        "    total_players = 0\n",
        "\n",
        "    for i, cls in enumerate(detections['classes']):\n",
        "        if cls == 'person':\n",
        "            total_players += 1\n",
        "            x1, y1, x2, y2 = detections['boxes'][i]\n",
        "\n",
        "            # a. Tính tỷ lệ chiều cao/chiều rộng\n",
        "            aspect_ratio = (y2-y1)/(x2-x1) if (x2-x1) > 0 else 0\n",
        "\n",
        "            # b. Đánh giá tư thế dựa trên tỷ lệ khung hình\n",
        "            # Tư thế không điển hình (có thể đang nhảy, cúi, nằm...)\n",
        "            if aspect_ratio < 1.2 or aspect_ratio > 2.5:\n",
        "                dynamic_posture_count += 1\n",
        "\n",
        "            # c. Tính diện tích tương đối (lớn = hành động gần hơn)\n",
        "            area_ratio = ((y2-y1) * (x2-x1)) / (height * width)\n",
        "            if area_ratio > 0.2:  # Vận động viên chiếm diện tích lớn, thường là hành động gần\n",
        "                action_posture_score += 0.2\n",
        "\n",
        "    # Nếu có người trong tư thế không điển hình, đó có thể là hành động năng động\n",
        "    if total_players > 0:\n",
        "        dynamic_posture_ratio = dynamic_posture_count / total_players\n",
        "        action_posture_score += dynamic_posture_ratio * 0.5\n",
        "\n",
        "    # 3. Tính action_level cải tiến\n",
        "    action_level = 0\n",
        "\n",
        "    # Nếu có thiết bị thể thao (giữ nguyên)\n",
        "    if has_equipment:\n",
        "        action_level += 0.4\n",
        "\n",
        "    # Thêm điểm từ phân tích tư thế\n",
        "    action_level += min(0.6, action_posture_score)\n",
        "\n",
        "    # 4. Phân loại chất lượng hành động\n",
        "    return {\n",
        "        'has_equipment': has_equipment,\n",
        "        'equipment_types': equipment_types,\n",
        "        'dynamic_posture_score': action_posture_score,\n",
        "        'dynamic_posture_count': dynamic_posture_count,\n",
        "        'total_players': total_players,\n",
        "        'action_level': action_level,\n",
        "        'action_quality': \"High\" if action_level > 0.7 else\n",
        "                         \"Medium\" if action_level > 0.3 else \"Low\"\n",
        "    }\n",
        "\n",
        "def analyze_sports_composition(detections, analysis, img_data):\n",
        "    \"\"\"Analyze the composition with sports-specific context\"\"\"\n",
        "\n",
        "    # Basic composition from existing analysis\n",
        "    composition = analysis[\"composition_analysis\"] if \"composition_analysis\" in analysis else {}\n",
        "\n",
        "    # Sports specific enhancements\n",
        "    result = {\n",
        "        'sport_type': 'Unknown',\n",
        "        'framing_quality': 'Unknown',\n",
        "        'recommended_crop': None,\n",
        "        'action_focus': 'Unknown'\n",
        "    }\n",
        "\n",
        "    # Try to determine sport type\n",
        "    sport_equipment = {\n",
        "        'tennis racket': 'Tennis',\n",
        "        'tennis ball': 'Tennis',\n",
        "        'sports ball': 'Ball Sport',\n",
        "        'baseball bat': 'Baseball',\n",
        "        'baseball glove': 'Baseball',\n",
        "        'skateboard': 'Skateboarding',\n",
        "        'surfboard': 'Surfing',\n",
        "        'frisbee': 'Frisbee',\n",
        "        'skis': 'Skiing',\n",
        "        'snowboard': 'Snowboarding'\n",
        "    }\n",
        "\n",
        "    for cls in detections['classes']:\n",
        "        if cls in sport_equipment:\n",
        "            result['sport_type'] = sport_equipment[cls]\n",
        "            break\n",
        "\n",
        "    # Evaluate framing quality for sports action\n",
        "    if \"key_subjects\" in analysis and analysis['key_subjects']:\n",
        "        subject_positions = [subject['position'] for subject in analysis['key_subjects']]\n",
        "\n",
        "        # Check if key subjects are well placed (rule of thirds or centered)\n",
        "        well_placed_count = 0\n",
        "        for pos in subject_positions:\n",
        "            # Check rule of thirds points\n",
        "            thirds_points = [\n",
        "                (1/3, 1/3), (2/3, 1/3),\n",
        "                (1/3, 2/3), (2/3, 2/3)\n",
        "            ]\n",
        "\n",
        "            center_point = (0.5, 0.5)\n",
        "\n",
        "            # Check if close to rule of thirds points or center\n",
        "            for third in thirds_points:\n",
        "                dist = np.sqrt((pos[0]-third[0])**2 + (pos[1]-third[1])**2)\n",
        "                if dist < 0.1:  # 10% of image width/height\n",
        "                    well_placed_count += 1\n",
        "                    break\n",
        "\n",
        "            # Check if centered\n",
        "            dist_to_center = np.sqrt((pos[0]-center_point[0])**2 + (pos[1]-center_point[1])**2)\n",
        "            if dist_to_center < 0.1:\n",
        "                well_placed_count += 1\n",
        "\n",
        "        if well_placed_count / len(subject_positions) > 0.7:\n",
        "            result['framing_quality'] = 'Excellent'\n",
        "        elif well_placed_count / len(subject_positions) > 0.4:\n",
        "            result['framing_quality'] = 'Good'\n",
        "        else:\n",
        "            result['framing_quality'] = 'Could be improved'\n",
        "\n",
        "    # Recommend crop if needed\n",
        "    if \"key_subjects\" in analysis and analysis['key_subjects']:\n",
        "        main_subject = analysis['key_subjects'][0]\n",
        "        x_pos = main_subject['position'][0]\n",
        "        y_pos = main_subject['position'][1]\n",
        "\n",
        "        # If subject is too far from ideal positions, suggest crop\n",
        "        if not (0.3 < x_pos < 0.7 or 0.3 < y_pos < 0.7):\n",
        "            # Calculate ideal center point\n",
        "            if x_pos < 0.33:\n",
        "                ideal_x = 0.33\n",
        "            elif x_pos > 0.67:\n",
        "                ideal_x = 0.67\n",
        "            else:\n",
        "                ideal_x = 0.5\n",
        "\n",
        "            if y_pos < 0.33:\n",
        "                ideal_y = 0.33\n",
        "            elif y_pos > 0.67:\n",
        "                ideal_y = 0.67\n",
        "            else:\n",
        "                ideal_y = 0.5\n",
        "\n",
        "            # Calculate shift needed\n",
        "            shift_x = ideal_x - x_pos\n",
        "            shift_y = ideal_y - y_pos\n",
        "\n",
        "            result['recommended_crop'] = {\n",
        "                'shift_x': shift_x,\n",
        "                'shift_y': shift_y\n",
        "            }\n",
        "\n",
        "    # Evaluate action focus\n",
        "    if \"action_quality\" in analysis:\n",
        "        result['action_focus'] = analysis['action_quality']\n",
        "\n",
        "    return result\n",
        "\n",
        "def analyze_facial_expression(detections, img_data):\n",
        "    \"\"\"Phân tích biểu cảm khuôn mặt trong ảnh thể thao\"\"\"\n",
        "    try:\n",
        "        # Import thư viện phân tích khuôn mặt\n",
        "        from deepface import DeepFace\n",
        "        import tensorflow as tf\n",
        "    except ModuleNotFoundError:\n",
        "        !pip install deepface\n",
        "        from deepface import DeepFace\n",
        "\n",
        "    image = img_data['resized_array']\n",
        "\n",
        "    # Kết quả phân tích biểu cảm\n",
        "    expression_results = {\n",
        "        'has_faces': False,\n",
        "        'expressions': [],\n",
        "        'dominant_emotion': 'unknown',\n",
        "        'emotion_intensity': 0,\n",
        "        'emotional_value': 'Low'\n",
        "    }\n",
        "\n",
        "    # Phát hiện khuôn mặt từ hộp giới hạn người\n",
        "    faces_detected = 0\n",
        "    face_regions = []\n",
        "\n",
        "    for i, cls in enumerate(detections['classes']):\n",
        "        if cls == 'person':\n",
        "            x1, y1, x2, y2 = detections['boxes'][i]\n",
        "\n",
        "            # Ước tính vùng khuôn mặt (thường ở phần trên của hộp người)\n",
        "            face_h = (y2 - y1) // 4  # Ước lượng chiều cao khuôn mặt\n",
        "            face_y2 = y1 + face_h + face_h // 2  # Giới hạn dưới của khuôn mặt\n",
        "            face_region = image[max(0, y1):min(face_y2, image.shape[0]),\n",
        "                              max(0, x1):min(x2, image.shape[1])]\n",
        "\n",
        "            # Kiểm tra kích thước vùng mặt\n",
        "            if face_region.shape[0] > 20 and face_region.shape[1] > 20:\n",
        "                face_regions.append({\n",
        "                    'region': face_region,\n",
        "                    'box': (x1, y1, x2, y2)\n",
        "                })\n",
        "\n",
        "    # Phân tích biểu cảm cho từng khuôn mặt\n",
        "    significant_emotions = ['happy', 'sad', 'angry', 'surprise', 'fear', 'disgust']\n",
        "    emotion_scores = []\n",
        "\n",
        "    for face in face_regions:\n",
        "        try:\n",
        "            # Sử dụng DeepFace để phân tích biểu cảm\n",
        "            result = DeepFace.analyze(face['region'], actions=['emotion'], enforce_detection=False, silent=True)\n",
        "\n",
        "            if isinstance(result, list):\n",
        "                result = result[0]  # Lấy kết quả đầu tiên nếu có nhiều\n",
        "\n",
        "            # Lấy thông tin biểu cảm\n",
        "            emotion = result['dominant_emotion']\n",
        "            emotion_data = {\n",
        "                'box': face['box'],\n",
        "                'emotion': emotion,\n",
        "                'scores': result['emotion']\n",
        "            }\n",
        "\n",
        "            # Tính cường độ cảm xúc (emotion intensity)\n",
        "            # Cảm xúc mạnh hơn khi một cảm xúc vượt trội hẳn so với các cảm xúc khác\n",
        "            max_score = max(result['emotion'].values())\n",
        "            avg_other = sum([s for e, s in result['emotion'].items() if e != emotion]) / (len(result['emotion']) - 1)\n",
        "            intensity = (max_score - avg_other) / 100  # Chuẩn hóa về dải [0, 1]\n",
        "            emotion_data['intensity'] = intensity\n",
        "\n",
        "            # Thêm vào danh sách kết quả\n",
        "            expression_results['expressions'].append(emotion_data)\n",
        "\n",
        "            # Thêm vào danh sách điểm cảm xúc có ý nghĩa\n",
        "            if emotion in significant_emotions:\n",
        "                emotion_scores.append({\n",
        "                    'emotion': emotion,\n",
        "                    'intensity': intensity\n",
        "                })\n",
        "\n",
        "            faces_detected += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    # Tổng hợp kết quả\n",
        "    if faces_detected > 0:\n",
        "        expression_results['has_faces'] = True\n",
        "\n",
        "        # Xác định cảm xúc chủ đạo (nếu có nhiều mặt)\n",
        "        if emotion_scores:\n",
        "            # Sắp xếp theo cường độ\n",
        "            emotion_scores.sort(key=lambda x: x['intensity'], reverse=True)\n",
        "            dominant = emotion_scores[0]\n",
        "            expression_results['dominant_emotion'] = dominant['emotion']\n",
        "            expression_results['emotion_intensity'] = dominant['intensity']\n",
        "\n",
        "            # Đánh giá giá trị cảm xúc\n",
        "            if dominant['intensity'] > 0.7:\n",
        "                expression_results['emotional_value'] = 'Very High'\n",
        "            elif dominant['intensity'] > 0.5:\n",
        "                expression_results['emotional_value'] = 'High'\n",
        "            elif dominant['intensity'] > 0.3:\n",
        "                expression_results['emotional_value'] = 'Medium'\n",
        "            else:\n",
        "                expression_results['emotional_value'] = 'Low'\n",
        "\n",
        "    return expression_results\n",
        "\n",
        "def visualize_sports_results(img_data, detections, depth_map, sports_analysis, action_analysis, composition_analysis):\n",
        "    \"\"\"Create sports-specific visualization\"\"\"\n",
        "    img = np.array(img_data['resized']).copy()\n",
        "    height, width = img.shape[:2]\n",
        "\n",
        "    # Create detection visualization\n",
        "    det_viz = img.copy()\n",
        "\n",
        "    # Draw bounding boxes\n",
        "    for i, box in enumerate(detections['boxes']):\n",
        "        x1, y1, x2, y2 = box\n",
        "        label = detections['classes'][i]\n",
        "        conf = detections['scores'][i]\n",
        "\n",
        "        # Different colors for different classes\n",
        "        if label == 'person':\n",
        "            color = (0, 255, 0)  # Green for people\n",
        "        elif 'ball' in label:\n",
        "            color = (0, 0, 255)  # Red for balls\n",
        "        else:\n",
        "            color = (255, 0, 0)  # Blue for other equipment\n",
        "\n",
        "        cv2.rectangle(det_viz, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(det_viz, f\"{label} {conf:.2f}\", (x1, y1-10),\n",
        "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
        "\n",
        "    # Create composition analysis visualization\n",
        "    comp_viz = img.copy()\n",
        "\n",
        "    # Draw rule of thirds grid\n",
        "    for i in range(1, 3):\n",
        "        cv2.line(comp_viz, (0, int(height*i/3)), (width, int(height*i/3)), (255, 255, 255), 1)\n",
        "        cv2.line(comp_viz, (int(width*i/3), 0), (int(width*i/3), height), (255, 255, 255), 1)\n",
        "\n",
        "    # Draw key subjects with prominence\n",
        "    if \"key_subjects\" in sports_analysis:\n",
        "        for subject in sports_analysis['key_subjects']:\n",
        "            box = subject['box']\n",
        "            x1, y1, x2, y2 = box\n",
        "            # Color based on prominence - more red = more important\n",
        "            prominence = min(1.0, subject['prominence'] * 10)  # Scale for visibility\n",
        "            color = (0, int(255 * (1-prominence)), int(255 * prominence))\n",
        "            cv2.rectangle(comp_viz, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "    # Display results\n",
        "    plt.figure(figsize=(15, 10))\n",
        "\n",
        "    plt.subplot(2, 2, 1)\n",
        "    plt.imshow(img)\n",
        "    plt.title(\"Original Image\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, 2, 2)\n",
        "    plt.imshow(det_viz)\n",
        "    plt.title(f\"Detections: {detections['athletes']} athletes, {detections['sports_objects']} sports objects\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, 2, 3)\n",
        "    plt.imshow(comp_viz)\n",
        "    plt.title(\"Composition Analysis\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(2, 2, 4)\n",
        "    plt.imshow(depth_map, cmap='plasma')\n",
        "    plt.title(\"Depth Map\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print detailed analysis\n",
        "    print(\"\\n==== SPORTS IMAGE ANALYSIS ====\")\n",
        "    print(f\"Detected {detections['athletes']} athletes and {len(detections['classes'])-detections['athletes']} other objects\")\n",
        "\n",
        "    if \"sport_type\" in composition_analysis:\n",
        "        print(f\"\\nSport type: {composition_analysis['sport_type']}\")\n",
        "\n",
        "    if detections['athletes'] > 0:\n",
        "        print(\"\\nPlayer Analysis:\")\n",
        "        print(f\"- Number of players: {detections['athletes']}\")\n",
        "        if detections['athletes'] > 1:\n",
        "            print(f\"- Player dispersion: {sports_analysis['player_dispersion']:.2f}\")\n",
        "\n",
        "    print(\"\\nAction Analysis:\")\n",
        "    print(f\"- Equipment detected: {', '.join(action_analysis['equipment_types']) if action_analysis['equipment_types'] else 'None'}\")\n",
        "    print(f\"- Action level: {action_analysis['action_quality']} ({action_analysis['action_level']:.2f})\")\n",
        "\n",
        "    print(\"\\nComposition Analysis:\")\n",
        "    print(f\"- Framing quality: {composition_analysis['framing_quality']}\")\n",
        "\n",
        "    if composition_analysis['recommended_crop']:\n",
        "        crop = composition_analysis['recommended_crop']\n",
        "        direction_x = \"right\" if crop['shift_x'] < 0 else \"left\"\n",
        "        direction_y = \"down\" if crop['shift_y'] < 0 else \"up\"\n",
        "        print(f\"- Recommended crop: Shift {abs(crop['shift_x'])*100:.1f}% {direction_x} and {abs(crop['shift_y'])*100:.1f}% {direction_y}\")\n",
        "\n",
        "    # Key subjects\n",
        "    if sports_analysis['key_subjects']:\n",
        "        print(\"\\nKey Subjects by Prominence:\")\n",
        "        for i, subject in enumerate(sports_analysis['key_subjects']):\n",
        "            print(f\"{i+1}. {subject['class']} (Prominence: {subject['prominence']:.2f})\")\n",
        "\n",
        "    if facial_analysis and facial_analysis.get('has_faces', False):\n",
        "        print(\"\\nPhân tích biểu cảm:\")\n",
        "        print(f\"- Cảm xúc chủ đạo: {facial_analysis['dominant_emotion']}\")\n",
        "        print(f\"- Cường độ cảm xúc: {facial_analysis['emotion_intensity']:.2f}\")\n",
        "        print(f\"- Giá trị cảm xúc: {facial_analysis['emotional_value']}\")\n",
        "\n",
        "def analyze_sports_image(file_path):\n",
        "    \"\"\"Main function to analyze sports images\"\"\"\n",
        "    t_start = time.time()\n",
        "\n",
        "    # Load models\n",
        "    sam_predictor, midas, yolo = load_models()\n",
        "    img_data = preprocess_image(file_path)\n",
        "\n",
        "    # Step 1: Object detection with YOLO\n",
        "    detections = detect_sports_objects(yolo, img_data)\n",
        "\n",
        "    # Step 2: Generate depth map\n",
        "    depth_map, depth_mask, depth_contour = generate_depth_map(midas, img_data)\n",
        "\n",
        "    # Step 3: Optional - Segment with SAM if needed\n",
        "    # seg_mask, color_mask = segment_with_sam(sam_predictor, img_data)\n",
        "\n",
        "    # Step 4: Analyze sports scene\n",
        "    sports_analysis = analyze_sports_scene(detections, depth_map, img_data)\n",
        "\n",
        "    # Step 5: Analyze action quality\n",
        "    action_analysis = analyze_action_quality(detections, img_data)\n",
        "\n",
        "    # Step 6: Sports composition analysis\n",
        "    composition_analysis = analyze_sports_composition(detections, sports_analysis, img_data)\n",
        "\n",
        "    # Bổ sung: Phân tích biểu cảm (tuỳ chọn)\n",
        "    facial_analysis = None\n",
        "    try:\n",
        "        facial_analysis = analyze_facial_expression(detections, img_data)\n",
        "    except Exception as e:\n",
        "        print(f\"Không thể phân tích biểu cảm: {str(e)}\")\n",
        "\n",
        "    # Step 7: Visualize results - thêm facial_analysis vào tham số\n",
        "    visualize_sports_results(img_data, detections, depth_map,\n",
        "                            sports_analysis, action_analysis, composition_analysis,\n",
        "                            facial_analysis)\n",
        "\n",
        "    t_end = time.time()\n",
        "    print(f\"\\nAnalysis completed in {t_end - t_start:.2f} seconds\")\n",
        "\n",
        "    return {\n",
        "        'detections': detections,\n",
        "        'sports_analysis': sports_analysis,\n",
        "        'action_analysis': action_analysis,\n",
        "        'composition_analysis': composition_analysis,\n",
        "        'facial_analysis': facial_analysis\n",
        "    }\n",
        "\n",
        "def main():\n",
        "    # Upload image\n",
        "    print(\"Please upload a sports image:\")\n",
        "    uploaded = files.upload()\n",
        "    file_name = next(iter(uploaded))\n",
        "\n",
        "    # Analyze image\n",
        "    analysis = analyze_sports_image(file_name)\n",
        "    return analysis\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "2VQ_3ZqWra2A",
        "outputId": "45b4b16d-5209-41eb-aa86-98da97f21323"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload a sports image:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-db0de683-5229-4ad1-a027-2cb3fdd0e533\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-db0de683-5229-4ad1-a027-2cb3fdd0e533\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 2166064646.webp to 2166064646 (3).webp\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/intel-isl_MiDaS_master\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 480x640 5 persons, 1 baseball bat, 74.9ms\n",
            "Speed: 24.9ms preprocess, 74.9ms inference, 11.8ms postprocess per image at shape (1, 3, 480, 640)\n",
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.67.1)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.2.1)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.11.0.86)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\n",
            "Collecting flask-cors>=4.0.1 (from deepface)\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (3.1.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (3.18.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->deepface) (24.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.9)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.13.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.15.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2025.4.26)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (5.29.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.7)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading lz4-4.4.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=8a4bbd36c143204bd3f63b5b9d3f83a5e61e6eccf5b99541c7374420c1d4c820\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\n",
            "Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.1 gunicorn-23.0.0 lz4-4.4.4 mtcnn-1.0.0 retina-face-0.0.17\n",
            "25-05-12 14:38:38 - Directory /root/.deepface has been created\n",
            "25-05-12 14:38:38 - Directory /root/.deepface/weights has been created\n",
            "25-05-12 14:38:39 - facial_expression_model_weights.h5 will be downloaded...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://github.com/serengil/deepface_models/releases/download/v1.0/facial_expression_model_weights.h5\n",
            "To: /root/.deepface/weights/facial_expression_model_weights.h5\n",
            "100%|██████████| 5.98M/5.98M [00:00<00:00, 103MB/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "visualize_sports_results() takes 6 positional arguments but 7 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-cee6104fc972>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 703\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-cee6104fc972>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    697\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m     \u001b[0;31m# Analyze image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m     \u001b[0manalysis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manalyze_sports_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manalysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-cee6104fc972>\u001b[0m in \u001b[0;36manalyze_sports_image\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0;31m# Step 7: Visualize results - thêm facial_analysis vào tham số\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m     visualize_sports_results(img_data, detections, depth_map, \n\u001b[0m\u001b[1;32m    678\u001b[0m                             \u001b[0msports_analysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_analysis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomposition_analysis\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m                             facial_analysis)\n",
            "\u001b[0;31mTypeError\u001b[0m: visualize_sports_results() takes 6 positional arguments but 7 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N6V6ewbc6yKc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}