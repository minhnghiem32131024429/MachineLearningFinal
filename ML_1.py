import os
import traceback
import PIL.Image
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"
import torch, cv2, numpy as np, matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms
import time
import argparse
import sys
import math
import matplotlib
matplotlib.use('Agg')
try:
    import clip
    import torch
    CLIP_AVAILABLE = True
    from ultralytics import YOLO
    POSE_MODEL_AVAILABLE = True
except ImportError:
    CLIP_AVAILABLE = False
    print("CLIP not available. To use precise ball detection, install with: pip install clip")
    POSE_MODEL_AVAILABLE = False
    print("YOLOv8-Pose kh√¥ng kh·∫£ d·ª•ng. C√†i ƒë·∫∑t v·ªõi: pip install ultralytics")



def classify_sports_ball_with_clip(image, box, device=None):
    """
    S·ª≠ d·ª•ng CLIP ƒë·ªÉ ph√¢n lo·∫°i ch√≠nh x√°c lo·∫°i b√≥ng t·ª´ v√πng ƒë√£ ph√°t hi·ªán l√† 'sports ball'

    Args:
        image: ·∫¢nh numpy array (RGB)
        box: [x1, y1, x2, y2] - T·ªça ƒë·ªô bounding box c·ªßa b√≥ng
        device: Thi·∫øt b·ªã t√≠nh to√°n (cuda/cpu)

    Returns:
        String: Lo·∫°i b√≥ng c·ª• th·ªÉ ("soccer ball", "basketball", ...)
    """
    if not CLIP_AVAILABLE:
        return "sports ball"

    # X√°c ƒë·ªãnh thi·∫øt b·ªã n·∫øu ch∆∞a ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # T·∫£i model CLIP (ch·ªâ t·∫£i m·ªôt l·∫ßn)
    if not hasattr(classify_sports_ball_with_clip, 'model'):
        print("ƒêang t·∫£i model CLIP...")
        classify_sports_ball_with_clip.model, classify_sports_ball_with_clip.preprocess = clip.load("ViT-B/32",
                                                                                                    device=device)
        print("ƒê√£ t·∫£i model CLIP th√†nh c√¥ng")

    model = classify_sports_ball_with_clip.model
    preprocess = classify_sports_ball_with_clip.preprocess

    # C·∫Øt v√πng ·∫£nh ch·ª©a b√≥ng t·ª´ box
    x1, y1, x2, y2 = [int(coord) for coord in box]
    # Th√™m padding nh·ªè xung quanh ƒë·ªÉ ƒë·∫£m b·∫£o l·∫•y to√†n b·ªô b√≥ng
    padding = int(min(x2 - x1, y2 - y1) * 0.1)
    x1 = max(0, x1 - padding)
    y1 = max(0, y1 - padding)
    x2 = min(image.shape[1], x2 + padding)
    y2 = min(image.shape[0], y2 + padding)

    ball_img = image[y1:y2, x1:x2]

    # Chuy·ªÉn th√†nh ƒë·ªãnh d·∫°ng PIL Image v√† ti·ªÅn x·ª≠ l√Ω cho CLIP
    try:
        pil_img = PIL.Image.fromarray(ball_img.astype('uint8'))
        processed_img = preprocess(pil_img).unsqueeze(0).to(device)
    except Exception as e:
        print(f"L·ªói khi x·ª≠ l√Ω ·∫£nh b√≥ng: {str(e)}")
        return "sports ball"

    # M·ªû R·ªòNG DANH S√ÅCH C√ÅC LO·∫†I B√ìNG CLIP C√ì TH·ªÇ PH√ÇN LO·∫†I
    ball_descriptions = [
        # B√≥ng ƒë√°
        "a soccer ball", "a white and black soccer ball", "a football used in soccer games",
        "a FIFA soccer ball", "a round soccer ball with pentagonal patterns",

        # B√≥ng r·ªï
        "a basketball", "an orange basketball with black lines", "a ball used in basketball",
        "a Spalding basketball", "an NBA basketball",

        # B√≥ng tennis
        "a tennis ball", "a yellow-green tennis ball", "a small fuzzy ball used in tennis",
        "a Wilson tennis ball", "a bright yellow tennis ball",

        # B√≥ng chuy·ªÅn
        "a volleyball", "a white volleyball with panels", "a ball used in volleyball games",
        "a Mikasa volleyball", "a white and blue volleyball",

        # B√≥ng ch√†y
        "a baseball", "a white baseball with red stitching", "a small hard ball used in baseball",
        "a Major League baseball", "a leather baseball",

        # B√≥ng golf
        "a golf ball", "a small white golf ball with dimples", "a ball used in golf",
        "a Titleist golf ball", "a dimpled white golf ball",

        # B√≥ng rugby
        "a rugby ball", "an oval-shaped rugby ball", "a ball used in rugby",
        "an American football", "an NFL football",

        # C√°c lo·∫°i b√≥ng kh√°c
        "a ping pong ball", "a small white table tennis ball", "a ball used in table tennis",
        "a bowling ball", "a heavy ball used for bowling", "a black bowling ball",
        "a beach ball", "a large inflatable ball", "a colorful beach ball"
    ]

    # M√£ h√≥a c√°c m√¥ t·∫£ vƒÉn b·∫£n
    text_inputs = clip.tokenize(ball_descriptions).to(device)

    with torch.no_grad():
        # M√£ h√≥a h√¨nh ·∫£nh
        image_features = model.encode_image(processed_img)
        # M√£ h√≥a vƒÉn b·∫£n
        text_features = model.encode_text(text_inputs)

        # T√≠nh to√°n ƒë·ªô t∆∞∆°ng ƒë·ªìng gi·ªØa h√¨nh ·∫£nh v√† vƒÉn b·∫£n
        image_features /= image_features.norm(dim=-1, keepdim=True)
        text_features /= text_features.norm(dim=-1, keepdim=True)
        similarity = (100.0 * image_features @ text_features.T).softmax(dim=-1)

        # T√¨m m√¥ t·∫£ c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t
        values, indices = similarity[0].topk(3)

    # Ch·ªçn m√¥ t·∫£ c√≥ ƒë·ªô t∆∞∆°ng ƒë·ªìng cao nh·∫•t
    best_match_idx = indices[0].item()
    best_match = ball_descriptions[best_match_idx]

    # M·ªû R·ªòNG MAPPING C√ÅC LO·∫†I B√ìNG
    if "soccer" in best_match:
        return "soccer ball"
    elif "basketball" in best_match or "NBA" in best_match:
        return "basketball"
    elif "tennis" in best_match:
        return "tennis ball"
    elif "volleyball" in best_match:
        return "volleyball"
    elif "baseball" in best_match and "Major League" not in best_match:
        return "baseball"
    elif "golf" in best_match:
        return "golf ball"
    elif "rugby" in best_match or "American football" in best_match or "NFL" in best_match:
        return "american football"
    elif "ping pong" in best_match or "table tennis" in best_match:
        return "ping pong ball"
    elif "bowling" in best_match:
        return "bowling ball"
    elif "beach" in best_match:
        return "beach ball"

    # N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, gi·ªØ nguy√™n nh√£n g·ªëc
    return "sports ball"

# DNN Face Detection Functions
def detect_faces_improved(image):
    """
    Ph√°t hi·ªán khu√¥n m·∫∑t v·ªõi DNN model - c·∫£i thi·ªán cho nhi·ªÅu lo·∫°i da v√† g√≥c quay

    Args:
        image: ·∫¢nh RGB (kh√¥ng ph·∫£i BGR)

    Returns:
        faces: Danh s√°ch c√°c khu√¥n m·∫∑t d∆∞·ªõi d·∫°ng (x, y, w, h)
    """
    print("Ph√°t hi·ªán khu√¥n m·∫∑t v·ªõi DNN model...")

    # ƒê·∫£m b·∫£o ·∫£nh ƒë√∫ng ƒë·ªãnh d·∫°ng BGR (OpenCV)
    if len(image.shape) == 3 and image.shape[2] == 3:
        # Ki·ªÉm tra n·∫øu ƒë·∫ßu v√†o l√† RGB, chuy·ªÉn sang BGR
        img_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    else:
        img_bgr = image.copy()

    # L·∫•y k√≠ch th∆∞·ªõc ·∫£nh
    height, width = img_bgr.shape[:2]

    # T·∫°o th∆∞ m·ª•c cho model
    model_dir = "face_models"
    os.makedirs(model_dir, exist_ok=True)

    # ƒê∆∞·ªùng d·∫´n t·ªõi c√°c file model
    model_files = {
        "prototxt": os.path.join(model_dir, "deploy.prototxt"),
        "model": os.path.join(model_dir, "res10_300x300_ssd_iter_140000.caffemodel")
    }

    # Ki·ªÉm tra v√† t·∫£i model n·∫øu ch∆∞a c√≥
    if not os.path.exists(model_files["prototxt"]) or not os.path.exists(model_files["model"]):
        print("ƒêang t·∫£i model ph√°t hi·ªán khu√¥n m·∫∑t...")

        # URLs cho model files
        urls = {
            "prototxt": "https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt",
            "model": "https://raw.githubusercontent.com/opencv/opencv_3rdparty/dnn_samples_face_detector_20180205_fp16/res10_300x300_ssd_iter_140000_fp16.caffemodel"
        }

        # T·∫£i c√°c file
        try:
            import urllib.request
            for name, url in urls.items():
                if not os.path.exists(model_files[name]):
                    print(f"ƒêang t·∫£i {name}...")
                    urllib.request.urlretrieve(url, model_files[name])
        except Exception as e:
            print(f"L·ªói khi t·∫£i model: {str(e)}")
            # S·ª≠ d·ª•ng haar cascade n·∫øu kh√¥ng t·∫£i ƒë∆∞·ª£c DNN model
            return []

    # T·∫£i model
    try:
        face_net = cv2.dnn.readNetFromCaffe(model_files["prototxt"], model_files["model"])

        # Chu·∫©n b·ªã blob t·ª´ ·∫£nh - quan tr·ªçng v·ªõi preprocess
        # mean subtraction gi√∫p c·∫£i thi·ªán v·ªõi c√°c t√¥ng da kh√°c nhau
        blob = cv2.dnn.blobFromImage(img_bgr, 1.0, (300, 300),
                                     [104, 117, 123], False, False)

        # ƒê∆∞a blob v√†o network
        face_net.setInput(blob)

        # Th·ª±c hi·ªán ph√°t hi·ªán
        detections = face_net.forward()

        # Danh s√°ch khu√¥n m·∫∑t ph√°t hi·ªán ƒë∆∞·ª£c
        faces = []

        # Ng∆∞·ª°ng tin c·∫≠y - c√≥ th·ªÉ gi·∫£m xu·ªëng ƒë·ªÉ ph√°t hi·ªán th√™m khu√¥n m·∫∑t kh√≥
        confidence_threshold = 0.6

        if not faces and confidence_threshold > 0.5:
            print("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t, gi·∫£m ng∆∞·ª°ng tin c·∫≠y...")
            confidence_threshold = 0.5

        # Duy·ªát qua c√°c khu√¥n m·∫∑t ph√°t hi·ªán
        for i in range(detections.shape[2]):
            confidence = detections[0, 0, i, 2]

            # L·ªçc theo ng∆∞·ª°ng tin c·∫≠y
            if confidence > confidence_threshold:
                # L·∫•y t·ªça ƒë·ªô khu√¥n m·∫∑t
                box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])
                (x1, y1, x2, y2) = box.astype("int")

                # ƒê·∫£m b·∫£o t·ªça ƒë·ªô n·∫±m trong ·∫£nh
                x1 = max(0, x1)
                y1 = max(0, y1)
                x2 = min(width, x2)
                y2 = min(height, y2)

                # T√≠nh to√°n width v√† height
                w = x2 - x1
                h = y2 - y1

                # Th√™m v√†o danh s√°ch n·∫øu k√≠ch th∆∞·ªõc h·ª£p l√Ω
                if w > 20 and h > 20:
                    faces.append((x1, y1, w, h))
                    print(f"Ph√°t hi·ªán khu√¥n m·∫∑t: {x1},{y1} - {w}x{h} (tin c·∫≠y: {confidence:.2f})")

        # N·∫øu kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t n√†o, gi·∫£m ng∆∞·ª°ng v√† th·ª≠ l·∫°i
        if not faces and confidence_threshold > 0.3:
            print("Kh√¥ng ph√°t hi·ªán khu√¥n m·∫∑t, gi·∫£m ng∆∞·ª°ng tin c·∫≠y...")
            confidence_threshold = 0.3

            for i in range(detections.shape[2]):
                confidence = detections[0, 0, i, 2]

                if confidence > confidence_threshold:
                    box = detections[0, 0, i, 3:7] * np.array([width, height, width, height])
                    (x1, y1, x2, y2) = box.astype("int")

                    x1 = max(0, x1)
                    y1 = max(0, y1)
                    x2 = min(width, x2)
                    y2 = min(height, y2)

                    w = x2 - x1
                    h = y2 - y1

                    if w > 20 and h > 20:
                        faces.append((x1, y1, w, h))
                        print(f"Ph√°t hi·ªán khu√¥n m·∫∑t (ng∆∞·ª°ng th·∫•p): {x1},{y1} - {w}x{h} (tin c·∫≠y: {confidence:.2f})")

        return faces

    except Exception as e:
        print(f"L·ªói khi s·ª≠ d·ª•ng DNN face detector: {str(e)}")
        print(f"Chi ti·∫øt: {traceback.format_exc()}")
        # S·ª≠ d·ª•ng haar cascade n·∫øu DNN g·∫∑p l·ªói
        return []




def select_best_face(faces, image):
    """
    Ch·ªçn khu√¥n m·∫∑t t·ªët nh·∫•t t·ª´ danh s√°ch c√°c khu√¥n m·∫∑t ph√°t hi·ªán ƒë∆∞·ª£c
    """
    if not faces:
        return None

    # N·∫øu ch·ªâ c√≥ 1 khu√¥n m·∫∑t, tr·∫£ v·ªÅ lu√¥n
    if len(faces) == 1:
        return faces[0]

    # Ti√™u ch√≠ ch·ªçn khu√¥n m·∫∑t:
    # 1. Khu√¥n m·∫∑t ·ªü gi·ªØa ·∫£nh
    # 2. Khu√¥n m·∫∑t c√≥ k√≠ch th∆∞·ªõc l·ªõn
    # 3. Khu√¥n m·∫∑t c√≥ ƒë·ªô t∆∞∆°ng ph·∫£n t·ªët

    height, width = image.shape[:2]
    center_x, center_y = width // 2, height // 2

    best_face = None
    best_score = -1

    for (x, y, w, h) in faces:
        # T√≠nh ƒëi·ªÉm t√¢m khu√¥n m·∫∑t
        face_center_x = x + w // 2
        face_center_y = y + h // 2

        # Kho·∫£ng c√°ch t·ª´ t√¢m khu√¥n m·∫∑t ƒë·∫øn t√¢m ·∫£nh (chu·∫©n h√≥a)
        distance_to_center = math.sqrt(((face_center_x - center_x) / width) ** 2 +
                                       ((face_center_y - center_y) / height) ** 2)

        # K√≠ch th∆∞·ªõc t∆∞∆°ng ƒë·ªëi c·ªßa khu√¥n m·∫∑t
        relative_size = (w * h) / (width * height)

        # T√≠nh ƒë·ªô t∆∞∆°ng ph·∫£n c·ªßa khu√¥n m·∫∑t
        face_roi = image[y:y + h, x:x + w]
        if len(face_roi.shape) == 3:
            face_roi = cv2.cvtColor(face_roi, cv2.COLOR_BGR2GRAY)
        contrast = np.std(face_roi)

        # T√≠nh ƒëi·ªÉm s·ªë t·ªïng h·ª£p (c√°c h·ªá s·ªë c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh)
        # - Khu√¥n m·∫∑t g·∫ßn t√¢m c√≥ ƒëi·ªÉm cao (1 - distance_to_center)
        # - Khu√¥n m·∫∑t l·ªõn c√≥ ƒëi·ªÉm cao (relative_size)
        # - Khu√¥n m·∫∑t c√≥ ƒë·ªô t∆∞∆°ng ph·∫£n cao c√≥ ƒëi·ªÉm cao (contrast/128)
        score = (1 - distance_to_center) * 0.5 + relative_size * 0.3 + (contrast / 128) * 0.2

        if score > best_score:
            best_score = score
            best_face = (x, y, w, h)

    return best_face

def check_dependencies():
    required_packages = {
        'ultralytics': 'ultralytics',
        'mtcnn': 'mtcnn',
        'timm': 'timm',  # Th√™m timm cho MiDaS
        'torch': 'torch',
        'torchvision': 'torchvision',
        'cv2': 'opencv-python',
        'numpy': 'numpy',
        'matplotlib': 'matplotlib',
        'ultralytics': 'ultralytics',
        'scipy': 'scipy',
        'PIL': 'pillow',
        'clip': 'git+https://github.com/openai/CLIP.git',
        'ftfy': 'ftfy',
        'regex': 'regex'
    }

    missing_packages = []

    for module, package in required_packages.items():
        try:
            __import__(module)
            print(f"‚úì {module} already installed")
        except ImportError:
            print(f"‚úó {module} missing - will install {package}")
            missing_packages.append(package)

    if missing_packages:
        print("\nInstalling missing packages...")
        for package in missing_packages:
            print(f"Installing {package}...")
            os.system(f"pip install {package}")
            print(f"Installed {package}")

        # N·∫øu c√≥ g√≥i n√†o ƒë√£ ƒë∆∞·ª£c c√†i ƒë·∫∑t, reload m√¥i tr∆∞·ªùng
        if missing_packages:
            print("\nReloading environment with new packages...")
            # Force reload modules in case they were partially loaded
            for module in sys.modules.copy():
                if any(module.startswith(pkg.split('.')[0]) for pkg in required_packages.keys()):
                    try:
                        del sys.modules[module]
                    except:
                        pass

    print("All dependencies checked and installed.")


# Setup device and load models
def load_models():
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"Using device: {device}")

        # Load MiDaS depth model with explicit trust_repo
        print("Loading MiDaS depth model...")
        midas = torch.hub.load("intel-isl/MiDaS", "DPT_Large", trust_repo=True)
        midas.to(device)
        midas.eval()
        print("MiDaS model loaded successfully.")

        # Load YOLO model for object detection
        print("Loading YOLO model...")
        from ultralytics import YOLO
        yolo = YOLO("yolov8x.pt")  # Load the largest YOLOv8 model
        print("YOLO model loaded successfully.")
        yolo_seg = YOLO("yolov8x-seg.pt")
        print("YOLOv8-seg model loaded successfully.")

        return midas, yolo, yolo_seg, device

    except Exception as e:
        print(f"Error loading models: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


def preprocess_image(image_path):
    img = Image.open(image_path).convert('RGB')
    img_array = np.array(img)

    # Resize keeping aspect ratio
    width, height = img.size
    ratio = min(520 / width, 520 / height)
    new_size = (int(width * ratio), int(height * ratio))
    resized_img = img.resize(new_size, Image.LANCZOS)

    # Get the device
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Process for MiDaS
    midas_transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Resize((384, 384), antialias=True),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])
    ])
    midas_input = midas_transform(img).unsqueeze(0).to(device)

    return {
        'original': img, 'array': img_array,
        'resized': resized_img, 'resized_array': np.array(resized_img),
        'midas_input': midas_input
    }


def detect_sports_objects(yolo, img_data):
    """Detect sports-related objects using YOLO"""
    results = yolo(img_data['resized_array'], conf=0.25)

    # Extract detections - CH·ªà C√ÅC L·ªöP YOLO TH·ª∞C S·ª∞ C√ì
    sports_classes = [
        'person',           # Ng∆∞·ªùi
        'sports ball',      # B√≥ng th·ªÉ thao (s·∫Ω d√πng CLIP ph√¢n lo·∫°i chi ti·∫øt)
        'tennis racket',    # V·ª£t tennis
        'baseball bat',     # G·∫≠y baseball
        'baseball glove',   # GƒÉng tay baseball
        'frisbee',         # ƒêƒ©a bay
        'skis',            # V√°n tr∆∞·ª£t tuy·∫øt
        'snowboard',       # V√°n tr∆∞·ª£t tuy·∫øt ƒë∆°n
        'surfboard',       # V√°n l∆∞·ªõt s√≥ng
        'bicycle',         # Xe ƒë·∫°p
        'motorcycle',      # Xe m√°y
        'kite',            # Di·ªÅu
        'skateboard',      # V√°n tr∆∞·ª£t
        'bottle',          # Chai n∆∞·ªõc (ph·ª• ki·ªán th·ªÉ thao)
        'backpack',        # Ba l√¥ th·ªÉ thao
        'handbag',         # T√∫i th·ªÉ thao
        'umbrella',        # √î (cho golf)
        'tie',             # C√† v·∫°t (trang ph·ª•c th·ªÉ thao ch√≠nh th·ª©c)
        'suitcase',        # Vali ƒë·ª±ng ƒë·ªì th·ªÉ thao
        'cup'              # C·ªëc/ly (gi·∫£i th∆∞·ªüng ho·∫∑c n∆∞·ªõc u·ªëng)
    ]

    result = results[0]  # First image result

    detections = {
        'boxes': [],
        'classes': [],
        'scores': [],
        'sports_objects': 0,
        'athletes': 0
    }

    if hasattr(result, 'boxes') and len(result.boxes) > 0:
        for box in result.boxes:
            x1, y1, x2, y2 = map(int, box.xyxy[0])
            conf = float(box.conf[0])
            cls = int(box.cls[0])
            class_name = result.names[cls]

            detections['boxes'].append([x1, y1, x2, y2])
            detections['classes'].append(class_name)
            detections['scores'].append(conf)

            # S·ª≠ d·ª•ng CLIP ƒë·ªÉ ph√¢n lo·∫°i ch√≠nh x√°c lo·∫°i b√≥ng
            if class_name == 'sports ball' and CLIP_AVAILABLE and conf > 0.4:
                try:
                    # X√°c ƒë·ªãnh lo·∫°i b√≥ng c·ª• th·ªÉ b·∫±ng CLIP
                    specific_ball = classify_sports_ball_with_clip(img_data['resized_array'], [x1, y1, x2, y2])
                    print(f"CLIP ph√¢n lo·∫°i: 'sports ball' -> '{specific_ball}'")

                    # C·∫≠p nh·∫≠t l·∫°i class_name v·ªõi lo·∫°i b√≥ng x√°c ƒë·ªãnh ƒë∆∞·ª£c
                    class_name = specific_ball
                    detections['classes'][-1] = class_name
                except Exception as e:
                    print(f"L·ªói khi ph√¢n lo·∫°i b√≥ng v·ªõi CLIP: {str(e)}")

            if class_name == 'person':
                detections['athletes'] += 1
            if class_name in sports_classes:
                detections['sports_objects'] += 1

    return detections


def generate_depth_map(midas, img_data):
    with torch.no_grad():
        depth_map = midas(img_data['midas_input'])
        depth_map = torch.nn.functional.interpolate(
            depth_map.unsqueeze(1),
            size=img_data['array'].shape[:2],
            mode="bicubic",
            align_corners=False
        ).squeeze().cpu().numpy()

    normalized_depth = (depth_map - depth_map.min()) / (depth_map.max() - depth_map.min() + 1e-5)

    depth_8bit = (normalized_depth * 255).astype(np.uint8)
    _, depth_mask = cv2.threshold(depth_8bit, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)

    kernel = np.ones((5, 5), np.uint8)
    depth_mask = cv2.morphologyEx(depth_mask, cv2.MORPH_OPEN, kernel)

    contours, _ = cv2.findContours(depth_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
    main_contour = max(contours, key=cv2.contourArea) if contours else None

    return normalized_depth, depth_mask, main_contour


# H√†m ph√¢n t√≠ch ƒë·ªô s·∫Øc n√©t c·ªßa ƒë·ªëi t∆∞·ª£ng (M·ªöI)
def analyze_object_sharpness(image, boxes):
    """
    Ph√¢n t√≠ch ƒë·ªô s·∫Øc n√©t c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng ƒë∆∞·ª£c ph√°t hi·ªán
    C·∫£i thi·ªán: S·ª≠ d·ª•ng nhi·ªÅu ph∆∞∆°ng ph√°p ƒë√°nh gi√° sharpness
    """
    import cv2
    import numpy as np

    if len(boxes) == 0:
        return []

    # Convert to grayscale n·∫øu c·∫ßn
    if len(image.shape) == 3:
        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
    else:
        gray = image.copy()

    sharpness_scores = []
    sharpness_details = []
    h, w = gray.shape

    for box in boxes:
        try:
            # ƒê·∫£m b·∫£o box n·∫±m trong ·∫£nh
            x1, y1, x2, y2 = map(int, box[:4])
            x1 = max(0, min(x1, w - 1))
            y1 = max(0, min(y1, h - 1))
            x2 = max(x1 + 1, min(x2, w))
            y2 = max(y1 + 1, min(y2, h))

            # Tr√≠ch xu·∫•t v√πng quan t√¢m
            roi = gray[y1:y2, x1:x2]

            # B·ªè qua v√πng qu√° nh·ªè
            if roi.shape[0] < 10 or roi.shape[1] < 10:
                sharpness_scores.append(0.0)
                continue

            # **PH∆Ø∆†NG PH√ÅP 1: Laplacian Variance (c√≥ l·ªçc noise)**
            # √Åp d·ª•ng Gaussian blur nh·∫π ƒë·ªÉ gi·∫£m noise
            roi_blur = cv2.GaussianBlur(roi, (3, 3), 0)
            laplacian = cv2.Laplacian(roi_blur, cv2.CV_64F)
            laplacian_var = laplacian.var()

            # **PH∆Ø∆†NG PH√ÅP 2: Sobel Gradient Magnitude**
            sobelx = cv2.Sobel(roi, cv2.CV_64F, 1, 0, ksize=3)
            sobely = cv2.Sobel(roi, cv2.CV_64F, 0, 1, ksize=3)
            sobel_magnitude = np.sqrt(sobelx ** 2 + sobely ** 2)
            sobel_mean = np.mean(sobel_magnitude)

            # **PH∆Ø∆†NG PH√ÅP 3: Brenner Gradient**
            # T√≠nh gradient theo ph∆∞∆°ng ngang
            if roi.shape[1] > 2:
                brenner = np.sum((roi[:, 2:] - roi[:, :-2]) ** 2)
                brenner_norm = brenner / (roi.shape[0] * (roi.shape[1] - 2))
            else:
                brenner_norm = 0

            # **PH∆Ø∆†NG PH√ÅP 4: Tenengrad (Sobel based)**
            sobelx_thresh = np.where(np.abs(sobelx) > 10, sobelx, 0)
            sobely_thresh = np.where(np.abs(sobely) > 10, sobely, 0)
            tenengrad = np.sum(sobelx_thresh ** 2 + sobely_thresh ** 2)
            tenengrad_norm = tenengrad / (roi.shape[0] * roi.shape[1])

            # **T·ªîNG H·ª¢P ƒêI·ªÇM SHARPNESS**
            # Normalize t·ª´ng th√†nh ph·∫ßn
            laplacian_norm = min(laplacian_var / 1000.0, 1.0)  # Cap ·ªü 1.0
            sobel_norm = min(sobel_mean / 50.0, 1.0)
            brenner_norm = min(brenner_norm / 100.0, 1.0)
            tenengrad_norm = min(tenengrad_norm / 500.0, 1.0)

            # Weighted combination
            final_score = (
                    0.3 * laplacian_norm +  # Laplacian variance
                    0.3 * sobel_norm +  # Sobel gradient
                    0.2 * brenner_norm +  # Brenner gradient
                    0.2 * tenengrad_norm  # Tenengrad
            )

            # **ƒêI·ªÄU CH·ªàNH THEO K√çCH TH∆Ø·ªöC**
            # V√πng l·ªõn h∆°n th∆∞·ªùng c√≥ ƒëi·ªÉm sharpness ·ªïn ƒë·ªãnh h∆°n
            area = (x2 - x1) * (y2 - y1)
            size_factor = min(np.sqrt(area) / 100.0, 1.2)  # Bonus cho v√πng l·ªõn

            final_score *= size_factor
            final_score = min(final_score, 1.0)  # Cap ·ªü 1.0

            sharpness_scores.append(float(final_score))

            # TH√äM ƒêO·∫†N N√ÄY - L∆∞u chi ti·∫øt cho debugging
            sharpness_details.append({
                'laplacian_var': laplacian_var,
                'sobel_mean': sobel_mean,
                'brenner_norm': brenner_norm,
                'tenengrad_norm': tenengrad_norm,
                'combined_score': final_score
            })

        except Exception as e:
            print(f"Error calculating sharpness for box {box}: {e}")
            sharpness_scores.append(0.0)
            sharpness_details.append({
                'laplacian_var': 0,
                'sobel_mean': 0,
                'brenner_norm': 0,
                'tenengrad_norm': 0,
                'combined_score': 0
            })

    return sharpness_scores, sharpness_details


def create_sharpness_heatmap(image, sharpness_map=None):
    """
    T·∫°o heatmap ƒë·ªô s·∫Øc n√©t cho to√†n b·ªô ·∫£nh
    """
    try:
        import cv2
        import numpy as np

        # ƒê·∫£m b·∫£o image kh√¥ng None v√† c√≥ shape h·ª£p l·ªá
        if image is None or len(image.shape) < 2:
            print("Invalid image input for sharpness heatmap")
            return image.copy() if image is not None else np.zeros((100, 100, 3), dtype=np.uint8), None

        if len(image.shape) == 3:
            gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)
        else:
            gray = image.copy()

        # T√≠nh Laplacian cho to√†n ·∫£nh v·ªõi kernel l·ªõn h∆°n
        laplacian = cv2.Laplacian(gray, cv2.CV_64F, ksize=5)
        laplacian_abs = np.abs(laplacian)

        # L√†m m·ªãn ƒë·ªÉ t·∫°o heatmap
        heatmap = cv2.GaussianBlur(laplacian_abs, (21, 21), 0)

        # Normalize v·ªÅ 0-255
        heatmap_norm = cv2.normalize(heatmap, None, 0, 255, cv2.NORM_MINMAX)
        heatmap_norm = heatmap_norm.astype(np.uint8)

        # √Åp d·ª•ng colormap JET
        heatmap_colored = cv2.applyColorMap(heatmap_norm, cv2.COLORMAP_JET)

        # Chuy·ªÉn v·ªÅ RGB
        heatmap_colored = cv2.cvtColor(heatmap_colored, cv2.COLOR_BGR2RGB)

        # Overlay l√™n ·∫£nh g·ªëc
        if len(image.shape) == 3:
            overlay = cv2.addWeighted(image, 0.6, heatmap_colored, 0.4, 0)
        else:
            image_colored = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)
            overlay = cv2.addWeighted(image_colored, 0.6, heatmap_colored, 0.4, 0)

        print("Sharpness heatmap created successfully")
        return overlay, heatmap_norm

    except Exception as e:
        print(f"Error in create_sharpness_heatmap: {e}")
        # Tr·∫£ v·ªÅ ·∫£nh g·ªëc v√† None n·∫øu c√≥ l·ªói
        if image is not None:
            return image.copy(), None
        else:
            return np.zeros((100, 100, 3), dtype=np.uint8), None


def analyze_sports_scene(detections, depth_map, img_data, yolo_seg=None):
    """Analyze the sports scene based on detected objects and depth"""
    height, width = depth_map.shape[:2]

    # Ph·∫ßn ph√¢n t√≠ch ph√¢n b·ªë ng∆∞·ªùi ch∆°i (gi·ªØ nguy√™n)
    player_positions = []
    for i, cls in enumerate(detections['classes']):
        if cls == 'person':
            x1, y1, x2, y2 = detections['boxes'][i]
            # Scale box coordinates to match depth map dimensions
            x1 = int(x1 * depth_map.shape[1] / img_data['resized_array'].shape[1])
            y1 = int(y1 * depth_map.shape[0] / img_data['resized_array'].shape[0])
            x2 = int(x2 * depth_map.shape[1] / img_data['resized_array'].shape[1])
            y2 = int(y2 * depth_map.shape[0] / img_data['resized_array'].shape[0])

            center_x = (x1 + x2) / 2 / width
            center_y = (y1 + y2) / 2 / height
            player_positions.append((center_x, center_y))

    # T√≠nh ƒë·ªô ph√¢n t√°n ng∆∞·ªùi ch∆°i (gi·ªØ nguy√™n)
    player_dispersion = 0
    if len(player_positions) > 1:
        total_distance = 0
        count = 0
        for i in range(len(player_positions)):
            for j in range(i + 1, len(player_positions)):
                p1 = player_positions[i]
                p2 = player_positions[j]
                dist = np.sqrt((p1[0] - p2[0]) ** 2 + (p1[1] - p2[1]) ** 2)
                total_distance += dist
                count += 1
        if count > 0:
            player_dispersion = total_distance / count

    # Ph√¢n t√≠ch ƒë·ªô s·∫Øc n√©t c·ªßa c√°c ƒë·ªëi t∆∞·ª£ng (gi·ªØ nguy√™n)
    image = img_data['resized_array']
    sharpness_scores, sharpness_details = analyze_object_sharpness(image, detections['boxes'])
    print(f"Sharpness scores: {[f'{score:.2f}' for score in sharpness_scores]}")

    # PH·∫¶N C·∫¢I TI·∫æN: X√°c ƒë·ªãnh ƒë·ªëi t∆∞·ª£ng ch√≠nh (key_subjects)
    key_subjects = []

    # T√≠nh k√≠ch th∆∞·ªõc t·ªëi thi·ªÉu (3% di·ªán t√≠ch ·∫£nh)
    min_size_threshold = 0.03 * (img_data['resized_array'].shape[0] * img_data['resized_array'].shape[1])

    # Tr√≠ch xu·∫•t th√¥ng tin m√¥n th·ªÉ thao (n·∫øu c√≥)
    sport_type = "unknown"
    if 'composition_analysis' in img_data and 'sport_type' in img_data['composition_analysis']:
        sport_type = img_data['composition_analysis']['sport_type'].lower()

    for i, box in enumerate(detections['boxes']):
        x1, y1, x2, y2 = box
        area = (x2 - x1) * (y2 - y1)
        area_ratio = area / (img_data['resized_array'].shape[0] * img_data['resized_array'].shape[1])

        # Scale coordinates to match depth map dimensions
        depth_x1 = int(x1 * depth_map.shape[1] / img_data['resized_array'].shape[1])
        depth_y1 = int(y1 * depth_map.shape[0] / img_data['resized_array'].shape[0])
        depth_x2 = int(x2 * depth_map.shape[1] / img_data['resized_array'].shape[1])
        depth_y2 = int(y2 * depth_map.shape[0] / img_data['resized_array'].shape[0])

        # Ensure coordinates are within depth map bounds
        depth_x1 = max(0, min(depth_x1, depth_map.shape[1] - 1))
        depth_y1 = max(0, min(depth_y1, depth_map.shape[0] - 1))
        depth_x2 = max(0, min(depth_x2, depth_map.shape[1] - 1))
        depth_y2 = max(0, min(depth_y2, depth_map.shape[0] - 1))

        # Create mask with depth map dimensions
        mask = np.zeros((depth_map.shape[0], depth_map.shape[1]), dtype=np.uint8)
        if depth_y2 > depth_y1 and depth_x2 > depth_x1:  # Ensure box has valid dimensions
            mask[depth_y1:depth_y2, depth_x1:depth_x2] = 1

        # Calculate average depth
        obj_depth = np.mean(depth_map[mask > 0]) if np.sum(mask) > 0 else 0

        # L·∫•y ƒë·ªô s·∫Øc n√©t
        sharpness = sharpness_scores[i] if i < len(sharpness_scores) else 0

        # T√≠nh v·ªã tr√≠ trung t√¢m v√† kho·∫£ng c√°ch ƒë·∫øn trung t√¢m ·∫£nh
        center_x = (x1 + x2) / 2 / img_data['resized_array'].shape[1]
        center_y = (y1 + y2) / 2 / img_data['resized_array'].shape[0]
        center_dist = np.sqrt((center_x - 0.5) ** 2 + (center_y - 0.5) ** 2)

        # T·∫°o th√¥ng tin ƒë·ªëi t∆∞·ª£ng
        subject_info = {
            'class': detections['classes'][i],
            'box': box,
            'area': area,
            'area_ratio': area_ratio,
            'depth': obj_depth,
            'sharpness': sharpness,
            'sharpness_details': sharpness_details[i] if i < len(sharpness_details) else None,
            'position': (center_x, center_y),
            'center_dist': center_dist
        }

        # C·∫¢I TI·∫æN 1: ƒêi·ªÅu ch·ªânh t·ª∑ tr·ªçng c√°c y·∫øu t·ªë
        # K√≠ch th∆∞·ªõc v√† v·ªã tr√≠ (60% thay v√¨ 40%)
        position_weight = 0.2  # Tr·ªçng s·ªë cho v·ªã tr√≠ trung t√¢m
        size_weight = 0.4  # Tr·ªçng s·ªë cho k√≠ch th∆∞·ªõc

        # Gi·∫£m t·ª∑ tr·ªçng ƒë·ªô s·∫Øc n√©t (20% thay v√¨ 40%)
        sharpness_weight = 0.2

        # Gi·ªØ nguy√™n t·ª∑ tr·ªçng ƒë·ªô s√¢u (20%)
        depth_weight = 0.2

        # T√≠nh ƒëi·ªÉm cho v·ªã tr√≠ (c√†ng g·∫ßn trung t√¢m c√†ng cao)
        position_score = (1 - min(1.0, center_dist * 1.5)) * position_weight

        # T√≠nh ƒëi·ªÉm cho k√≠ch th∆∞·ªõc
        size_score = area_ratio * size_weight

        # T√≠nh ƒëi·ªÉm cho ƒë·ªô s·∫Øc n√©t
        sharpness_score = sharpness * sharpness_weight

        # T√≠nh ƒëi·ªÉm cho ƒë·ªô s√¢u (ƒë·ªëi t∆∞·ª£ng g·∫ßn h∆°n c√≥ ƒëi·ªÉm cao h∆°n)
        depth_score = (1 - obj_depth) * depth_weight

        # C·∫¢I TI·∫æN 2: ∆Øu ti√™n ƒë·ªëi t∆∞·ª£ng ng∆∞·ªùi v√† k√≠ch th∆∞·ªõc l·ªõn
        class_multiplier = 1.0  # M·∫∑c ƒë·ªãnh

        # N·∫øu l√† ng∆∞·ªùi, nh√¢n h·ªá s·ªë l·ªõn (3.5x)
        if detections['classes'][i] == 'person':
            class_multiplier *= 3.5

        # N·∫øu k√≠ch th∆∞·ªõc ƒë·ªëi t∆∞·ª£ng ƒë·ªß l·ªõn (>5% di·ªán t√≠ch ·∫£nh), th√™m ƒëi·ªÉm
        if area_ratio > 0.05:
            class_multiplier *= 1.5

        # L·ªçc k√≠ch th∆∞·ªõc t·ªëi thi·ªÉu (3% di·ªán t√≠ch ·∫£nh)
        if area < min_size_threshold:
            class_multiplier *= 0.5  # Gi·∫£m 50% ƒëi·ªÉm cho ƒë·ªëi t∆∞·ª£ng qu√° nh·ªè

        # C·∫¢I TI·∫æN 3: Logic ƒë·∫∑c th√π cho m√¥n th·ªÉ thao
        sport_bonus = 1.0

        # ƒê·ªëi v·ªõi tr∆∞·ª£t tuy·∫øt, ∆∞u ti√™n ƒë·ªëi t∆∞·ª£ng ·ªü ph√≠a tr∆∞·ªõc/d∆∞·ªõi (y l·ªõn h∆°n)
        if "ski" in sport_type or "snow" in sport_type:
            # ƒê·ªëi t∆∞·ª£ng c√†ng th·∫•p (y l·ªõn) c√†ng c√≥ ∆∞u th·∫ø
            if center_y > 0.6:  # N·∫±m ph√≠a d∆∞·ªõi ·∫£nh
                sport_bonus *= 1.3

        # T·ªïng h·ª£p ƒëi·ªÉm s·ªë v·ªõi tr·ªçng s·ªë m·ªõi v√† c√°c h·ªá s·ªë ƒëi·ªÅu ch·ªânh
        subject_info['prominence'] = (
                                                 position_score + size_score + sharpness_score + depth_score) * class_multiplier * sport_bonus

        # L∆∞u th√¥ng tin t√≠nh to√°n ƒë·ªÉ debug
        subject_info['debug'] = {
            'position_score': position_score,
            'size_score': size_score,
            'sharpness_score': sharpness_score,
            'depth_score': depth_score,
            'class_multiplier': class_multiplier,
            'sport_bonus': sport_bonus
        }

        key_subjects.append(subject_info)

    # S·∫Øp x·∫øp theo prominence
    key_subjects.sort(key=lambda x: x['prominence'], reverse=True)

    # In th√¥ng tin debug cho c√°c ƒë·ªëi t∆∞·ª£ng h√†ng ƒë·∫ßu
    if key_subjects:
        print(f"\nƒê·ªëi t∆∞·ª£ng ch√≠nh: {key_subjects[0]['class']}, Prominence: {key_subjects[0]['prominence']:.3f}")
        print(
            f"K√≠ch th∆∞·ªõc: {key_subjects[0]['area_ratio'] * 100:.1f}% ·∫£nh, ƒê·ªô s·∫Øc n√©t: {key_subjects[0]['sharpness']:.2f}")
        print(f"V·ªã tr√≠: {key_subjects[0]['position']}, Kho·∫£ng c√°ch ƒë·∫øn trung t√¢m: {key_subjects[0]['center_dist']:.2f}")

        if len(key_subjects) > 1:
            print(f"\nƒê·ªëi t∆∞·ª£ng th·ª© 2: {key_subjects[1]['class']}, Prominence: {key_subjects[1]['prominence']:.3f}")
            print(
                f"K√≠ch th∆∞·ªõc: {key_subjects[1]['area_ratio'] * 100:.1f}% ·∫£nh, ƒê·ªô s·∫Øc n√©t: {key_subjects[1]['sharpness']:.2f}")

    # T·∫°o bi·∫øn sports_analysis
    sports_analysis = {
        'player_count': detections['athletes'],
        'player_positions': player_positions,
        'player_dispersion': player_dispersion,
        'key_subjects': key_subjects[:5] if key_subjects else [],
        'sharpness_scores': sharpness_scores
    }

    # Ph√¢n ƒëo·∫°n main subject n·∫øu t√¨m th·∫•y v√† l√† ng∆∞·ªùi
    if key_subjects and key_subjects[0]['class'] == 'person':
        main_subject_box = key_subjects[0]['box']
        print("Th·ª±c hi·ªán ph√¢n ƒëo·∫°n main subject...")
        main_subject_mask = segment_main_subject(img_data['resized_array'], yolo_seg, main_subject_box)
        if main_subject_mask is not None:
            print(f"ƒê√£ t√¨m th·∫•y mask cho main subject, k√≠ch th∆∞·ªõc: {main_subject_mask.shape}")
        else:
            print("Kh√¥ng t√¨m ƒë∆∞·ª£c mask ph√π h·ª£p cho main subject")

        # L∆∞u mask v√†o k·∫øt qu·∫£ ph√¢n t√≠ch
        sports_analysis['main_subject_mask'] = main_subject_mask

    return sports_analysis


def analyze_action_quality(detections, img_data):
    """Analyze action quality in sports image with improved approach"""
    height, width = img_data['resized_array'].shape[:2]

    # 1. Check for sports equipment
    has_equipment = False
    equipment_types = []
    for cls in detections['classes']:
        if cls in ['sports ball', 'tennis racket', 'baseball bat', 'baseball glove',
                   'skateboard', 'surfboard', 'tennis ball', 'frisbee', 'skis', 'snowboard']:
            has_equipment = True
            if cls not in equipment_types:
                equipment_types.append(cls)

    # 2. Analyze individual posture instead of comparing multiple people
    action_posture_score = 0
    dynamic_posture_count = 0
    total_players = 0

    for i, cls in enumerate(detections['classes']):
        if cls == 'person':
            total_players += 1
            x1, y1, x2, y2 = detections['boxes'][i]

            # a. Calculate height/width ratio
            aspect_ratio = (y2 - y1) / (x2 - x1) if (x2 - x1) > 0 else 0

            # b. Evaluate posture based on aspect ratio
            # Non-typical posture (jumping, crouching, lying...)
            if aspect_ratio < 1.2 or aspect_ratio > 2.5:
                dynamic_posture_count += 1

            # c. Calculate relative area (larger = closer action)
            area_ratio = ((y2 - y1) * (x2 - x1)) / (height * width)
            if area_ratio > 0.2:  # Athlete takes up large area, often close action
                action_posture_score += 0.2

    # If there are people in non-typical postures, that could be dynamic action
    if total_players > 0:
        dynamic_posture_ratio = dynamic_posture_count / total_players
        action_posture_score += dynamic_posture_ratio * 0.5

    # 3. Calculate improved action_level
    action_level = 0

    # If there's sports equipment
    if has_equipment:
        action_level += 0.4

    # Add points from posture analysis
    action_level += min(0.6, action_posture_score)

    # 4. Classify action quality
    return {
        'has_equipment': has_equipment,
        'equipment_types': equipment_types,
        'dynamic_posture_score': action_posture_score,
        'dynamic_posture_count': dynamic_posture_count,
        'total_players': total_players,
        'action_level': action_level,
        'action_quality': "High" if action_level > 0.7 else
        "Medium" if action_level > 0.3 else "Low"
    }


def verify_face(face_img):
    """Ph√°t hi·ªán khu√¥n m·∫∑t kh√¥ng h·ª£p l·ªá v·ªõi c√°c ti√™u ch√≠ nghi√™m ng·∫∑t h∆°n"""
    try:
        # 1. Ki·ªÉm tra k√≠ch th∆∞·ªõc t·ªëi thi·ªÉu
        if face_img.shape[0] < 20 or face_img.shape[1] < 20:
            return False, "Khu√¥n m·∫∑t qu√° nh·ªè (nh·ªè h∆°n 20px)"

        # 2. Ki·ªÉm tra t·ª∑ l·ªá khung h√¨nh
        h, w = face_img.shape[:2]
        aspect_ratio = h / w
        if aspect_ratio < 0.5 or aspect_ratio > 2.0:
            return False, "T·ª∑ l·ªá khu√¥n m·∫∑t b·∫•t th∆∞·ªùng"

        # 3. Ki·ªÉm tra ƒë·ªô t∆∞∆°ng ph·∫£n - khu√¥n m·∫∑t th·ª±c ph·∫£i c√≥ ƒë·ªô t∆∞∆°ng ph·∫£n
        gray = cv2.cvtColor(face_img, cv2.COLOR_RGB2GRAY)
        std_dev = np.std(gray)
        if std_dev < 25:  # TƒÉng ng∆∞·ª°ng (tr∆∞·ªõc l√† 10)
            return False, "ƒê·ªô t∆∞∆°ng ph·∫£n qu√° th·∫•p, c√≥ th·ªÉ kh√¥ng ph·∫£i khu√¥n m·∫∑t"

        # 4. M·ªöI: Ki·ªÉm tra k·∫øt c·∫•u khu√¥n m·∫∑t s·ª≠ d·ª•ng b·ªô l·ªçc Sobel
        sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
        sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
        sobel_mag = np.sqrt(sobelx ** 2 + sobely ** 2)
        avg_edge_strength = np.mean(sobel_mag)

        # Khu√¥n m·∫∑t th·ª±c ph·∫£i c√≥ c·∫°nh r√µ r√†ng (m·∫Øt, m≈©i, mi·ªáng)
        if avg_edge_strength < 10.0:
            return False, "Kh√¥ng ph√°t hi·ªán ƒë·∫∑c ƒëi·ªÉm khu√¥n m·∫∑t r√µ r√†ng"

        # 5. M·ªöI: Ki·ªÉm tra v√πng m·∫Øt - khu√¥n m·∫∑t th·∫≠t ph·∫£i c√≥ v√πng m·∫Øt c√≥ ƒë·ªô t∆∞∆°ng ph·∫£n
        eye_region_h = int(h * 0.3)
        eye_region = gray[:eye_region_h, :]
        eye_std = np.std(eye_region)

        if eye_std < 20:
            return False, "Kh√¥ng ph√°t hi·ªán v√πng m·∫Øt r√µ r√†ng"

        # 6. M·ªöI: Ki·ªÉm tra h∆∞·ªõng khu√¥n m·∫∑t b·∫±ng c√°ch t√≠nh to√°n ph√¢n ph·ªëi gradient
        # N·∫øu khu√¥n m·∫∑t quay l∆∞ng, gradient s·∫Ω kh√¥ng ƒë·ªìng ƒë·ªÅu
        gradient_y_ratio = np.mean(np.abs(sobely)) / (np.mean(np.abs(sobelx)) + 1e-5)
        if gradient_y_ratio < 0.5:
            return False, "C√≥ th·ªÉ khu√¥n m·∫∑t ƒëang quay ƒëi"

        return True, "Khu√¥n m·∫∑t h·ª£p l·ªá"

    except Exception as e:
        return False, f"L·ªói x√°c th·ª±c: {str(e)}"


def analyze_sports_environment(img_data, depth_map=None):
    # Ki·ªÉm tra xem c√≥ action boxing n√†o kh√¥ng
    if 'detected_actions' in img_data:
        for action in img_data['detected_actions']:
            boxing_actions = ['straight_punch', 'left_hook', 'right_hook', 'uppercut', 'body_shot', 'defensive_guard']
            if action['action'] in boxing_actions and action.get('confidence', 0) > 0.5:
                print(f"üí• FORCE BOXING t·ª´ analyze_sports_environment: {action['action']}")
                return {'sport_type': 'Boxing', 'confidence': 0.99,
                        'sport_type_source': f'boxing_action_{action["action"]}',
                        'environment_indicators': {'boxing_ring': 0.8}}
    # L·∫•y ·∫£nh ƒë√£ resize ƒë·ªÉ ph√¢n t√≠ch
    image = img_data['resized_array']
    height, width = image.shape[:2]

    # K·∫øt qu·∫£ ch·ª©a x√°c su·∫•t c√°c m√¥n th·ªÉ thao
    env_results = {
        'detected_environments': [],
        'sport_probabilities': {},
        'dominant_colors': [],
        'surface_type': 'unknown',
        'confidence': 0.0
    }

    # Ph√¢n t√≠ch m√†u s·∫Øc ƒë·∫∑c tr∆∞ng
    # Chuy·ªÉn sang kh√¥ng gian m√†u HSV ƒë·ªÉ ph√¢n t√≠ch t·ªët h∆°n
    hsv_img = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)

    # 1. Ph√°t hi·ªán m√†u s·∫Øc ƒë·∫∑c tr∆∞ng
    # T·∫°o mask cho t·ª´ng v√πng m√†u quan tr·ªçng

    # Xanh n∆∞·ªõc (b∆°i l·ªôi)
    lower_blue = np.array([90, 50, 50])
    upper_blue = np.array([130, 255, 255])
    blue_mask = cv2.inRange(hsv_img, lower_blue, upper_blue)
    blue_ratio = np.sum(blue_mask > 0) / (height * width)

    # Xanh c·ªè (s√¢n c·ªè - b√≥ng ƒë√°, ƒëi·ªÅn kinh)
    lower_green = np.array([35, 50, 50])
    upper_green = np.array([85, 255, 255])
    green_mask = cv2.inRange(hsv_img, lower_green, upper_green)
    green_ratio = np.sum(green_mask > 0) / (height * width)

    # ƒê·ªè/n√¢u (s√¢n ƒë·∫•t n·ªán - tennis, ƒëi·ªÅn kinh)
    lower_red1 = np.array([0, 70, 50])
    upper_red1 = np.array([10, 255, 255])
    lower_red2 = np.array([170, 70, 50])
    upper_red2 = np.array([180, 255, 255])
    red_mask1 = cv2.inRange(hsv_img, lower_red1, upper_red1)
    red_mask2 = cv2.inRange(hsv_img, lower_red2, upper_red2)
    red_mask = red_mask1 | red_mask2
    red_ratio = np.sum(red_mask > 0) / (height * width)

    # M√†u ƒëen/x√°m ƒë·∫≠m (ƒë∆∞·ªùng ch·∫°y nh·ª±a)
    lower_dark = np.array([0, 0, 0])
    upper_dark = np.array([180, 30, 80])
    dark_mask = cv2.inRange(hsv_img, lower_dark, upper_dark)
    dark_ratio = np.sum(dark_mask > 0) / (height * width)

    # M√†u tr·∫Øng (s√¢n v√µ thu·∫≠t, s√†n boxing)
    lower_white = np.array([0, 0, 200])
    upper_white = np.array([180, 30, 255])
    white_mask = cv2.inRange(hsv_img, lower_white, upper_white)
    white_ratio = np.sum(white_mask > 0) / (height * width)

    # L∆∞u t·ª∑ l·ªá m√†u ch√≠nh
    color_ratios = {
        'blue': blue_ratio,
        'green': green_ratio,
        'red': red_ratio,
        'dark': dark_ratio,
        'white': white_ratio
    }

    # L·∫•y 2 m√†u chi·∫øm t·ª∑ l·ªá cao nh·∫•t
    dominant_colors = sorted(color_ratios.items(), key=lambda x: x[1], reverse=True)[:2]
    env_results['dominant_colors'] = dominant_colors

    # 2. Ph√¢n t√≠ch k·∫øt c·∫•u s√¢n ƒë·∫•u

    # Chuy·ªÉn sang grayscale cho ph√¢n t√≠ch k·∫øt c·∫•u
    gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)

    # Ph√°t hi·ªán c·∫°nh v·ªõi Canny
    edges = cv2.Canny(gray, 50, 150)

    # Ph√°t hi·ªán ƒë∆∞·ªùng th·∫≥ng v·ªõi Hough Transform
    lines = cv2.HoughLinesP(edges, 1, np.pi / 180, threshold=100, minLineLength=100, maxLineGap=10)

    # ƒê·∫øm s·ªë ƒë∆∞·ªùng th·∫≥ng ngang v√† d·ªçc
    horizontal_lines = 0
    vertical_lines = 0

    if lines is not None:
        for line in lines:
            x1, y1, x2, y2 = line[0]
            # T√≠nh g√≥c c·ªßa ƒë∆∞·ªùng
            angle = np.abs(np.arctan2(y2 - y1, x2 - x1) * 180 / np.pi)

            # Ph√¢n lo·∫°i d·ª±a tr√™n g√≥c
            if angle < 30 or angle > 150:
                horizontal_lines += 1
            elif angle > 60 and angle < 120:
                vertical_lines += 1

    # 3. Suy ƒëo√°n m√¥i tr∆∞·ªùng th·ªÉ thao d·ª±a tr√™n c√°c ƒë·∫∑c ƒëi·ªÉm
    sport_probs = {}

    # B∆°i l·ªôi
    if blue_ratio > 0.3 and horizontal_lines >= 3:
        sport_probs['Swimming'] = min(1.0, blue_ratio * 1.5)
        env_results['detected_environments'].append('swimming pool')
        env_results['surface_type'] = 'water'

    # S√¢n c·ªè (b√≥ng ƒë√°, rugby)
    if green_ratio > 0.4:
        sport_probs['Soccer'] = min(1.0, green_ratio * 1.2)
        sport_probs['Rugby'] = min(1.0, green_ratio * 1.0)
        env_results['detected_environments'].append('grass field')
        env_results['surface_type'] = 'grass'

    # S√¢n tennis ƒë·∫•t n·ªán
    if red_ratio > 0.3 and horizontal_lines >= 2 and vertical_lines >= 2:
        sport_probs['Tennis'] = min(1.0, red_ratio * 1.3)
        env_results['detected_environments'].append('clay court')
        env_results['surface_type'] = 'clay'

    # ƒê∆∞·ªùng ch·∫°y ƒëi·ªÅn kinh
    if dark_ratio > 0.2 and horizontal_lines >= 3 and vertical_lines <= 2:
        sport_probs['Track and Field'] = min(1.0, dark_ratio * 1.2)
        sport_probs['Running'] = min(1.0, dark_ratio * 1.3)
        env_results['detected_environments'].append('running track')
        env_results['surface_type'] = 'track'

    # S√†n ƒë·∫•u Boxing/UFC/v√µ thu·∫≠t
    if white_ratio > 0.3 and dark_ratio < 0.3:
        if horizontal_lines <= 3 and vertical_lines <= 3:
            env_canvas_ratio = 0.0
            if depth_map is not None:
                # D√πng depth map ƒë·ªÉ x√°c ƒë·ªãnh ph·∫ßn s√†n ƒë·∫•u ƒë∆∞·ª£c n√¢ng cao
                # ƒê√¢y l√† logic ƒë∆°n gi·∫£n, c√≥ th·ªÉ c·∫£i ti·∫øn th√™m
                center_depth = depth_map[height // 3:2 * height // 3, width // 3:2 * width // 3]
                border_depth = np.concatenate([
                    depth_map[:height // 3, :],  # Tr√™n
                    depth_map[2 * height // 3:, :],  # D∆∞·ªõi
                    depth_map[height // 3:2 * height // 3, :width // 3],  # Tr√°i
                    depth_map[height // 3:2 * height // 3, 2 * width // 3:]  # Ph·∫£i
                ])

                if np.mean(center_depth) < np.mean(border_depth):
                    env_canvas_ratio = 0.3

            sport_probs['Boxing'] = min(1.0, white_ratio * 0.8 + env_canvas_ratio)
            sport_probs['Martial Arts'] = min(1.0, white_ratio * 0.7 + env_canvas_ratio)
            env_results['detected_environments'].append('fighting ring/mat')
            env_results['surface_type'] = 'canvas'

    # S√¢n b√≥ng r·ªï
    if (dark_ratio > 0.2 or red_ratio > 0.2) and horizontal_lines >= 2 and vertical_lines >= 2:
        court_prob = max(dark_ratio, red_ratio) * 0.8
        sport_probs['Basketball'] = min(1.0, court_prob)
        env_results['detected_environments'].append('basketball court')
        env_results['surface_type'] = 'court'

    env_results['sport_probabilities'] = sport_probs

    # T√≠nh m·ª©c ƒë·ªô tin c·∫≠y t·ªïng th·ªÉ
    if sport_probs:
        max_prob = max(sport_probs.values())
        env_results['confidence'] = max_prob

    return env_results

def detect_human_pose(img_data, conf_threshold=0.15, main_subject_box=None):
    print("DEBUG - B·∫Øt ƒë·∫ßu ph√°t hi·ªán pose")
    """
    S·ª≠ d·ª•ng YOLOv8-Pose ƒë·ªÉ ph√°t hi·ªán c√°c keypoint tr√™n c∆° th·ªÉ ng∆∞·ªùi

    Args:
        img_data: Dict ch·ª©a ·∫£nh g·ªëc v√† ·∫£nh ƒë√£ resize
        conf_threshold: Ng∆∞·ª°ng confidence cho vi·ªác ph√°t hi·ªán

    Returns:
        Dict: Th√¥ng tin v·ªÅ pose c√°c ng∆∞·ªùi ƒë∆∞·ª£c ph√°t hi·ªán
    """
    if not POSE_MODEL_AVAILABLE:
        return {"poses": []}

    # T·∫°o c·∫•u tr√∫c k·∫øt qu·∫£
    pose_results = {
        "poses": [],
        "keypoint_names": {
            0: "nose", 1: "left_eye", 2: "right_eye", 3: "left_ear", 4: "right_ear",
            5: "left_shoulder", 6: "right_shoulder", 7: "left_elbow", 8: "right_elbow",
            9: "left_wrist", 10: "right_wrist", 11: "left_hip", 12: "right_hip",
            13: "left_knee", 14: "right_knee", 15: "left_ankle", 16: "right_ankle"
        }
    }

    # Load model (ch·ªâ t·∫£i m·ªôt l·∫ßn)
    if not hasattr(detect_human_pose, 'model'):
        print("ƒêang t·∫£i model YOLOv8-Pose...")
        detect_human_pose.model = YOLO('yolov8x-pose-p6.pt')  # model nh·ªè
        print("ƒê√£ t·∫£i model YOLOv8-Pose th√†nh c√¥ng")

    # D·ª± ƒëo√°n tr√™n ·∫£nh
    results = detect_human_pose.model(img_data['resized_array'])

    # Ch·ªâ l·∫•y k·∫øt qu·∫£ ƒë·∫ßu ti√™n
    for result in results:
        if hasattr(result, 'keypoints') and result.keypoints is not None:
            keypoints = result.keypoints.data
            for person_id, person_keypoints in enumerate(keypoints):
                # T·∫°o dict ch·ª©a th√¥ng tin pose c·ªßa m·ªói ng∆∞·ªùi
                person_pose = {
                    "person_id": person_id,
                    "keypoints": [],
                    "bbox": None
                }
                for kp_id, kp in enumerate(person_keypoints):
                    x, y, conf = kp.tolist()
                    # S·ª≠ d·ª•ng threshold linh ho·∫°t cho c√°c keypoints quan tr·ªçng
                    dynamic_threshold = conf_threshold
                    if kp_id in [5, 6, 11, 12, 13, 14, 15, 16]:  # Keypoints quan tr·ªçng cho th·ªÉ thao
                        dynamic_threshold = max(0.08, conf_threshold - 0.05)

                    if conf >= dynamic_threshold:
                        person_pose["keypoints"].append({
                            "id": kp_id,
                            "name": pose_results["keypoint_names"].get(kp_id, f"kp_{kp_id}"),
                            "x": float(x),
                            "y": float(y),
                            "confidence": float(conf)
                        })

                # T√≠nh bounding box t·ª´ keypoints
                if person_pose["keypoints"]:
                    kp_coords = [(kp["x"], kp["y"]) for kp in person_pose["keypoints"]]
                    x_min = min([x for x, _ in kp_coords])
                    y_min = min([y for _, y in kp_coords])
                    x_max = max([x for x, _ in kp_coords])
                    y_max = max([y for _, y in kp_coords])
                    person_pose["bbox"] = [x_min, y_min, x_max, y_max]

                # Th√™m v√†o k·∫øt qu·∫£
                if person_pose["keypoints"]:
                    pose_results["poses"].append(person_pose)


    print(f"DEBUG - pose_results c√≥ s·ªë poses: {len(pose_results.get('poses', []))}")
    return pose_results

def detect_sports_actions(pose_data, sport_type, image_shape, detected_equipment=None, environment_sport=None):
    """
    Ph√°t hi·ªán h√†nh ƒë·ªông th·ªÉ thao c·ª• th·ªÉ d·ª±a tr√™n pose keypoints v·ªõi equipment filtering

    Args:
        pose_data: D·ªØ li·ªáu pose t·ª´ detect_human_pose
        sport_type: Lo·∫°i th·ªÉ thao ƒë∆∞·ª£c ph√°t hi·ªán
        image_shape: K√≠ch th∆∞·ªõc ·∫£nh (height, width)
        detected_equipment: List c√°c equipment ƒë√£ ph√°t hi·ªán
        environment_sport: Sport type t·ª´ environment analysis

    Returns:
        Dict: Th√¥ng tin v·ªÅ h√†nh ƒë·ªông ƒë∆∞·ª£c ph√°t hi·ªán
    """
    if not pose_data or 'poses' not in pose_data or not pose_data['poses']:
        return {'detected_actions': [], 'confidence': 0.0, 'details': 'No pose data available'}
    # EQUIPMENT FILTERING LOGIC
    equipment_to_sport = {
        'tennis racket': 'tennis',
        'tennis ball': 'tennis',
        'soccer ball': 'soccer',
        'basketball': 'basketball',
        'volleyball': 'volleyball',
        'baseball bat': 'baseball',
        'baseball glove': 'baseball',
        'baseball': 'baseball',
        'american football': 'football',
        'golf ball': 'golf',
        'golf club': 'golf',
        'boxing gloves': 'boxing',
        'ping pong ball': 'table tennis',
        'badminton racket': 'badminton',
        'skis': 'skiing',
        'snowboard': 'snowboarding',
        'surfboard': 'surfing',
        'skateboard': 'skateboarding',
        'frisbee': 'frisbee'
    }

    # X√°c ƒë·ªãnh sport t·ª´ equipment
    equipment_detected_sport = None
    if detected_equipment:
        for equipment in detected_equipment:
            if equipment.lower() in equipment_to_sport:
                equipment_detected_sport = equipment_to_sport[equipment.lower()]
                print(f"Equipment-based sport detection: {equipment} -> {equipment_detected_sport}")
                break

    # Ki·ªÉm tra swimming t·ª´ environment (ƒë·∫∑c bi·ªát)
    is_swimming_environment = False
    if environment_sport and 'swimming' in environment_sport.lower():
        is_swimming_environment = True
        print("Swimming detected from environment analysis")

    # FILTERING RULES - S·ª¨A L·∫†I:
    # 1. N·∫øu c√≥ equipment, ch·ªâ detect action c·ªßa sport ƒë√≥
    # 2. N·∫øu environment detect swimming, th√™m swimming v√†o allowed
    # 3. N·∫øu kh√¥ng c√≥ equipment, cho ph√©p sports kh√¥ng c·∫ßn equipment + swimming n·∫øu c√≥

    allowed_sports = []

    if equipment_detected_sport:
        # Ch·ªâ cho ph√©p sport t·ª´ equipment
        allowed_sports = [equipment_detected_sport]
        # NH∆ØNG v·∫´n cho ph√©p swimming n·∫øu environment detect
        if is_swimming_environment:
            allowed_sports.append('swimming')
        print(f"Action detection limited to: {allowed_sports}")
    else:
        # Kh√¥ng c√≥ equipment - cho ph√©p sports kh√¥ng c·∫ßn equipment c·ª• th·ªÉ
        allowed_sports = ['soccer', 'basketball', 'volleyball', 'running', 'track', 'boxing', 'martial arts']
        # LU√îN LU√îN th√™m swimming n·∫øu environment detect
        if is_swimming_environment:
            allowed_sports.append('swimming')
        print(f"No equipment detected - allowed sports: {allowed_sports}")

    height, width = image_shape[:2]
    detected_actions = []

    # L·∫•y pose ch√≠nh (th∆∞·ªùng l√† pose ƒë·∫ßu ti√™n ho·∫∑c c√≥ nhi·ªÅu keypoints nh·∫•t)
    main_pose = None
    max_keypoints = 0

    for pose in pose_data['poses']:
        if len(pose.get('keypoints', [])) > max_keypoints:
            max_keypoints = len(pose.get('keypoints', []))
            main_pose = pose

    if not main_pose or not main_pose.get('keypoints'):
        return {'detected_actions': [], 'confidence': 0.0, 'details': 'No valid pose found'}

    # T·∫°o dict keypoints ƒë·ªÉ d·ªÖ truy c·∫≠p
    kp_dict = {}
    for kp in main_pose['keypoints']:
        if kp['confidence'] > 0.3:  # Ch·ªâ l·∫•y keypoints c√≥ ƒë·ªô tin c·∫≠y cao
            kp_dict[kp['id']] = {
                'x': kp['x'],
                'y': kp['y'],
                'confidence': kp['confidence'],
                'name': kp['name']
            }

    # ƒê·ªãnh nghƒ©a keypoints theo COCO format
    # 0: nose, 1: left_eye, 2: right_eye, 3: left_ear, 4: right_ear,
    # 5: left_shoulder, 6: right_shoulder, 7: left_elbow, 8: right_elbow,
    # 9: left_wrist, 10: right_wrist, 11: left_hip, 12: right_hip,
    # 13: left_knee, 14: right_knee, 15: left_ankle, 16: right_ankle

    def calculate_angle(p1, p2, p3):
        """T√≠nh g√≥c t·∫°i ƒëi·ªÉm p2 gi·ªØa p1-p2-p3"""
        try:
            import math
            v1 = (p1['x'] - p2['x'], p1['y'] - p2['y'])
            v2 = (p3['x'] - p2['x'], p3['y'] - p2['y'])

            dot_product = v1[0] * v2[0] + v1[1] * v2[1]
            mag1 = math.sqrt(v1[0] ** 2 + v1[1] ** 2)
            mag2 = math.sqrt(v2[0] ** 2 + v2[1] ** 2)

            if mag1 == 0 or mag2 == 0:
                return 0

            cos_angle = dot_product / (mag1 * mag2)
            cos_angle = max(-1, min(1, cos_angle))  # Clamp to [-1, 1]
            angle = math.acos(cos_angle) * 180 / math.pi
            return angle
        except:
            return 0

    def get_body_orientation():
        """X√°c ƒë·ªãnh h∆∞·ªõng c·ªßa c∆° th·ªÉ (tr∆∞·ªõc/sau/tr√°i/ph·∫£i)"""
        orientation = "unknown"

        # Ki·ªÉm tra vai
        if 5 in kp_dict and 6 in kp_dict:  # left_shoulder, right_shoulder
            left_shoulder = kp_dict[5]
            right_shoulder = kp_dict[6]
            shoulder_width = abs(left_shoulder['x'] - right_shoulder['x'])

            # N·∫øu vai r·ªông -> nh√¨n tr∆∞·ªõc/sau
            # N·∫øu vai h·∫πp -> nh√¨n nghi√™ng
            if shoulder_width > width * 0.15:  # Vai r·ªông
                # Ki·ªÉm tra m≈©i ƒë·ªÉ x√°c ƒë·ªãnh tr∆∞·ªõc/sau
                if 0 in kp_dict:  # nose
                    nose = kp_dict[0]
                    shoulder_center_x = (left_shoulder['x'] + right_shoulder['x']) / 2
                    nose_offset = abs(nose['x'] - shoulder_center_x)

                    if nose_offset < width * 0.05:  # M≈©i ·ªü gi·ªØa
                        orientation = "front"
                    else:
                        orientation = "front_angled"
                else:
                    orientation = "front"
            else:  # Vai h·∫πp - nh√¨n nghi√™ng
                # X√°c ƒë·ªãnh tr√°i/ph·∫£i d·ª±a tr√™n v·ªã tr√≠ vai
                if left_shoulder['x'] < right_shoulder['x']:
                    orientation = "left_side"
                else:
                    orientation = "right_side"

        return orientation

    # X√°c ƒë·ªãnh h∆∞·ªõng c∆° th·ªÉ
    body_orientation = get_body_orientation()

    # PH√ÇN T√çCH H√ÄNH ƒê·ªòNG THEO T·ª™NG M√îN TH·ªÇ THAO
    sport_lower = sport_type.lower()

    # ==================== B√ìNG ƒê√Å (SOCCER) ====================
    if ('soccer' in sport_lower or 'football' in sport_lower) and 'soccer' in allowed_sports:
        action_confidence = 0.0

        # 1. PRE-KICK STANCE - T∆∞ th·∫ø chu·∫©n b·ªã ƒë√° b√≥ng
        def detect_pre_kick_stance():
            confidence = 0.0
            details = []

            # Ki·ªÉm tra ch√¢n tr·ª• v√† ch√¢n ƒë√°
            if (11 in kp_dict and 12 in kp_dict and 13 in kp_dict and 14 in kp_dict and
                    15 in kp_dict and 16 in kp_dict):

                left_hip = kp_dict[11]
                right_hip = kp_dict[12]
                left_knee = kp_dict[13]
                right_knee = kp_dict[14]
                left_ankle = kp_dict[15]
                right_ankle = kp_dict[16]

                # T√≠nh kho·∫£ng c√°ch gi·ªØa hai ch√¢n (stance width)
                ankle_distance = abs(left_ankle['x'] - right_ankle['x'])
                hip_width = abs(left_hip['x'] - right_hip['x'])

                # Stance r·ªông h∆°n b√¨nh th∆∞·ªùng (chu·∫©n b·ªã ƒë√°)
                wide_stance = ankle_distance > hip_width * 1.5

                # Ki·ªÉm tra tr·ªçng t√¢m nghi√™ng (weight shift)
                # Ch√¢n tr·ª• th·∫≥ng, ch√¢n ƒë√° h∆°i g·∫•p
                left_knee_angle = calculate_angle(left_hip, left_knee, left_ankle)
                right_knee_angle = calculate_angle(right_hip, right_knee, right_ankle)

                # M·ªôt ch√¢n th·∫≥ng (supporting leg), m·ªôt ch√¢n h∆°i g·∫•p (kicking leg)
                straight_leg = left_knee_angle > 160 or right_knee_angle > 160
                bent_leg = left_knee_angle < 140 or right_knee_angle < 140

                # Ch√¢n ƒë√° h∆°i n√¢ng ho·∫∑c l√πi v·ªÅ ph√≠a sau
                if left_knee_angle < right_knee_angle:  # Left leg is kicking leg
                    kicking_leg_back = left_ankle['x'] < right_ankle['x'] - width * 0.05
                    kicking_leg_lifted = left_ankle['y'] < right_ankle['y']
                    kicking_side = "left"
                else:  # Right leg is kicking leg
                    kicking_leg_back = right_ankle['x'] > left_ankle['x'] + width * 0.05
                    kicking_leg_lifted = right_ankle['y'] < left_ankle['y']
                    kicking_side = "right"

                # T√≠nh ƒëi·ªÉm confidence
                score = 0.0
                if wide_stance:
                    score += 0.3
                    details.append("wide stance detected")

                if straight_leg and bent_leg:
                    score += 0.4
                    details.append("supporting leg positioned")

                if kicking_leg_back or kicking_leg_lifted:
                    score += 0.3
                    details.append(f"{kicking_side} leg in pre-kick position")

                confidence = min(0.85, score)

            return confidence, details

        # 2. SHOOTING/KICKING - ƒêang th·ª±c hi·ªán c√∫ ƒë√°
        def detect_shooting():
            confidence = 0.0
            details = []

            if 13 in kp_dict and 15 in kp_dict and 11 in kp_dict:  # left_knee, left_ankle, left_hip
                left_knee = kp_dict[13]
                left_ankle = kp_dict[15]
                left_hip = kp_dict[11]

                # T√≠nh g√≥c ch√¢n tr√°i
                knee_angle = calculate_angle(left_hip, left_knee, left_ankle)

                # Ki·ªÉm tra ch√¢n c√≥ ƒëang du·ªói kh√¥ng (shooting motion)
                if knee_angle > 140:  # Ch√¢n du·ªói
                    # Ki·ªÉm tra ƒë·ªô cao c·ªßa ch√¢n
                    if left_ankle['y'] < left_hip['y']:  # Ch√¢n n√¢ng cao
                        confidence = 0.8
                        details.append(f'Left leg extended for shot (angle: {knee_angle:.1f}¬∞)')

            # Ki·ªÉm tra ch√¢n ph·∫£i t∆∞∆°ng t·ª±
            if 14 in kp_dict and 16 in kp_dict and 12 in kp_dict:  # right_knee, right_ankle, right_hip
                right_knee = kp_dict[14]
                right_ankle = kp_dict[16]
                right_hip = kp_dict[12]

                knee_angle = calculate_angle(right_hip, right_knee, right_ankle)

                if knee_angle > 140 and right_ankle['y'] < right_hip['y']:
                    confidence = max(confidence, 0.8)
                    details.append(f'Right leg extended for shot (angle: {knee_angle:.1f}¬∞)')

            return confidence, details

        # 3. APPROACH RUN - Ch·∫°y t·ªõi ƒë·ªÉ ƒë√° b√≥ng
        def detect_approach_run():
            confidence = 0.0
            details = []

            if (13 in kp_dict and 14 in kp_dict and 15 in kp_dict and 16 in kp_dict and
                    5 in kp_dict and 11 in kp_dict):

                left_knee = kp_dict[13]
                right_knee = kp_dict[14]
                shoulder = kp_dict[5]
                hip = kp_dict[11]

                # Ki·ªÉm tra b∆∞·ªõc ch·∫°y (m·ªôt ch√¢n n√¢ng cao)
                ground_level = height * 0.9
                high_knee = (left_knee['y'] < ground_level * 0.8 or
                             right_knee['y'] < ground_level * 0.8)

                # C∆° th·ªÉ nghi√™ng v·ªÅ ph√≠a tr∆∞·ªõc (running lean)
                forward_lean = shoulder['y'] < hip['y'] - height * 0.02

                if high_knee and forward_lean:
                    confidence = 0.7
                    details.append("Running approach with forward lean")

            return confidence, details

        # 4. DRIBBLING - Gi·ªØ b√≥ng, di chuy·ªÉn
        def detect_dribbling():
            confidence = 0.0
            details = []

            if (13 in kp_dict and 15 in kp_dict and 14 in kp_dict and 16 in kp_dict and
                    5 in kp_dict and 11 in kp_dict):

                left_ankle = kp_dict[15]
                right_ankle = kp_dict[16]
                shoulder = kp_dict[5]
                hip = kp_dict[11]

                # C·∫£ hai ch√¢n g·∫ßn ƒë·∫•t (controlling stance)
                ground_level = height * 0.9
                both_feet_low = (left_ankle['y'] > ground_level * 0.85 and
                                 right_ankle['y'] > ground_level * 0.85)

                # C∆° th·ªÉ h∆°i nghi√™ng ƒë·ªÉ ki·ªÉm so√°t b√≥ng
                slight_lean = abs(shoulder['y'] - hip['y']) < height * 0.08

                if both_feet_low and slight_lean:
                    confidence = 0.7
                    details.append('Low stance with controlled ball movement')

            return confidence, details

        # Ch·∫°y t·∫•t c·∫£ detections
        pre_kick_conf, pre_kick_details = detect_pre_kick_stance()
        shooting_conf, shooting_details = detect_shooting()
        approach_conf, approach_details = detect_approach_run()
        dribbling_conf, dribbling_details = detect_dribbling()

        # ∆Øu ti√™n theo th·ª© t·ª±: shooting > pre_kick > approach > dribbling
        if shooting_conf > 0.6:
            action_confidence = shooting_conf
            detected_actions.append({
                'action': 'shooting',
                'confidence': shooting_conf,
                'details': '; '.join(shooting_details),
                'body_part': 'legs'
            })
        elif pre_kick_conf > 0.5:
            action_confidence = pre_kick_conf
            detected_actions.append({
                'action': 'pre_kick_stance',
                'confidence': pre_kick_conf,
                'details': '; '.join(pre_kick_details),
                'body_part': 'full_body'
            })
        elif approach_conf > 0.5:
            action_confidence = approach_conf
            detected_actions.append({
                'action': 'approach_run',
                'confidence': approach_conf,
                'details': '; '.join(approach_details),
                'body_part': 'full_body'
            })
        elif dribbling_conf > 0.5:
            action_confidence = dribbling_conf
            detected_actions.append({
                'action': 'dribbling',
                'confidence': dribbling_conf,
                'details': '; '.join(dribbling_details),
                'body_part': 'full_body'
            })

    # ==================== B√ìNG R·ªî (BASKETBALL) ====================
    elif 'basketball' in sport_lower and 'basketball' in allowed_sports:
        action_confidence = 0.0

        # 1. SHOOTING - Tay n√¢ng cao, khu·ª∑u tay g·∫•p
        if (9 in kp_dict and 10 in kp_dict and 7 in kp_dict and 8 in kp_dict and
                5 in kp_dict and 6 in kp_dict):

            left_wrist = kp_dict[9]
            right_wrist = kp_dict[10]
            left_elbow = kp_dict[7]
            right_elbow = kp_dict[8]
            left_shoulder = kp_dict[5]
            right_shoulder = kp_dict[6]

            # Ki·ªÉm tra tay c√≥ n√¢ng cao kh√¥ng
            avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2
            hands_raised = (left_wrist['y'] < avg_shoulder_y or right_wrist['y'] < avg_shoulder_y)

            if hands_raised:
                # T√≠nh g√≥c khu·ª∑u tay
                left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)
                right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)

                # G√≥c khu·ª∑u trong kho·∫£ng shooting (60-120 ƒë·ªô)
                shooting_angles = (60 <= left_elbow_angle <= 120 or 60 <= right_elbow_angle <= 120)

                if shooting_angles:
                    action_confidence = 0.85
                    detected_actions.append({
                        'action': 'shooting',
                        'confidence': action_confidence,
                        'details': f'Arms raised for shot (L: {left_elbow_angle:.1f}¬∞, R: {right_elbow_angle:.1f}¬∞)',
                        'body_part': 'arms'
                    })

        # 2. DRIBBLING - M·ªôt tay xu·ªëng th·∫•p, c∆° th·ªÉ c√∫i
        if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 11 in kp_dict):
            left_wrist = kp_dict[9]
            right_wrist = kp_dict[10]
            shoulder = kp_dict[5]
            hip = kp_dict[11]

            # M·ªôt tay th·∫•p h∆°n h√¥ng
            low_hand = (left_wrist['y'] > hip['y'] or right_wrist['y'] > hip['y'])

            # C∆° th·ªÉ h∆°i c√∫i
            body_lean = shoulder['y'] < hip['y'] + height * 0.1

            if low_hand and body_lean:
                action_confidence = max(action_confidence, 0.75)
                detected_actions.append({
                    'action': 'dribbling',
                    'confidence': 0.75,
                    'details': 'Low hand position with controlled dribble stance',
                    'body_part': 'arms'
                })

    # ==================== TENNIS ====================
    elif 'tennis' in sport_lower and 'tennis' in allowed_sports:
        action_confidence = 0.0

        # 1. SERVING - M·ªôt tay n√¢ng cao (lempar b√≥ng), tay kia chu·∫©n b·ªã ƒë√°nh
        if (9 in kp_dict and 10 in kp_dict and 7 in kp_dict and 8 in kp_dict and
                5 in kp_dict and 6 in kp_dict):

            left_wrist = kp_dict[9]
            right_wrist = kp_dict[10]
            left_shoulder = kp_dict[5]
            right_shoulder = kp_dict[6]

            avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2

            # Ki·ªÉm tra m·ªôt tay n√¢ng r·∫•t cao (lempar b√≥ng)
            left_very_high = left_wrist['y'] < avg_shoulder_y - height * 0.15
            right_very_high = right_wrist['y'] < avg_shoulder_y - height * 0.15

            if left_very_high or right_very_high:
                action_confidence = 0.8
                serving_hand = "left" if left_very_high else "right"
                detected_actions.append({
                    'action': 'serving',
                    'confidence': action_confidence,
                    'details': f'{serving_hand.title()} hand raised for ball toss',
                    'body_part': f'{serving_hand}_arm'
                })

        # 2. FOREHAND/BACKHAND - Tay du·ªói ra m·ªôt b√™n
        if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
            left_wrist = kp_dict[9]
            right_wrist = kp_dict[10]
            left_shoulder = kp_dict[5]
            right_shoulder = kp_dict[6]

            shoulder_center_x = (left_shoulder['x'] + right_shoulder['x']) / 2

            # Ki·ªÉm tra tay du·ªói ra xa kh·ªèi c∆° th·ªÉ
            left_extended = abs(left_wrist['x'] - shoulder_center_x) > width * 0.15
            right_extended = abs(right_wrist['x'] - shoulder_center_x) > width * 0.15

            if left_extended or right_extended:
                action_confidence = max(action_confidence, 0.7)
                stroke_type = "forehand" if (
                            left_extended and body_orientation in ["front", "left_side"]) else "backhand"
                detected_actions.append({
                    'action': stroke_type,
                    'confidence': 0.7,
                    'details': f'Arm extended for {stroke_type} stroke',
                    'body_part': 'arms'
                })

    # ==================== B√ìNG CHUY·ªÄN (VOLLEYBALL) ====================
    elif 'volleyball' in sport_lower and 'volleyball' in allowed_sports:
        action_confidence = 0.0

        # 1. SPIKING - Nhi·ªÅu bi·∫øn th·ªÉ t∆∞ th·∫ø tay
        def detect_spiking_variations():
            confidence = 0.0
            details = []
            spike_type = "unknown"

            if (9 in kp_dict and 10 in kp_dict and 7 in kp_dict and 8 in kp_dict and
                    5 in kp_dict and 6 in kp_dict):

                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_elbow = kp_dict[7]
                right_elbow = kp_dict[8]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2

                # VARIATION 1: Classic Spike - M·ªôt tay cao, m·ªôt tay th·∫•p
                left_very_high = left_wrist['y'] < avg_shoulder_y - height * 0.15
                right_very_high = right_wrist['y'] < avg_shoulder_y - height * 0.15
                left_moderate = avg_shoulder_y - height * 0.15 <= left_wrist['y'] < avg_shoulder_y
                right_moderate = avg_shoulder_y - height * 0.15 <= right_wrist['y'] < avg_shoulder_y

                if (left_very_high and right_moderate) or (right_very_high and left_moderate):
                    confidence = 0.85
                    spike_type = "classic_spike"
                    hitting_hand = "left" if left_very_high else "right"
                    details.append(f"{hitting_hand} hand in classic spike position")

                # VARIATION 2: Two-Hand Spike Preparation - C·∫£ hai tay cao
                elif left_very_high and right_very_high:
                    # Ki·ªÉm tra tay n√†o cao h∆°n
                    height_diff = abs(left_wrist['y'] - right_wrist['y'])
                    if height_diff > height * 0.08:
                        confidence = 0.8
                        spike_type = "power_spike_prep"
                        higher_hand = "left" if left_wrist['y'] < right_wrist['y'] else "right"
                        details.append(f"Two-hand spike prep, {higher_hand} hand leading")
                    else:
                        confidence = 0.75
                        spike_type = "double_hand_spike"
                        details.append("Both hands equally high for power spike")

                # VARIATION 3: Quick Attack - Tay ng·∫Øn, nhanh
                elif (left_moderate and right_moderate):
                    # Ki·ªÉm tra g√≥c khu·ª∑u tay (quick attack c√≥ g√≥c nh·ªè h∆°n)
                    left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)
                    right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)

                    quick_angle = left_elbow_angle < 100 or right_elbow_angle < 100
                    if quick_angle:
                        confidence = 0.7
                        spike_type = "quick_attack"
                        details.append("Compact arm position for quick attack")

                # VARIATION 4: Back Row Attack - Tay cao, c∆° th·ªÉ nghi√™ng
                if confidence > 0 and 11 in kp_dict:  # C√≥ detect spike + c√≥ hip data
                    hip = kp_dict[11]
                    body_lean = abs(avg_shoulder_y - hip['y']) > height * 0.15
                    if body_lean:
                        confidence += 0.1
                        spike_type += "_back_row"
                        details.append("body lean indicates back row attack")

                # VARIATION 5: Cross-Court vs Line Shot - D·ª±a v√†o h∆∞·ªõng tay
                if confidence > 0:
                    hitting_hand_x = left_wrist['x'] if left_very_high else right_wrist['x']
                    body_center_x = (left_shoulder['x'] + right_shoulder['x']) / 2

                    cross_court = abs(hitting_hand_x - body_center_x) > width * 0.1
                    if cross_court:
                        details.append("cross-court angle detected")
                    else:
                        details.append("straight line attack angle")

            return confidence, spike_type, details

        # 2. SETTING - C·∫£ hai tay n√¢ng l√™n ·ªü m·ª©c vai
        def detect_setting():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                # C·∫£ hai tay ngang m·ª©c vai ho·∫∑c h∆°i cao h∆°n
                hands_at_shoulder_level = (
                        abs(left_wrist['y'] - left_shoulder['y']) < height * 0.1 and
                        abs(right_wrist['y'] - right_shoulder['y']) < height * 0.1
                )

                # Hai tay g·∫ßn nhau (setting position)
                hands_close = abs(left_wrist['x'] - right_wrist['x']) < width * 0.15

                if hands_at_shoulder_level and hands_close:
                    confidence = 0.75
                    details.append("Both hands positioned for setting")

            return confidence, details

        # 3. BLOCKING - Variations of blocking positions
        def detect_blocking_variations():
            confidence = 0.0
            details = []
            block_type = "unknown"

            if (9 in kp_dict and 10 in kp_dict and 7 in kp_dict and 8 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_elbow = kp_dict[7]
                right_elbow = kp_dict[8]

                # VARIATION 1: Classic Block - C·∫£ hai tay th·∫≥ng l√™n
                left_arm_vertical = abs(left_wrist['x'] - left_elbow['x']) < width * 0.08
                right_arm_vertical = abs(right_wrist['x'] - right_elbow['x']) < width * 0.08

                if left_arm_vertical and right_arm_vertical:
                    confidence = 0.8
                    block_type = "double_block"
                    details.append("Classic double-hand block position")

                # VARIATION 2: Single Block - M·ªôt tay ch·∫∑n
                elif left_arm_vertical or right_arm_vertical:
                    confidence = 0.7
                    block_type = "single_block"
                    blocking_hand = "left" if left_arm_vertical else "right"
                    details.append(f"{blocking_hand} hand single block")

                # VARIATION 3: Soft Block/Tool - Tay h∆°i nghi√™ng
                else:
                    # Ki·ªÉm tra tay c√≥ n√¢ng cao kh√¥ng nh∆∞ng kh√¥ng th·∫≥ng
                    hands_raised = (left_wrist['y'] < left_elbow['y'] and
                                    right_wrist['y'] < right_elbow['y'])
                    if hands_raised:
                        confidence = 0.6
                        block_type = "soft_block"
                        details.append("Angled hands for soft block or tool")

            return confidence, block_type, details

        # 4. DIGGING/RECEIVE - Tay xu·ªëng th·∫•p
        def detect_digging():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 11 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                hip = kp_dict[11]

                # C·∫£ hai tay th·∫•p h∆°n h√¥ng
                hands_low = (left_wrist['y'] > hip['y'] and right_wrist['y'] > hip['y'])

                # Hai tay g·∫ßn nhau (platform)
                hands_together = abs(left_wrist['x'] - right_wrist['x']) < width * 0.12

                if hands_low and hands_together:
                    confidence = 0.75
                    details.append("Low platform position for dig/receive")

            return confidence, details

        # Ch·∫°y t·∫•t c·∫£ detections
        spike_conf, spike_type, spike_details = detect_spiking_variations()
        setting_conf, setting_details = detect_setting()
        block_conf, block_type, block_details = detect_blocking_variations()
        dig_conf, dig_details = detect_digging()

        # ∆Øu ti√™n theo confidence score
        max_conf = max(spike_conf, setting_conf, block_conf, dig_conf)

        if max_conf == spike_conf and spike_conf > 0.5:
            action_confidence = spike_conf
            detected_actions.append({
                'action': spike_type,
                'confidence': spike_conf,
                'details': '; '.join(spike_details),
                'body_part': 'arms'
            })
        elif max_conf == block_conf and block_conf > 0.5:
            action_confidence = block_conf
            detected_actions.append({
                'action': block_type,
                'confidence': block_conf,
                'details': '; '.join(block_details),
                'body_part': 'arms'
            })
        elif max_conf == setting_conf and setting_conf > 0.5:
            action_confidence = setting_conf
            detected_actions.append({
                'action': 'setting',
                'confidence': setting_conf,
                'details': '; '.join(setting_details),
                'body_part': 'both_arms'
            })
        elif max_conf == dig_conf and dig_conf > 0.5:
            action_confidence = dig_conf
            detected_actions.append({
                'action': 'digging',
                'confidence': dig_conf,
                'details': '; '.join(dig_details),
                'body_part': 'both_arms'
            })

    # ==================== BOXING/MARTIAL ARTS ====================
    elif (any(sport in sport_lower for sport in ['boxing', 'martial arts', 'mma', 'karate', 'taekwondo']) or
          any(sport in allowed_sports for sport in ['boxing', 'martial arts'])):
        print(f"DEBUG BOXING - Entering boxing detection. sport_lower: {sport_lower}, allowed_sports: {allowed_sports}")
        action_confidence = 0.0

        # 1. PUNCHING - C·∫£i thi·ªán detection logic
        def detect_punching_improved():
            confidence = 0.0
            details = []
            punch_type = "unknown"

            if (9 in kp_dict and 10 in kp_dict and 7 in kp_dict and 8 in kp_dict and
                    5 in kp_dict and 6 in kp_dict):

                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_elbow = kp_dict[7]
                right_elbow = kp_dict[8]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                # C·∫¢I THI·ªÜN 1: Gi·∫£m ng∆∞·ª°ng extension ratio
                shoulder_width = abs(left_shoulder['x'] - right_shoulder['x'])
                left_extension = abs(left_wrist['x'] - left_shoulder['x'])
                right_extension = abs(right_wrist['x'] - right_shoulder['x'])

                # GI·∫¢M NG∆Ø·ª†NG t·ª´ 0.8 xu·ªëng 0.5
                left_extended = left_extension > shoulder_width * 0.5
                right_extended = right_extension > shoulder_width * 0.5

                # C·∫¢I THI·ªÜN 2: Ki·ªÉm tra ƒë·ªô cao tay (boxing stance)
                avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2
                left_hand_elevated = left_wrist['y'] < avg_shoulder_y + height * 0.15  # Tay ·ªü m·ª©c vai ho·∫∑c cao h∆°n
                right_hand_elevated = right_wrist['y'] < avg_shoulder_y + height * 0.15

                # STRAIGHT PUNCH - Tay du·ªói th·∫≥ng ra
                if left_extended or right_extended:
                    punching_hand = "left" if left_extended else "right"
                    punch_height = left_wrist['y'] if left_extended else right_wrist['y']
                    shoulder_height = left_shoulder['y'] if left_extended else right_shoulder['y']

                    if abs(punch_height - shoulder_height) < height * 0.15:  # TƒÉng tolerance
                        punch_type = "straight_punch"
                        confidence = 0.85
                        details.append(f"{punching_hand} straight punch at head level")
                    else:
                        punch_type = "body_shot"
                        confidence = 0.8
                        details.append(f"{punching_hand} body shot")

                # C·∫¢I THI·ªÜN 3: HOOK PUNCH - Gi·∫£m y√™u c·∫ßu g√≥c khu·ª∑u
                elif not (left_extended or right_extended):
                    left_elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)
                    right_elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)

                    # GI·∫¢M Y√äU C·∫¶U t·ª´ 60-90 xu·ªëng 45-110
                    if 45 <= left_elbow_angle <= 110 and left_hand_elevated:
                        punch_type = "left_hook"
                        confidence = 0.8
                        details.append(f"Left hook with {left_elbow_angle:.1f}¬∞ elbow angle")
                    elif 45 <= right_elbow_angle <= 110 and right_hand_elevated:
                        punch_type = "right_hook"
                        confidence = 0.8
                        details.append(f"Right hook with {right_elbow_angle:.1f}¬∞ elbow angle")

                # C·∫¢I THI·ªÜN 4: BOXING STANCE DETECTION (m·ªõi)
                if confidence == 0.0:
                    # Ki·ªÉm tra t∆∞ th·∫ø boxing c∆° b·∫£n (c·∫£ 2 tay n√¢ng l√™n)
                    both_hands_up = left_hand_elevated and right_hand_elevated

                    if both_hands_up:
                        # Ki·ªÉm tra kho·∫£ng c√°ch tay (boxing guard)
                        hand_distance = abs(left_wrist['x'] - right_wrist['x'])

                        if hand_distance > shoulder_width * 0.3:  # Tay r·ªông ra
                            punch_type = "boxing_stance"
                            confidence = 0.7
                            details.append("Active boxing stance with hands up")
                        else:
                            punch_type = "defensive_guard"
                            confidence = 0.75
                            details.append("Tight defensive guard position")

                # C·∫¢I THI·ªÜN 5: UPPERCUT - M·ªü r·ªông detection
                if confidence == 0.0:
                    left_rising = left_wrist['y'] < avg_shoulder_y and left_elbow['y'] > left_wrist['y']
                    right_rising = right_wrist['y'] < avg_shoulder_y and right_elbow['y'] > right_wrist['y']

                    if left_rising or right_rising:
                        punch_type = "uppercut"
                        confidence = 0.75
                        rising_hand = "left" if left_rising else "right"
                        details.append(f"{rising_hand} uppercut motion")



            return confidence, punch_type, details

        # 2. DEFENSIVE GUARD - C·∫£i thi·ªán
        def detect_guard_improved():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2

                # GI·∫¢M Y√äU C·∫¶U - tay ·ªü m·ª©c vai ho·∫∑c h∆°i cao h∆°n
                hands_up = (left_wrist['y'] <= avg_shoulder_y + height * 0.1 and
                            right_wrist['y'] <= avg_shoulder_y + height * 0.1)

                # TƒÇNG TOLERANCE cho kho·∫£ng c√°ch tay
                shoulder_center_x = (left_shoulder['x'] + right_shoulder['x']) / 2
                hands_close = (abs(left_wrist['x'] - shoulder_center_x) < width * 0.25 and
                               abs(right_wrist['x'] - shoulder_center_x) < width * 0.25)

                if hands_up and hands_close:
                    confidence = 0.8
                    details.append("Defensive guard position with hands up")
                elif hands_up:  # Ch·ªâ c·∫ßn tay n√¢ng l√™n
                    confidence = 0.6
                    details.append("Hands elevated in fighting position")

            return confidence, details

        # 3. AGGRESSIVE STANCE - M·ªõi th√™m
        def detect_aggressive_stance():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict and
                    11 in kp_dict and 12 in kp_dict):

                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]
                left_hip = kp_dict[11]
                right_hip = kp_dict[12]

                # Ki·ªÉm tra t∆∞ th·∫ø t·∫•n c√¥ng (1 tay xa, 1 tay g·∫ßn)
                shoulder_center_x = (left_shoulder['x'] + right_shoulder['x']) / 2
                left_distance = abs(left_wrist['x'] - shoulder_center_x)
                right_distance = abs(right_wrist['x'] - shoulder_center_x)

                # M·ªôt tay xa, m·ªôt tay g·∫ßn
                distance_diff = abs(left_distance - right_distance)

                if distance_diff > width * 0.15:  # Ch√™nh l·ªách ƒë√°ng k·ªÉ
                    # Ki·ªÉm tra c∆° th·ªÉ c√≥ nghi√™ng v·ªÅ ph√≠a tr∆∞·ªõc kh√¥ng
                    avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2
                    avg_hip_y = (left_hip['y'] + right_hip['y']) / 2

                    forward_lean = avg_shoulder_y < avg_hip_y - height * 0.05

                    if forward_lean:
                        confidence = 0.75
                        details.append("Aggressive forward stance detected")
                    else:
                        confidence = 0.65
                        details.append("Asymmetric hand position suggesting attack")

            return confidence, details

        # Ch·∫°y t·∫•t c·∫£ detections
        punch_conf, punch_type, punch_details = detect_punching_improved()
        guard_conf, guard_details = detect_guard_improved()
        aggressive_conf, aggressive_details = detect_aggressive_stance()
        # TH√äM: Fallback detection cho boxing
        if max(punch_conf, guard_conf, aggressive_conf) == 0:
            print("DEBUG BOXING - Trying fallback detection")

            # Fallback 1: Ch·ªâ c·∫ßn c√≥ tay n√¢ng l√™n
            if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2

                # Ch·ªâ c·∫ßn m·ªôt tay ngang ho·∫∑c cao h∆°n vai
                hands_elevated = (left_wrist['y'] <= avg_shoulder_y + height * 0.2 or
                                  right_wrist['y'] <= avg_shoulder_y + height * 0.2)

                if hands_elevated:
                    punch_conf = 0.5
                    punch_type = "boxing_stance"
                    punch_details = ["Basic boxing position with elevated hands"]
                    print("DEBUG BOXING - Fallback detection successful")

        # ∆Øu ti√™n theo confidence
        max_conf = max(punch_conf, guard_conf, aggressive_conf)

        if max_conf == punch_conf and punch_conf > 0.4:  # Gi·∫£m ng∆∞·ª°ng t·ª´ 0.5 xu·ªëng 0.4
            action_confidence = punch_conf
            detected_actions.append({
                'action': punch_type,
                'confidence': punch_conf,
                'details': '; '.join(punch_details),
                'body_part': 'arms'
            })
        elif max_conf == aggressive_conf and aggressive_conf > 0.4:  # Gi·∫£m ng∆∞·ª°ng
            action_confidence = aggressive_conf
            detected_actions.append({
                'action': 'aggressive_stance',
                'confidence': aggressive_conf,
                'details': '; '.join(aggressive_details),
                'body_part': 'full_body'
            })
        elif max_conf == guard_conf and guard_conf > 0.4:  # Gi·∫£m ng∆∞·ª°ng
            action_confidence = guard_conf
            detected_actions.append({
                'action': 'defensive_guard',
                'confidence': guard_conf,
                'details': '; '.join(guard_details),
                'body_part': 'both_arms'
            })

        # C·∫¨P NH·∫¨T TH√îNG TIN V·ªÄ LO·∫†I TH·ªÇ THAO N·∫æU PH√ÅT HI·ªÜN BOXING - LOGIC M·ªöI
        if action_confidence > 0.5:  # Gi·∫£m ng∆∞·ª°ng t·ª´ 0.7 xu·ªëng 0.5
            detected_action = detected_actions[-1]['action'] if detected_actions else None
            boxing_actions = ['straight_punch', 'left_hook', 'right_hook', 'uppercut', 'body_shot',
                              'defensive_guard', 'boxing_stance', 'aggressive_stance']

            if detected_action in boxing_actions:
                detected_actions[-1]['detected_sport'] = 'boxing'
                # Th√™m m·ªôt flag ƒë·∫∑c bi·ªát ƒë·ªÉ force sport type
                detected_actions[-1]['force_boxing'] = True
                print(
                    f"DEBUG - Boxing action detected: {detected_action} - confidence: {detected_actions[-1]['confidence']:.2f} - FORCE BOXING TYPE")


    # ==================== CH·∫†Y/ƒêI·ªÄN KINH (RUNNING/TRACK) ====================
    elif (any(sport in sport_lower for sport in ['running', 'track', 'sprint', 'marathon']) and
          any(sport in allowed_sports for sport in ['running', 'track'])):
        action_confidence = 0.0

        # 1. SPRINTING - Ch√¢n n√¢ng cao, tay vung m·∫°nh
        if (13 in kp_dict and 14 in kp_dict and 15 in kp_dict and 16 in kp_dict and
                9 in kp_dict and 10 in kp_dict):

            left_knee = kp_dict[13]
            right_knee = kp_dict[14]
            left_ankle = kp_dict[15]
            right_ankle = kp_dict[16]
            left_wrist = kp_dict[9]
            right_wrist = kp_dict[10]

            # Ki·ªÉm tra ch√¢n n√¢ng cao
            ground_level = height * 0.9
            high_knee = (left_knee['y'] < ground_level * 0.7 or right_knee['y'] < ground_level * 0.7)

            # Ki·ªÉm tra tay vung (ch√™nh l·ªách ƒë·ªô cao gi·ªØa hai tay)
            arm_swing = abs(left_wrist['y'] - right_wrist['y']) > height * 0.1

            if high_knee and arm_swing:
                action_confidence = 0.85
                detected_actions.append({
                    'action': 'sprinting',
                    'confidence': action_confidence,
                    'details': 'High knee lift with dynamic arm swing',
                    'body_part': 'full_body'
                })
            elif high_knee:
                action_confidence = 0.7
                detected_actions.append({
                    'action': 'running',
                    'confidence': 0.7,
                    'details': 'Elevated knee position indicating running motion',
                    'body_part': 'legs'
                })

    # ==================== C·∫¶U L√îNG (BADMINTON) ====================
    elif 'badminton' in sport_lower and 'badminton' in allowed_sports:
        action_confidence = 0.0

        # 1. SMASH - Tay n√¢ng cao, chu·∫©n b·ªã ƒë·∫≠p m·∫°nh
        def detect_smash():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 7 in kp_dict and 8 in kp_dict and
                    5 in kp_dict and 6 in kp_dict):

                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_elbow = kp_dict[7]
                right_elbow = kp_dict[8]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2

                # M·ªôt tay n√¢ng r·∫•t cao (c·∫ßm v·ª£t)
                left_very_high = left_wrist['y'] < avg_shoulder_y - height * 0.2
                right_very_high = right_wrist['y'] < avg_shoulder_y - height * 0.2

                if left_very_high or right_very_high:
                    smashing_hand = "left" if left_very_high else "right"

                    # Ki·ªÉm tra g√≥c khu·ª∑u tay (smash prep c√≥ g√≥c ƒë·∫∑c tr∆∞ng)
                    if smashing_hand == "left":
                        elbow_angle = calculate_angle(left_shoulder, left_elbow, left_wrist)
                    else:
                        elbow_angle = calculate_angle(right_shoulder, right_elbow, right_wrist)

                    if 90 <= elbow_angle <= 150:
                        confidence = 0.85
                        details.append(f"{smashing_hand} hand in smash position (angle: {elbow_angle:.1f}¬∞)")
                    else:
                        confidence = 0.7
                        details.append(f"{smashing_hand} hand raised for overhead shot")

            return confidence, details

        # 2. CLEAR/DROP - Tay vung t·ª´ sau ra tr∆∞·ªõc
        def detect_clear_drop():
            confidence = 0.0
            details = []
            shot_type = "unknown"

            if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                # Ki·ªÉm tra tay c√≥ du·ªói ra kh√¥ng (follow-through)
                shoulder_center_x = (left_shoulder['x'] + right_shoulder['x']) / 2
                left_extended = abs(left_wrist['x'] - shoulder_center_x) > width * 0.15
                right_extended = abs(right_wrist['x'] - shoulder_center_x) > width * 0.15

                if left_extended or right_extended:
                    hitting_hand = "left" if left_extended else "right"

                    # X√°c ƒë·ªãnh shot type d·ª±a tr√™n ƒë·ªô cao
                    if hitting_hand == "left":
                        wrist_height = left_wrist['y']
                        shoulder_height = left_shoulder['y']
                    else:
                        wrist_height = right_wrist['y']
                        shoulder_height = right_shoulder['y']

                    if wrist_height < shoulder_height:
                        shot_type = "clear_shot"
                        confidence = 0.75
                        details.append(f"{hitting_hand} hand clear shot motion")
                    else:
                        shot_type = "drop_shot"
                        confidence = 0.7
                        details.append(f"{hitting_hand} hand drop shot motion")

            return confidence, shot_type, details

        # 3. DEFENSIVE POSITION - Tay th·∫•p, s·∫µn s√†ng ph√≤ng th·ªß
        def detect_defensive():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 11 in kp_dict and 13 in kp_dict and 14 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                hip = kp_dict[11]
                left_knee = kp_dict[13]
                right_knee = kp_dict[14]

                # Tay ·ªü m·ª©c th·∫•p (ready position)
                hands_low = (left_wrist['y'] > hip['y'] - height * 0.1 and
                             right_wrist['y'] > hip['y'] - height * 0.1)

                # Ch√¢n h∆°i g·∫•p (athletic stance)
                left_knee_bent = calculate_angle(hip, left_knee, kp_dict[15]) < 160 if 15 in kp_dict else False
                right_knee_bent = calculate_angle(hip, right_knee, kp_dict[16]) < 160 if 16 in kp_dict else False

                if hands_low and (left_knee_bent or right_knee_bent):
                    confidence = 0.7
                    details.append("Low ready position for defensive play")

            return confidence, details

        # Ch·∫°y detections
        smash_conf, smash_details = detect_smash()
        clear_conf, clear_type, clear_details = detect_clear_drop()
        def_conf, def_details = detect_defensive()

        # ∆Øu ti√™n
        if smash_conf > 0.6:
            action_confidence = smash_conf
            detected_actions.append({
                'action': 'badminton_smash',
                'confidence': smash_conf,
                'details': '; '.join(smash_details),
                'body_part': 'dominant_arm'
            })
        elif clear_conf > 0.6:
            action_confidence = clear_conf
            detected_actions.append({
                'action': clear_type,
                'confidence': clear_conf,
                'details': '; '.join(clear_details),
                'body_part': 'dominant_arm'
            })
        elif def_conf > 0.5:
            action_confidence = def_conf
            detected_actions.append({
                'action': 'defensive_ready',
                'confidence': def_conf,
                'details': '; '.join(def_details),
                'body_part': 'full_body'
            })

    # ==================== GOLF ====================
    elif 'golf' in sport_lower and 'golf' in allowed_sports:
        action_confidence = 0.0

        # 1. BACKSWING - Tay vung l√™n cao v·ªÅ ph√≠a sau
        def detect_backswing():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2
                shoulder_center_x = (left_shoulder['x'] + right_shoulder['x']) / 2

                # C·∫£ hai tay n√¢ng cao v√† h∆°i l·ªách v·ªÅ m·ªôt b√™n
                hands_high = (left_wrist['y'] < avg_shoulder_y and right_wrist['y'] < avg_shoulder_y)

                # Ki·ªÉm tra swing arc (tay l·ªách sang m·ªôt b√™n)
                hands_offset = abs(((left_wrist['x'] + right_wrist['x']) / 2) - shoulder_center_x)
                swing_arc = hands_offset > width * 0.1

                if hands_high and swing_arc:
                    confidence = 0.8
                    swing_side = "right" if ((left_wrist['x'] + right_wrist['x']) / 2) > shoulder_center_x else "left"
                    details.append(f"Backswing motion toward {swing_side} side")

            return confidence, details

        # 2. DOWNSWING/IMPACT - Tay vung xu·ªëng
        def detect_downswing():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 11 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                hip = kp_dict[11]

                # Tay ·ªü m·ª©c h√¥ng ho·∫∑c th·∫•p h∆°n (impact position)
                hands_at_impact = (left_wrist['y'] >= hip['y'] - height * 0.1 and
                                   right_wrist['y'] >= hip['y'] - height * 0.1)

                # Hai tay g·∫ßn nhau (grip)
                hands_together = abs(left_wrist['x'] - right_wrist['x']) < width * 0.08

                if hands_at_impact and hands_together:
                    confidence = 0.75
                    details.append("Impact position with hands at ball level")

            return confidence, details

        # 3. FOLLOW THROUGH - Tay vung qua b√™n kia
        def detect_follow_through():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                left_shoulder = kp_dict[5]
                right_shoulder = kp_dict[6]

                avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2
                shoulder_center_x = (left_shoulder['x'] + right_shoulder['x']) / 2

                # Tay cao v√† ·ªü ph√≠a ƒë·ªëi di·ªán v·ªõi backswing
                hands_high = (left_wrist['y'] < avg_shoulder_y and right_wrist['y'] < avg_shoulder_y)

                # Ki·ªÉm tra follow-through direction
                hands_center_x = (left_wrist['x'] + right_wrist['x']) / 2
                follow_through = abs(hands_center_x - shoulder_center_x) > width * 0.12

                if hands_high and follow_through:
                    confidence = 0.75
                    follow_side = "left" if hands_center_x < shoulder_center_x else "right"
                    details.append(f"Follow-through completion toward {follow_side}")

            return confidence, details

        # 4. PUTTING STANCE - Tay th·∫•p, t∆∞ th·∫ø c√∫i
        def detect_putting():
            confidence = 0.0
            details = []

            if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 11 in kp_dict):
                left_wrist = kp_dict[9]
                right_wrist = kp_dict[10]
                shoulder = kp_dict[5]
                hip = kp_dict[11]

                # Tay th·∫•p (putting grip)
                hands_low = (left_wrist['y'] > hip['y'] and right_wrist['y'] > hip['y'])

                # C∆° th·ªÉ c√∫i v·ªÅ ph√≠a tr∆∞·ªõc
                forward_lean = shoulder['y'] < hip['y'] - height * 0.05

                # Hai tay g·∫ßn nhau v√† th·∫≥ng xu·ªëng
                hands_together = abs(left_wrist['x'] - right_wrist['x']) < width * 0.06

                if hands_low and forward_lean and hands_together:
                    confidence = 0.8
                    details.append("Putting stance with forward lean")

            return confidence, details

        # Ch·∫°y detections
        back_conf, back_details = detect_backswing()
        down_conf, down_details = detect_downswing()
        follow_conf, follow_details = detect_follow_through()
        putt_conf, putt_details = detect_putting()

        # ∆Øu ti√™n theo confidence
        max_conf = max(back_conf, down_conf, follow_conf, putt_conf)

        if max_conf == back_conf and back_conf > 0.6:
            action_confidence = back_conf
            detected_actions.append({
                'action': 'golf_backswing',
                'confidence': back_conf,
                'details': '; '.join(back_details),
                'body_part': 'both_arms'
            })
        elif max_conf == down_conf and down_conf > 0.6:
            action_confidence = down_conf
            detected_actions.append({
                'action': 'golf_impact',
                'confidence': down_conf,
                'details': '; '.join(down_details),
                'body_part': 'both_arms'
            })
        elif max_conf == follow_conf and follow_conf > 0.6:
            action_confidence = follow_conf
            detected_actions.append({
                'action': 'golf_follow_through',
                'confidence': follow_conf,
                'details': '; '.join(follow_details),
                'body_part': 'both_arms'
            })
        elif max_conf == putt_conf and putt_conf > 0.6:
            action_confidence = putt_conf
            detected_actions.append({
                'action': 'putting',
                'confidence': putt_conf,
                'details': '; '.join(putt_details),
                'body_part': 'both_arms'
            })


    # ==================== B∆†I L·ªòI (SWIMMING) ====================
    elif ('swimming' in sport_lower or 'swimming' in allowed_sports or is_swimming_environment):
        print(f"DEBUG SWIMMING - Entering swimming detection")
        print(
            f"sport_lower: {sport_lower}, allowed_sports: {allowed_sports}, is_swimming_environment: {is_swimming_environment}")
        action_confidence = 0.0

        # FREESTYLE STROKE - M·ªôt tay du·ªói v·ªÅ ph√≠a tr∆∞·ªõc
        if (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
            left_wrist = kp_dict[9]
            right_wrist = kp_dict[10]
            left_shoulder = kp_dict[5]
            right_shoulder = kp_dict[6]

            # Ki·ªÉm tra m·ªôt tay du·ªói xa
            left_extended = abs(left_wrist['x'] - left_shoulder['x']) > width * 0.2
            right_extended = abs(right_wrist['x'] - right_shoulder['x']) > width * 0.2

            if left_extended or right_extended:
                action_confidence = 0.75
                stroke_arm = "left" if left_extended else "right"
                detected_actions.append({
                    'action': 'freestyle_stroke',
                    'confidence': action_confidence,
                    'details': f'{stroke_arm.title()} arm extended for freestyle stroke',
                    'body_part': f'{stroke_arm}_arm'
                })
                print(f"SWIMMING DEBUG - Detected freestyle stroke with {stroke_arm} arm")

        # BREASTSTROKE - C·∫£ hai tay ra ngo√†i
        if not detected_actions and (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
            left_wrist = kp_dict[9]
            right_wrist = kp_dict[10]
            left_shoulder = kp_dict[5]
            right_shoulder = kp_dict[6]

            # C·∫£ hai tay ƒë·ªÅu ra ngo√†i (breaststroke)
            left_out = abs(left_wrist['x'] - left_shoulder['x']) > width * 0.15
            right_out = abs(right_wrist['x'] - right_shoulder['x']) > width * 0.15

            if left_out and right_out:
                action_confidence = 0.7
                detected_actions.append({
                    'action': 'breaststroke',
                    'confidence': action_confidence,
                    'details': 'Both arms extended for breaststroke motion',
                    'body_part': 'both_arms'
                })
                print(f"SWIMMING DEBUG - Detected breaststroke")

        # BACKSTROKE - M·ªôt tay l√™n cao
        if not detected_actions and (9 in kp_dict and 10 in kp_dict and 5 in kp_dict and 6 in kp_dict):
            left_wrist = kp_dict[9]
            right_wrist = kp_dict[10]
            left_shoulder = kp_dict[5]
            right_shoulder = kp_dict[6]

            avg_shoulder_y = (left_shoulder['y'] + right_shoulder['y']) / 2

            # M·ªôt tay n√¢ng cao (backstroke)
            left_high = left_wrist['y'] < avg_shoulder_y - height * 0.1
            right_high = right_wrist['y'] < avg_shoulder_y - height * 0.1

            if left_high or right_high:
                action_confidence = 0.65
                stroke_arm = "left" if left_high else "right"
                detected_actions.append({
                    'action': 'backstroke',
                    'confidence': action_confidence,
                    'details': f'{stroke_arm.title()} arm raised for backstroke motion',
                    'body_part': f'{stroke_arm}_arm'
                })
                print(f"SWIMMING DEBUG - Detected backstroke with {stroke_arm} arm")

        # DIVING/STARTING POSITION - C∆° th·ªÉ c√∫i v·ªÅ ph√≠a tr∆∞·ªõc
        if not detected_actions and (5 in kp_dict and 11 in kp_dict):
            shoulder = kp_dict[5]
            hip = kp_dict[11]

            # C∆° th·ªÉ nghi√™ng v·ªÅ ph√≠a tr∆∞·ªõc (diving stance)
            forward_lean = shoulder['y'] < hip['y'] - height * 0.05

            if forward_lean:
                action_confidence = 0.6
                detected_actions.append({
                    'action': 'diving_start',
                    'confidence': action_confidence,
                    'details': 'Forward lean indicating diving start position',
                    'body_part': 'full_body'
                })
                print(f"SWIMMING DEBUG - Detected diving start position")

        # FALLBACK cho swimming environment
        if not detected_actions:
            action_confidence = 0.4
            detected_actions.append({
                'action': 'swimming_position',
                'confidence': action_confidence,
                'details': 'General swimming position in pool environment',
                'body_part': 'full_body'
            })
            print(f"SWIMMING DEBUG - Using fallback swimming position")

    # FALLBACK DETECTION - Cho c√°c tr∆∞·ªùng h·ª£p kh√≥ detect
    if not detected_actions and len(kp_dict) >= 8:  # C√≥ ƒë·ªß keypoints nh∆∞ng kh√¥ng detect ƒë∆∞·ª£c action
        # Ph√¢n t√≠ch t·ªïng qu√°t d·ª±a tr√™n body posture
        fallback_confidence = 0.0
        fallback_action = "unknown"
        fallback_details = "General athletic posture detected"

        if 5 in kp_dict and 6 in kp_dict and 11 in kp_dict and 12 in kp_dict:
            shoulders = kp_dict[5], kp_dict[6]
            hips = kp_dict[11], kp_dict[12]

            # Athletic stance detection
            shoulder_width = abs(shoulders[0]['x'] - shoulders[1]['x'])
            hip_width = abs(hips[0]['x'] - hips[1]['x'])
            stance_ratio = shoulder_width / hip_width if hip_width > 0 else 1.0

            if 0.8 <= stance_ratio <= 1.2:  # Balanced athletic stance
                fallback_confidence = 0.4
                fallback_action = "athletic_stance"
                fallback_details = "Balanced athletic position detected"

                # Th√™m sport-specific fallback
                if 'soccer' in sport_lower or 'football' in sport_lower:
                    fallback_action = "soccer_ready_position"
                    fallback_details = "Ready position for soccer action"
                elif 'volleyball' in sport_lower:
                    fallback_action = "volleyball_ready_position"
                    fallback_details = "Ready position for volleyball action"

        if fallback_confidence > 0:
            detected_actions.append({
                'action': fallback_action,
                'confidence': fallback_confidence,
                'details': fallback_details,
                'body_part': 'full_body'
            })

    # T√≠nh confidence t·ªïng th·ªÉ
    if detected_actions:
        overall_confidence = max([action['confidence'] for action in detected_actions])
    else:
        overall_confidence = 0.0

    # Th√™m th√¥ng tin v·ªÅ body orientation v√†o k·∫øt qu·∫£
    for action in detected_actions:
        action['body_orientation'] = body_orientation

    return {
        'detected_actions': detected_actions,
        'confidence': overall_confidence,
        'body_orientation': body_orientation,
        'keypoints_count': len(kp_dict),
        'details': f'Analyzed {len(kp_dict)} keypoints for {sport_type} actions'
    }

def segment_main_subject(img, yolo_seg, main_subject_box):
    results = yolo_seg(img)
    best_mask = None
    best_iou = 0
    for result in results:
        if hasattr(result, 'masks') and result.masks is not None:
            for i, mask in enumerate(result.masks.data):
                box = result.boxes.xyxy[i].cpu().numpy()
                x1 = max(box[0], main_subject_box[0])
                y1 = max(box[1], main_subject_box[1])
                x2 = min(box[2], main_subject_box[2])
                y2 = min(box[3], main_subject_box[3])

                if x2 <= x1 or y2 <= y1:
                    continue

                intersection = (x2 - x1) * (y2 - y1)
                box_area = (box[2] - box[0]) * (box[3] - box[1])
                subject_area = (main_subject_box[2] - main_subject_box[0]) * (main_subject_box[3] - main_subject_box[1])
                union = box_area + subject_area - intersection

                iou = intersection / union

                if iou > best_iou:
                    best_iou = iou
                    best_mask = mask.cpu().numpy()

    if best_mask is not None and best_iou > 0.5:  # Ng∆∞·ª°ng IoU ƒë·ªÉ ch·∫•p nh·∫≠n mask
        print(f"T√¨m th·∫•y mask cho main subject v·ªõi IoU = {best_iou:.2f}")
        return best_mask

    print("Kh√¥ng t√¨m th·∫•y mask ph√π h·ª£p cho main subject")
    return None


def analyze_sports_composition(detections, analysis, img_data):
    analysis["composition_analysis"] if "composition_analysis" in analysis else {}
    depth_map = None
    if 'depth_map' in analysis:
        depth_map = analysis['depth_map']

    env_analysis = analyze_sports_environment(img_data, depth_map)
    result = {
        'sport_type': 'Unknown',
        'framing_quality': 'Unknown',
        'recommended_crop': None,
        'action_focus': 'Unknown'
    }
    sport_equipment = {
        # ƒê·ªëi t∆∞·ª£ng YOLO c∆° b·∫£n
        'tennis racket': 'Tennis',
        'boxing gloves': 'Boxing',
        'golf club': 'Golf',
        'badminton racket': 'Badminton',
        'baseball bat': 'Baseball',
        'baseball glove': 'Baseball',
        'skateboard': 'Skateboarding',
        'surfboard': 'Surfing',
        'frisbee': 'Ultimate Frisbee',
        'skis': 'Alpine Skiing',
        'snowboard': 'Snowboarding',
        'bicycle': 'Cycling',
        'motorcycle': 'Motocross',
        'kite': 'Kite Sports',

        # C√°c lo·∫°i b√≥ng ƒë∆∞·ª£c CLIP ph√¢n lo·∫°i t·ª´ 'sports ball'
        'soccer ball': 'Soccer',
        'basketball': 'Basketball',
        'volleyball': 'Volleyball',
        'tennis ball': 'Tennis',
        'baseball': 'Baseball',
        'golf ball': 'Golf',
        'american football': 'American Football',
        'rugby ball': 'Rugby',
        'ping pong ball': 'Table Tennis',
        'bowling ball': 'Bowling',
        'beach ball': 'Beach Sports'
    }

    # C·∫≠p nh·∫≠t k·∫øt qu·∫£ d·ª±a tr√™n m√¥i tr∆∞·ªùng
    detected_sport_from_env = None
    env_confidence = 0.0

    # L·∫•y m√¥n th·ªÉ thao c√≥ x√°c su·∫•t cao nh·∫•t t·ª´ ph√¢n t√≠ch m√¥i tr∆∞·ªùng
    if env_analysis['sport_probabilities']:
        env_sport, env_prob = max(env_analysis['sport_probabilities'].items(), key=lambda x: x[1])
        if env_prob > 0.8:  # TƒÉng ng∆∞·ª°ng tin c·∫≠y t·ª´ 0.5 l√™n 0.8 ƒë·ªÉ gi·∫£m l·ªói ph√¢n lo·∫°i
            detected_sport_from_env = env_sport
            env_confidence = env_prob
            print(f"Ph√°t hi·ªán m√¥n th·ªÉ thao t·ª´ m√¥i tr∆∞·ªùng: {env_sport} (ƒë·ªô tin c·∫≠y: {env_prob:.2f})")
        else:
            print(f"ƒê·ªô tin c·∫≠y ph√¢n t√≠ch m√¥i tr∆∞·ªùng qu√° th·∫•p ({env_prob:.2f} < 0.8), b·ªè qua k·∫øt qu·∫£: {env_sport}")

    detected_sport = None
    equipment_confidence = 0.0

    for cls in detections['classes']:
        if cls in sport_equipment:
            detected_sport = sport_equipment[cls]
            equipment_confidence = 0.7  # ƒê·ªô tin c·∫≠y khi ph√°t hi·ªán t·ª´ d·ª•ng c·ª•
            break

    # ==== M·ªöI: PH√ÇN T√çCH ACTION DETECTION ƒê·ªÇ SUY RA SPORT TYPE ====
    detected_sport_from_action = None
    action_confidence = 0.0

    # L·∫•y action detection results t·ª´ analysis
    if 'sports_analysis' in analysis and 'action_detection' in analysis['sports_analysis']:
        action_data = analysis['sports_analysis']['action_detection']
        actions = analysis['sports_analysis']['action_detection'].get('detected_actions', [])
        for action in actions:
            print(
                f"DEBUG COMPOSE - Action: {action['action']}, Confidence: {action['confidence']:.2f}, Sport: {action.get('detected_sport', 'unknown')}")
        detected_actions = action_data.get('detected_actions', [])

        if detected_actions:
            # Mapping t·ª´ action sang sport type
            action_to_sport = {
                # Boxing/Martial Arts actions
                'straight_punch': 'Boxing',
                'left_hook': 'Boxing',
                'right_hook': 'Boxing',
                'uppercut': 'Boxing',
                'body_shot': 'Boxing',
                'defensive_guard': 'Boxing',
                'high_kick': 'Martial Arts',
                'mid_kick': 'Martial Arts',

                # Badminton actions
                'badminton_smash': 'Badminton',
                'clear_shot': 'Badminton',
                'drop_shot': 'Badminton',
                'defensive_ready': 'Badminton',

                # Golf actions
                'golf_backswing': 'Golf',
                'golf_impact': 'Golf',
                'golf_follow_through': 'Golf',
                'putting': 'Golf',

                # Soccer actions
                'shooting': 'Soccer',
                'pre_kick_stance': 'Soccer',
                'approach_run': 'Soccer',
                'dribbling': 'Soccer',

                # Basketball actions (t·ª´ code c≈©)
                # 'shooting' ƒë√£ c√≥ ·ªü Soccer, c·∫ßn ph√¢n bi·ªát context

                # Volleyball actions
                'classic_spike': 'Volleyball',
                'power_spike_prep': 'Volleyball',
                'double_hand_spike': 'Volleyball',
                'quick_attack': 'Volleyball',
                'double_block': 'Volleyball',
                'single_block': 'Volleyball',
                'soft_block': 'Volleyball',
                'setting': 'Volleyball',
                'digging': 'Volleyball',

                # Tennis actions
                'serving': 'Tennis',
                'forehand': 'Tennis',
                'backhand': 'Tennis',

                # Running/Track actions
                'sprinting': 'Track and Field',
                'running': 'Running',

                # Swimming actions
                'freestyle_stroke': 'Swimming'
            }

            # T√¨m action c√≥ confidence cao nh·∫•t
            best_action = max(detected_actions, key=lambda x: x['confidence'])
            action_name = best_action['action']

            if action_name in action_to_sport and best_action['confidence'] > 0.6:
                detected_sport_from_action = action_to_sport[action_name]
                action_confidence = best_action['confidence']
                print(
                    f"Sport detected from action: {action_name} -> {detected_sport_from_action} (confidence: {action_confidence:.2f})")

                # X·ª¨ L√ù ƒê·∫∂C BI·ªÜT: 'shooting' c√≥ th·ªÉ l√† Soccer ho·∫∑c Basketball
                if action_name == 'shooting':
                    # Ki·ªÉm tra context ƒë·ªÉ ph√¢n bi·ªát
                    if 'soccer ball' in detections.get('classes', []):
                        detected_sport_from_action = 'Soccer'
                    elif 'basketball' in detections.get('classes', []):
                        detected_sport_from_action = 'Basketball'
                    elif 'sports ball' in detections.get('classes', []):
                        # D·ª±a v√†o environment ƒë·ªÉ ƒëo√°n
                        if env_analysis.get('surface_type') == 'grass':
                            detected_sport_from_action = 'Soccer'
                        elif env_analysis.get('surface_type') == 'court':
                            detected_sport_from_action = 'Basketball'
                        else:
                            detected_sport_from_action = 'Soccer'  # Default
    # PRIORITY ∆ØU TI√äN M·ªöI: ACTION -> EQUIPMENT -> ENVIRONMENT
    # (ƒë·ªÉ action detection c√≥ ƒë·ªô tin c·∫≠y cao ƒë∆∞·ª£c ∆∞u ti√™n)
    decision_log = []

    # PRIORITY 0: Ki·ªÉm tra boxing action t·ª´ action detection
    if 'sports_analysis' in analysis and 'action_detection' in analysis['sports_analysis'] and \
            analysis['sports_analysis']['action_detection'].get('detected_actions'):

        # Danh s√°ch c·ª• th·ªÉ c√°c h√†nh ƒë·ªông boxing
        boxing_actions = ['straight_punch', 'left_hook', 'right_hook', 'uppercut', 'body_shot',
                          'defensive_guard', 'boxing_stance', 'aggressive_stance']

        # Tr∆∞·ªõc ti√™n, ki·ªÉm tra tr·ª±c ti·∫øp t√™n h√†nh ƒë·ªông
        for action in analysis['sports_analysis']['action_detection']['detected_actions']:
            if action['action'] in boxing_actions and action.get('confidence', 0) > 0.6:
                result['sport_type'] = 'Boxing'
                decision_log.append(f"Boxing action detected: {action['action']} ({action['confidence']:.2f})")
                print(f"DEBUG-FIX: Ph√°t hi·ªán boxing action {action['action']}, g√°n sport_type=Boxing")
                break

        # N·∫øu ch∆∞a t√¨m th·∫•y, ki·ªÉm tra th√¥ng qua tr∆∞·ªùng detected_sport (logic c≈©)
        if result['sport_type'] != 'Boxing':
            for action in analysis['sports_analysis']['action_detection']['detected_actions']:
                if action.get('confidence', 0) > 0.7 and action.get('detected_sport') == 'boxing':
                    result['sport_type'] = 'Boxing'
                    decision_log.append(
                        f"High-confidence boxing action detection: {action['action']} ({action['confidence']:.2f})")
                    break

    # PRIORITY 1: Action detection c√≥ confidence cao (∆∞u ti√™n nh·∫•t)
    if detected_sport_from_action and action_confidence > 0.7:
        result['sport_type'] = detected_sport_from_action
        decision_log.append(f"High-confidence action detection: {detected_sport_from_action} ({action_confidence:.2f})")

    # PRIORITY 2: Equipment detection
    elif detected_sport and equipment_confidence > 0.6:
        result['sport_type'] = detected_sport
        decision_log.append(f"Equipment detection: {detected_sport} ({equipment_confidence:.2f})")

    # PRIORITY 3: Action detection v·ªõi confidence trung b√¨nh
    elif detected_sport_from_action and action_confidence > 0.6:
        result['sport_type'] = detected_sport_from_action
        decision_log.append(f"Action detection: {detected_sport_from_action} ({action_confidence:.2f})")

    # PRIORITY 4: Environment detection (ƒë·∫∑c bi·ªát cho swimming)
    elif detected_sport_from_env and env_confidence > 0.8:
        result['sport_type'] = detected_sport_from_env
        decision_log.append(f"Environment detection: {detected_sport_from_env} ({env_confidence:.2f})")

    # PRIORITY 5: Action detection v·ªõi confidence th·∫•p h∆°n
    elif detected_sport_from_action and action_confidence > 0.5:
        result['sport_type'] = detected_sport_from_action
        decision_log.append(
            f"Medium-confidence action detection: {detected_sport_from_action} ({action_confidence:.2f})")

    # FALLBACKS
    elif detected_sport and equipment_confidence > 0.4:  # Equipment fallback
        result['sport_type'] = detected_sport
        decision_log.append(f"Equipment detection (fallback): {detected_sport} ({equipment_confidence:.2f})")
    elif detected_sport_from_env and env_confidence > 0.5:  # Environment fallback
        result['sport_type'] = detected_sport_from_env
        decision_log.append(f"Environment detection (fallback): {detected_sport_from_env} ({env_confidence:.2f})")
    elif detected_sport_from_action and action_confidence > 0.4:  # Action fallback
        result['sport_type'] = detected_sport_from_action
        decision_log.append(f"Action detection (fallback): {detected_sport_from_action} ({action_confidence:.2f})")
        # Ki·ªÉm tra l·∫ßn cu·ªëi xem c√≥ action boxing n√†o kh√¥ng
    if result['sport_type'] == 'Unknown' or result['sport_type'] == 'Running':
        if 'sports_analysis' in analysis and 'action_detection' in analysis['sports_analysis']:
            actions = analysis['sports_analysis']['action_detection'].get('detected_actions', [])

            boxing_actions = ['straight_punch', 'left_hook', 'right_hook', 'uppercut', 'body_shot',
                              'defensive_guard', 'boxing_stance', 'aggressive_stance']

            for action in actions:
                if action['action'] in boxing_actions and action.get('confidence', 0) > 0.5:
                    result['sport_type'] = 'Boxing'
                    decision_log.append(
                        f"FINAL CHECK: Found boxing action {action['action']} - overriding to Boxing")
                    break

    # Ki·ªÉm tra l·∫ßn cu·ªëi xem c√≥ action boxing n√†o kh√¥ng
    boxing_actions = ['straight_punch', 'left_hook', 'right_hook', 'uppercut', 'body_shot',
                      'defensive_guard', 'boxing_stance', 'aggressive_stance']

    boxing_detected = False
    if 'sports_analysis' in analysis and 'action_detection' in analysis['sports_analysis']:
        actions = analysis['sports_analysis']['action_detection'].get('detected_actions', [])
        for action in actions:
            if action['action'] in boxing_actions and action.get('confidence', 0) > 0.5:
                result['sport_type'] = 'Boxing'
                decision_log.append(f"Final check: Boxing action found: {action['action']}")
                print(f"DEBUG-FIX: Ki·ªÉm tra cu·ªëi c√πng - ph√°t hi·ªán {action['action']} -> set Boxing")
                boxing_detected = True
                break

    # Ch·ªâ s·ª≠ d·ª•ng Running l√†m m·∫∑c ƒë·ªãnh khi kh√¥ng ph√°t hi·ªán boxing
    if not boxing_detected and result['sport_type'] == 'Unknown':
        result['sport_type'] = 'Running'  # Default cu·ªëi c√πng
        decision_log.append("Default: Running")

    print(f"Sport type decision: {' -> '.join(decision_log)}")

    # Debug equipment vs action relationship
    if detected_sport:
        print(f"Equipment detected: {detected_sport} (confidence: {equipment_confidence:.2f})")
    if detected_sport_from_action:
        print(f"Action-based sport: {detected_sport_from_action} (confidence: {action_confidence:.2f})")
    if detected_sport_from_env:
        print(f"Environment-based sport: {detected_sport_from_env} (confidence: {env_confidence:.2f})")

    # L∆∞u th√¥ng tin ph√¢n t√≠ch m√¥i tr∆∞·ªùng
    result['environment_analysis'] = env_analysis

    framing_score = 0.0
    framing_details = {}

    sports_analysis_data = analysis.get('sports_analysis', {})

    if "key_subjects" in sports_analysis_data and sports_analysis_data['key_subjects']:
        print("=== Analyzing Framing Quality ===")

        # TH√äM: Ph√°t hi·ªán nh√≥m v·∫≠n ƒë·ªông vi√™n
        people_subjects = [s for s in sports_analysis_data['key_subjects'] if s['class'] == 'person']
        total_athletes = detections.get('athletes', 0)

        print(f"DEBUG - Total athletes detected: {total_athletes}")
        print(f"DEBUG - People in key_subjects: {len(people_subjects)}")

        # LOGIC M·ªöI: Ph√°t hi·ªán nh√≥m ƒë√¥ng ng∆∞·ªùi (TI√äU CH√ç NGHI√äM NG·∫∂T H∆†N)
        is_group_scene = False

        # ƒêi·ªÅu ki·ªán 1: C√≥ nhi·ªÅu ng∆∞·ªùi
        if total_athletes >= 3 and len(people_subjects) >= 2:
            print("DEBUG - Condition 1 met: Multiple people detected")

            # ƒêi·ªÅu ki·ªán 2: K√≠ch th∆∞·ªõc ƒë·ªëi t∆∞·ª£ng ch√≠nh > 35% = c√≥ th·ªÉ l√† nh√≥m
            main_subject_temp = sports_analysis_data['key_subjects'][0]
            temp_box = main_subject_temp['box']
            temp_area = (temp_box[2] - temp_box[0]) * (temp_box[3] - temp_box[1])
            temp_ratio = temp_area / (img_data['resized_array'].shape[0] * img_data['resized_array'].shape[1])

            print(f"DEBUG - Main subject size ratio: {temp_ratio:.3f}")

            if temp_ratio > 0.35:  # N·∫øu ƒë·ªëi t∆∞·ª£ng ch√≠nh chi·∫øm > 35% ·∫£nh
                is_group_scene = True
                print("DEBUG - Condition 2 met: Large subject size suggests group")

            # ƒêi·ªÅu ki·ªán 3: Ki·ªÉm tra kho·∫£ng c√°ch (CH·ªà N·∫æU CH∆ØA X√ÅC ƒê·ªäNH ƒê∆Ø·ª¢C)
            if not is_group_scene and len(people_subjects) >= 3:
                # Ki·ªÉm tra xem c√°c ng∆∞·ªùi c√≥ g·∫ßn nhau kh√¥ng
                positions = [s['position'] for s in people_subjects[:6]]  # L·∫•y t·ªëi ƒëa 6 ng∆∞·ªùi ƒë·∫ßu

                # T√≠nh kho·∫£ng c√°ch trung b√¨nh gi·ªØa c√°c ng∆∞·ªùi
                distances = []
                for i in range(len(positions)):
                    for j in range(i + 1, len(positions)):
                        dist = np.sqrt((positions[i][0] - positions[j][0]) ** 2 +
                                       (positions[i][1] - positions[j][1]) ** 2)
                        distances.append(dist)

                if distances:
                    avg_distance = np.mean(distances)
                    print(f"DEBUG - Average distance between people: {avg_distance:.3f}")

                    # N·∫øu kho·∫£ng c√°ch trung b√¨nh < 0.3 (30% ·∫£nh) = nh√≥m s√°t nhau
                    if avg_distance < 0.3:
                        is_group_scene = True
                        print("DEBUG - Detected GROUP SCENE (crowded athletes)")

        if is_group_scene:
            # PH√ÇN T√çCH NH√ìM: L·∫•y bounding box bao quanh to√†n b·ªô nh√≥m
            all_boxes = [s['box'] for s in people_subjects[:8]]  # T·ªëi ƒëa 8 ng∆∞·ªùi

            # T√≠nh group bounding box
            min_x = min([box[0] for box in all_boxes])
            min_y = min([box[1] for box in all_boxes])
            max_x = max([box[2] for box in all_boxes])
            max_y = max([box[3] for box in all_boxes])

            group_box = [min_x, min_y, max_x, max_y]
            group_pos = [(min_x + max_x) / 2 / img_data['resized_array'].shape[1],
                         (min_y + max_y) / 2 / img_data['resized_array'].shape[0]]

            main_subject = {
                'box': group_box,
                'position': group_pos,
                'class': 'group',
                'is_group': True
            }

            print(f"DEBUG - Group position: {group_pos}")
            print(f"DEBUG - Group box: {group_box}")
        else:
            # PH√ÇN T√çCH ƒê∆†N L·∫∫: L·∫•y ƒë·ªëi t∆∞·ª£ng ch√≠nh nh∆∞ c≈©
            main_subject = sports_analysis_data['key_subjects'][0]
            main_subject['is_group'] = False

        main_pos = main_subject['position']
        main_box = main_subject['box']

        print(f"Main subject position: {main_pos}")
        print(f"Main subject class: {main_subject['class']}")

        # 1. PH√ÇN T√çCH V·ªä TR√ç THEO RULE OF THIRDS
        # C√°c ƒëi·ªÉm v√†ng theo quy t·∫Øc 1/3
        rule_of_thirds_points = [
            (1 / 3, 1 / 3), (2 / 3, 1 / 3),  # Top left, top right
            (1 / 3, 2 / 3), (2 / 3, 2 / 3)  # Bottom left, bottom right
        ]

        # T√¨m ƒëi·ªÉm g·∫ßn nh·∫•t v·ªõi rule of thirds
        min_dist_to_thirds = float('inf')

        for third_point in rule_of_thirds_points:
            dist = np.sqrt((main_pos[0] - third_point[0]) ** 2 + (main_pos[1] - third_point[1]) ** 2)
            if dist < min_dist_to_thirds:
                min_dist_to_thirds = dist

        # ƒêi·ªÉm cho rule of thirds (c√†ng g·∫ßn c√†ng cao)
        thirds_score = max(0, 1 - (min_dist_to_thirds / 0.3))  # Ng∆∞·ª°ng 30% kho·∫£ng c√°ch
        print(f"Rule of thirds score: {thirds_score:.3f} (distance: {min_dist_to_thirds:.3f})")

        # 2. PH√ÇN T√çCH V·ªä TR√ç TRUNG T√ÇM
        center_dist = np.sqrt((main_pos[0] - 0.5) ** 2 + (main_pos[1] - 0.5) ** 2)
        center_score = max(0, 1 - (center_dist / 0.4))  # Ng∆∞·ª°ng 40% t·ª´ trung t√¢m
        print(f"Center placement score: {center_score:.3f} (distance: {center_dist:.3f})")

        # 3. PH√ÇN T√çCH K√çCH TH∆Ø·ªöC ƒê·ªêI T∆Ø·ª¢NG CH√çNH
        img_height = img_data['resized_array'].shape[0]
        img_width = img_data['resized_array'].shape[1]

        subject_width = main_box[2] - main_box[0]
        subject_height = main_box[3] - main_box[1]
        subject_area = subject_width * subject_height
        img_area = img_width * img_height

        size_ratio = subject_area / img_area
        print(f"Subject size ratio: {size_ratio:.3f}")

        # ƒêI·ªÄU CH·ªàNH: Ti√™u chu·∫©n k√≠ch th∆∞·ªõc NGHI√äM NG·∫∂T cho nh√≥m
        if main_subject.get('is_group', False):
            # Nh√≥m ng∆∞·ªùi: √Åp d·ª•ng PENALTY cho k√≠ch th∆∞·ªõc qu√° l·ªõn
            if 0.25 <= size_ratio <= 0.45:
                size_score = 1.0  # K√≠ch th∆∞·ªõc l√Ω t∆∞·ªüng cho nh√≥m
            elif 0.45 < size_ratio <= 0.60:
                size_score = 0.6  # PENALTY: Nh√≥m h∆°i l·ªõn
            elif 0.60 < size_ratio <= 0.75:
                size_score = 0.3  # PENALTY M·∫†NH: Nh√≥m qu√° l·ªõn
            elif size_ratio > 0.75:
                size_score = 0.1  # PENALTY R·∫§T M·∫†NH: Nh√≥m chi·∫øm g·∫ßn h·∫øt ·∫£nh
            elif 0.15 <= size_ratio < 0.25:
                size_score = 0.7  # Nh√≥m h∆°i nh·ªè
            else:
                size_score = 0.4  # Nh√≥m qu√° nh·ªè

            print(f"DEBUG - Applied GROUP size scoring: {size_score:.3f} (ratio: {size_ratio:.3f})")
        else:
            # ƒê∆°n l·∫ª: k√≠ch th∆∞·ªõc l√Ω t∆∞·ªüng nh·ªè h∆°n (15-40%)
            if 0.15 <= size_ratio <= 0.40:
                size_score = 1.0
            elif 0.10 <= size_ratio < 0.15:
                size_score = 0.8
            elif 0.40 < size_ratio <= 0.60:
                size_score = 0.7
            elif 0.05 <= size_ratio < 0.10:
                size_score = 0.5
            else:
                size_score = 0.3
            print("DEBUG - Applied INDIVIDUAL size scoring")

        print(f"Size score: {size_score:.3f}")

        # 4. PH√ÇN T√çCH KH√îNG GIAN XUNG QUANH (BREATHING ROOM)
        # Ki·ªÉm tra ƒë·ªëi t∆∞·ª£ng c√≥ qu√° g·∫ßn vi·ªÅn kh√¥ng
        margin_left = main_pos[0]
        margin_right = 1 - main_pos[0]
        margin_top = main_pos[1]
        margin_bottom = 1 - main_pos[1]

        min_margin = min(margin_left, margin_right, margin_top, margin_bottom)

        # ƒêI·ªÄU CH·ªàNH: Ti√™u chu·∫©n margin NGHI√äM NG·∫∂T cho nh√≥m
        if main_subject.get('is_group', False):
            # Nh√≥m ng∆∞·ªùi: PENALTY cho margin qu√° l·ªõn (t·ª©c l√† nh√≥m qu√° nh·ªè ho·∫∑c ·ªü gi·ªØa)
            if 0.05 <= min_margin <= 0.15:
                breathing_score = 1.0  # Margin l√Ω t∆∞·ªüng cho nh√≥m
            elif 0.15 < min_margin <= 0.25:
                breathing_score = 0.7  # H∆°i nhi·ªÅu kh√¥ng gian tr·ªëng
            elif 0.25 < min_margin <= 0.35:
                breathing_score = 0.4  # PENALTY: Qu√° nhi·ªÅu kh√¥ng gian tr·ªëng
            elif min_margin > 0.35:
                breathing_score = 0.2  # PENALTY M·∫†NH: Nh√≥m qu√° nh·ªè so v·ªõi ·∫£nh
            elif 0.02 <= min_margin < 0.05:
                breathing_score = 0.8  # H∆°i s√°t vi·ªÅn nh∆∞ng ok cho nh√≥m
            else:
                breathing_score = 0.3  # Qu√° s√°t vi·ªÅn

            print(f"DEBUG - Applied GROUP breathing room scoring: {breathing_score:.3f} (margin: {min_margin:.3f})")
        else:
            # ƒê∆°n l·∫ª: y√™u c·∫ßu margin cao h∆°n
            if min_margin >= 0.15:
                breathing_score = 1.0
            elif min_margin >= 0.10:
                breathing_score = 0.8
            elif min_margin >= 0.05:
                breathing_score = 0.5
            else:
                breathing_score = 0.2
            print("DEBUG - Applied INDIVIDUAL breathing room scoring")

        print(f"Breathing room score: {breathing_score:.3f} (min margin: {min_margin:.3f})")

        # 5. PH√ÇN T√çCH ƒê·ªêI T∆Ø·ª¢NG PH·ª§
        secondary_objects_score = 1.0

        if len(sports_analysis_data['key_subjects']) > 1:
            # Ki·ªÉm tra ƒë·ªëi t∆∞·ª£ng ph·ª• c√≥ che khu·∫•t ƒë·ªëi t∆∞·ª£ng ch√≠nh kh√¥ng
            overlap_penalty = 0

            for i, secondary_subject in enumerate(sports_analysis_data['key_subjects'][1:4]):  # Ch·ªâ x√©t 3 ƒë·ªëi t∆∞·ª£ng ƒë·∫ßu
                secondary_subject['position']
                sec_box = secondary_subject['box']

                # T√≠nh overlap v·ªõi ƒë·ªëi t∆∞·ª£ng ch√≠nh
                x1_main, y1_main, x2_main, y2_main = main_box
                x1_sec, y1_sec, x2_sec, y2_sec = sec_box

                # T√≠nh intersection
                x_left = max(x1_main, x1_sec)
                y_top = max(y1_main, y1_sec)
                x_right = min(x2_main, x2_sec)
                y_bottom = min(y2_main, y2_sec)

                if x_right > x_left and y_bottom > y_top:
                    intersection = (x_right - x_left) * (y_bottom - y_top)
                    main_area = (x2_main - x1_main) * (y2_main - y1_main)
                    overlap_ratio = intersection / main_area

                    if overlap_ratio > 0.2:  # N·∫øu che khu·∫•t > 20%
                        overlap_penalty += overlap_ratio * 0.3
                        print(f"Overlap detected with secondary object {i + 1}: {overlap_ratio:.3f}")

            secondary_objects_score = max(0.2, 1.0 - overlap_penalty)

        print(f"Secondary objects score: {secondary_objects_score:.3f}")

        # 6. ƒêI·ªÄU CH·ªàNH THEO LO·∫†I TH·ªÇ THAO V√Ä NH√ìM
        sport_bonus = 1.0
        sport_type = result.get('sport_type', 'Unknown').lower()

        if main_subject.get('is_group', False):
            # NH√ìM: ∆Øu ti√™n k√≠ch th∆∞·ªõc v√† breathing room h∆°n v·ªã tr√≠
            if sport_type in ['track', 'running', 'marathon']:
                sport_bonus = 1.0 + (size_score * 0.15) + (breathing_score * 0.10)
            elif sport_type in ['soccer', 'football', 'basketball']:
                sport_bonus = 1.0 + (size_score * 0.10) + (thirds_score * 0.10)
            else:
                sport_bonus = 1.0 + (size_score * 0.10)
            print("DEBUG - Applied GROUP sport bonus")
        else:
            # ƒê∆†N L·∫∫: Logic c≈©
            if sport_type in ['soccer', 'football', 'basketball']:
                sport_bonus = 1.0 + (thirds_score * 0.2)
            elif sport_type in ['tennis', 'golf', 'individual']:
                sport_bonus = 1.0 + (center_score * 0.2)
            elif sport_type in ['track', 'running', 'swimming']:
                sport_bonus = 1.0 + (size_score * 0.1) + (breathing_score * 0.1)
            print("DEBUG - Applied INDIVIDUAL sport bonus")

        print(f"Sport bonus: {sport_bonus:.3f}")

        # 7. T√çNH ƒêI·ªÇM T·ªîNG H·ª¢P
        # Tr·ªçng s·ªë cho t·ª´ng y·∫øu t·ªë
        position_weight = 0.35  # Rule of thirds ho·∫∑c center
        size_weight = 0.25
        breathing_weight = 0.20
        secondary_weight = 0.20

        # Ch·ªçn ƒëi·ªÉm cao h∆°n gi·ªØa rule of thirds v√† center
        position_score = max(thirds_score, center_score)

        framing_score = (
                                position_score * position_weight +
                                size_score * size_weight +
                                breathing_score * breathing_weight +
                                secondary_objects_score * secondary_weight
                        ) * sport_bonus

        # 8. TH√äM PENALTY ƒê·∫∂C BI·ªÜT CHO NH√ìM ƒê√îNG NG∆Ø·ªúI
        group_penalty = 1.0
        if main_subject.get('is_group', False):
            # Penalty d·ª±a tr√™n s·ªë l∆∞·ª£ng ng∆∞·ªùi v√† m·∫≠t ƒë·ªô
            if total_athletes >= 8:
                group_penalty = 0.85  # 15% penalty cho nh√≥m r·∫•t ƒë√¥ng
            elif total_athletes >= 5:
                group_penalty = 0.90  # 10% penalty cho nh√≥m ƒë√¥ng

            # Penalty th√™m n·∫øu k√≠ch th∆∞·ªõc + margin ƒë·ªÅu cao (= framing k√©m)
            if size_ratio > 0.5 and min_margin > 0.3:
                group_penalty *= 0.8  # 20% penalty th√™m
                print("DEBUG - Applied DOUBLE PENALTY for large group with too much space")

            print(f"DEBUG - Group penalty applied: {group_penalty:.3f}")

        # 9. T√çNH ƒêI·ªÇM T·ªîNG H·ª¢P V·ªöI PENALTY
        # Tr·ªçng s·ªë cho t·ª´ng y·∫øu t·ªë
        position_weight = 0.25  # Gi·∫£m t·ª´ 0.35 xu·ªëng 0.25 cho nh√≥m
        size_weight = 0.35  # TƒÉng t·ª´ 0.25 l√™n 0.35 (quan tr·ªçng h∆°n)
        breathing_weight = 0.25  # TƒÉng t·ª´ 0.20 l√™n 0.25
        secondary_weight = 0.15  # Gi·∫£m t·ª´ 0.20 xu·ªëng 0.15

        # Ch·ªçn ƒëi·ªÉm cao h∆°n gi·ªØa rule of thirds v√† center
        position_score = max(thirds_score, center_score)

        framing_score = (
                                position_score * position_weight +
                                size_score * size_weight +
                                breathing_score * breathing_weight +
                                secondary_objects_score * secondary_weight
                        ) * sport_bonus * group_penalty  # TH√äM GROUP PENALTY

        framing_score = min(1.0, framing_score)  # Cap ·ªü 1.0

        print(f"Final framing score: {framing_score:.3f}")

        # 10. PH√ÇN LO·∫†I CH·∫§T L∆Ø·ª¢NG (NGHI√äM NG·∫∂T H∆†N CHO NH√ìM)
        if main_subject.get('is_group', False):
            # Ti√™u chu·∫©n nghi√™m ng·∫∑t h∆°n cho nh√≥m
            if framing_score >= 0.90:
                framing_quality = 'Excellent'
            elif framing_score >= 0.75:
                framing_quality = 'Very Good'
            elif framing_score >= 0.60:
                framing_quality = 'Good'
            elif framing_score >= 0.45:
                framing_quality = 'Fair'
            else:
                framing_quality = 'Could be improved'
            print("DEBUG - Applied GROUP quality standards")
        else:
            # Ti√™u chu·∫©n b√¨nh th∆∞·ªùng cho ƒë∆°n l·∫ª
            if framing_score >= 0.85:
                framing_quality = 'Excellent'
            elif framing_score >= 0.70:
                framing_quality = 'Very Good'
            elif framing_score >= 0.55:
                framing_quality = 'Good'
            elif framing_score >= 0.40:
                framing_quality = 'Fair'
            else:
                framing_quality = 'Could be improved'
            print("DEBUG - Applied INDIVIDUAL quality standards")

        result['framing_quality'] = framing_quality

        # L∆∞u chi ti·∫øt ph√¢n t√≠ch
        framing_details = {
            'overall_score': framing_score,
            'position_score': position_score,
            'size_score': size_score,
            'breathing_score': breathing_score,
            'secondary_objects_score': secondary_objects_score,
            'sport_bonus': sport_bonus,
            'main_subject_position': main_pos,
            'size_ratio': size_ratio,
            'min_margin': min_margin,
            'rule_of_thirds_distance': min_dist_to_thirds,
            'center_distance': center_dist
        }

        print(f"=== Framing Quality: {framing_quality} ({framing_score:.3f}) ===")

    else:
        result['framing_quality'] = 'Cannot analyze - no subjects detected'
        framing_details = {'error': 'No key subjects found'}
        print("Cannot analyze framing quality - no key subjects detected")

    # L∆∞u chi ti·∫øt v√†o k·∫øt qu·∫£
    result['framing_analysis'] = framing_details

    # Recommend crop if needed
    sports_analysis_data = analysis.get('sports_analysis', {})

    if "key_subjects" in sports_analysis_data and sports_analysis_data['key_subjects']:
        main_subject = sports_analysis_data['key_subjects'][0]
        x_pos = main_subject['position'][0]
        y_pos = main_subject['position'][1]

        # If subject is too far from ideal positions, suggest crop
        if not (0.3 < x_pos < 0.7 or 0.3 < y_pos < 0.7):
            # Calculate ideal center point
            if x_pos < 0.33:
                ideal_x = 0.33
            elif x_pos > 0.67:
                ideal_x = 0.67
            else:
                ideal_x = 0.5

            if y_pos < 0.33:
                ideal_y = 0.33
            elif y_pos > 0.67:
                ideal_y = 0.67
            else:
                ideal_y = 0.5

            # Calculate shift needed
            shift_x = ideal_x - x_pos
            shift_y = ideal_y - y_pos

            result['recommended_crop'] = {
                'shift_x': shift_x,
                'shift_y': shift_y
            }

    # Evaluate action focus
    if "action_quality" in analysis:
        result['action_focus'] = analysis['action_quality']

    return result


# H√†m ph√¢n t√≠ch bi·ªÉu c·∫£m n√¢ng cao k·∫øt h·ª£p DeepFace v√† ph√¢n t√≠ch ng·ªØ c·∫£nh

def analyze_facial_expression_advanced(detections, img_data, depth_map=None, sports_analysis=None):
    """Ph√¢n t√≠ch bi·ªÉu c·∫£m khu√¥n m·∫∑t n√¢ng cao v·ªõi OpenCV v√† HSEmotion, t·∫≠p trung v√†o ƒë·ªëi t∆∞·ª£ng ch√≠nh"""
    try:
        print("Starting advanced facial expression analysis...")
        import cv2
        import numpy as np
        import os
        import traceback

        # Thi·∫øt l·∫≠p ƒë·ªÉ gi·∫£m l·ªói TF/protobuf
        os.environ["TF_CPP_MIN_LOG_LEVEL"] = "2"
        os.environ["PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION"] = "python"

        image = img_data['resized_array']
        image.shape[0] * image.shape[1]

        # Directory ƒë·ªÉ l∆∞u debug images
        debug_dir = "face_debug"
        os.makedirs(debug_dir, exist_ok=True)

        # Default results with debug info
        expression_results = {
            'has_faces': False,
            'expressions': [],
            'dominant_emotion': "unknown",
            'emotion_intensity': 0,
            'emotional_value': 'Low',
            'debug_info': {'detected': False, 'reason': 'No face detected initially'}
        }

        # 1. IDENTIFY MAIN SUBJECT
        main_subject = None
        main_subject_idx = -1

        # A. T·ª´ ph√¢n t√≠ch sports_analysis
        if sports_analysis and 'key_subjects' in sports_analysis and sports_analysis['key_subjects']:
            for idx, subject in enumerate(sports_analysis['key_subjects']):
                if subject['class'] == 'person':
                    main_subject = subject
                    main_subject_idx = idx
                    print(
                        f"Main subject identified from key_subjects (idx={idx}, prominence={subject['prominence']:.3f}, sharpness={subject.get('sharpness', 0):.3f})")
                    break

        # B. N·∫øu kh√¥ng t√¨m th·∫•y t·ª´ key_subjects, t√¨m t·ª´ detections
        if main_subject is None and isinstance(detections, dict) and 'boxes' in detections:
            max_center_weight = 0

            # K√≠ch th∆∞·ªõc ·∫£nh
            height, width = image.shape[:2]
            center_x, center_y = width // 2, height // 2

            for i, cls in enumerate(detections['classes']):
                if cls == 'person':
                    box = detections['boxes'][i]
                    x1, y1, x2, y2 = box

                    # T√≠nh di·ªán t√≠ch
                    area = (x2 - x1) * (y2 - y1)

                    # T√≠nh v·ªã tr√≠ trung t√¢m v√† ƒë·ªô g·∫ßn v·ªõi trung t√¢m ·∫£nh
                    obj_center_x = (x1 + x2) // 2
                    obj_center_y = (y1 + y2) // 2

                    # Kho·∫£ng c√°ch ƒë·∫øn trung t√¢m (chu·∫©n h√≥a)
                    dist_to_center = np.sqrt(((obj_center_x - center_x) / width) ** 2 +
                                             ((obj_center_y - center_y) / height) ** 2)

                    # Tr·ªçng s·ªë k·∫øt h·ª£p (di·ªán t√≠ch + v·ªã tr√≠ trung t√¢m)
                    center_weight = area * (1 - min(1.0, dist_to_center))

                    # L∆∞u gi√° tr·ªã cao nh·∫•t
                    if center_weight > max_center_weight:
                        max_center_weight = center_weight
                        main_subject = {
                            'box': box,
                            'class': cls,
                            'prominence': center_weight
                        }
                        main_subject_idx = i

            if main_subject:
                print(
                    f"Main subject identified from detections (idx={main_subject_idx}, prominence={main_subject['prominence']:.3f})")

        # N·∫øu kh√¥ng t√¨m th·∫•y ƒë·ªëi t∆∞·ª£ng ch√≠nh
        if main_subject is None:
            print("Could not identify a main subject in the image")
            expression_results['debug_info']['reason'] = "No main subject detected"
            return expression_results

        # 2. EXTRACT MAIN SUBJECT REGION
        x1, y1, x2, y2 = main_subject['box']

        # L∆∞u ·∫£nh ƒë·ªëi t∆∞·ª£ng ch√≠nh ƒë·ªÉ debug
        subject_img = image[max(0, y1):min(y2, image.shape[0]),
                      max(0, x1):min(x2, image.shape[1])]
        subject_path = f"{debug_dir}/main_subject.jpg"
        cv2.imwrite(subject_path, cv2.cvtColor(subject_img, cv2.COLOR_RGB2BGR))

        # 3. PH√ÅT HI·ªÜN KHU√îN M·∫∂T TRONG V√ôNG ƒê·∫¶U C·ª¶A ƒê·ªêI T∆Ø·ª¢NG CH√çNH
        h, w = subject_img.shape[:2]
        head_height = int(h * 0.4)  # V√πng ƒë·∫ßu chi·∫øm 40% tr√™n c·ªßa ƒë·ªëi t∆∞·ª£ng
        head_region = subject_img[0:head_height, 0:w]

        # L∆∞u v√πng ƒë·∫ßu ƒë·ªÉ debug
        head_path = f"{debug_dir}/head_region.jpg"
        cv2.imwrite(head_path, cv2.cvtColor(head_region, cv2.COLOR_RGB2BGR))

        # Ph√°t hi·ªán khu√¥n m·∫∑t CH·ªà trong v√πng ƒë·∫ßu c·ªßa ƒë·ªëi t∆∞·ª£ng ch√≠nh
        faces = detect_faces_improved(head_region)

        # Ki·ªÉm tra n·∫øu t√¨m th·∫•y khu√¥n m·∫∑t
        face_found = len(faces) > 0
        face_img = None
        fx, fy, fw, fh = 0, 0, 0, 0

        if face_found:
            print(f"Found {len(faces)} faces in main subject's head region")

            # Ch·ªçn khu√¥n m·∫∑t t·ªët nh·∫•t n·∫øu c√≥ nhi·ªÅu khu√¥n m·∫∑t
            if len(faces) > 1:
                best_face = select_best_face(faces, head_region)
            else:
                best_face = faces[0]

            hx, hy, hw, hh = best_face

            # Tr√≠ch xu·∫•t khu√¥n m·∫∑t v√† th√™m padding
            padding = max(10, int(hw * 0.1))  # Padding t·ª∑ l·ªá v·ªõi k√≠ch th∆∞·ªõc khu√¥n m·∫∑t
            face_x1 = max(0, hx - padding)
            face_y1 = max(0, hy - padding)
            face_x2 = min(head_region.shape[1], hx + hw + padding)
            face_y2 = min(head_region.shape[0], hy + hh + padding)

            face_img = head_region[face_y1:face_y2, face_x1:face_x2]

            # L∆∞u khu√¥n m·∫∑t ƒë·ªÉ debug
            best_face_path = f"{debug_dir}/best_face.jpg"
            cv2.imwrite(best_face_path, cv2.cvtColor(face_img, cv2.COLOR_RGB2BGR))
            print(f"Face dimensions: {face_img.shape[1]}x{face_img.shape[0]}")

            # ƒêi·ªÅu ch·ªânh t·ªça ƒë·ªô khu√¥n m·∫∑t v·ªÅ kh√¥ng gian h√¨nh ·∫£nh g·ªëc
            fx = hx + x1
            fy = hy + y1  # Khu√¥n m·∫∑t ƒë√£ n·∫±m trong v√πng head_region

        if not face_found or face_img is None or face_img.size == 0:
            print("No valid face detected in the main subject's head region")
            expression_results['debug_info']['reason'] = "No valid face detected in the main subject"
            return expression_results

        # 4. KI·ªÇM TRA T√çNH H·ª¢P L·ªÜ C·ª¶A KHU√îN M·∫∂T
        is_valid_face, reason = verify_face(face_img)
        if not is_valid_face:
            print(f"Face verification failed: {reason}")
            expression_results['debug_info']['reason'] = f"Invalid face: {reason}"
            return expression_results

        # 5. PH√ÇN T√çCH BI·ªÇU C·∫¢M S·ª¨ D·ª§NG MODEL DNN
        try:
            print("Ph√¢n t√≠ch c·∫£m x√∫c v·ªõi DNN Model...")
            import os
            import urllib.request

            # T·∫°o th∆∞ m·ª•c cho model
            model_dir = "emotion_models"
            os.makedirs(model_dir, exist_ok=True)

            # ƒê∆∞·ªùng d·∫´n t·ªõi model v√† proto
            prototxt_url = "https://raw.githubusercontent.com/opencv/opencv/master/samples/dnn/face_detector/deploy.prototxt"
            prototxt_path = os.path.join(model_dir, "deploy.prototxt")

            # URL model GitHub ch·ª©a m√¥ h√¨nh emotion recognition onnx ƒë∆°n gi·∫£n
            model_url = "https://github.com/onnx/models/raw/main/validated/vision/body_analysis/emotion_ferplus/model/emotion-ferplus-7.onnx"
            model_path = os.path.join(model_dir, "emotion-ferplus.onnx")

            # T·∫£i model n·∫øu ch∆∞a c√≥
            if not os.path.exists(prototxt_path):
                print("ƒêang t·∫£i prototxt...")
                urllib.request.urlretrieve(prototxt_url, prototxt_path)

            if not os.path.exists(model_path):
                print(f"ƒêang t·∫£i emotion model t·ª´ GitHub...")
                urllib.request.urlretrieve(model_url, model_path)

            # Chu·∫©n b·ªã ·∫£nh cho model
            face_gray = cv2.cvtColor(face_img, cv2.COLOR_RGB2GRAY)
            face_resized = cv2.resize(face_gray, (64, 64))

            # Ti·ªÅn x·ª≠ l√Ω ·∫£nh - c√¢n b·∫±ng histogram ƒë·ªÉ tƒÉng ƒë·ªô t∆∞∆°ng ph·∫£n
            face_equalized = cv2.equalizeHist(face_resized)

            # Chu·∫©n h√≥a ·∫£nh (gi√° tr·ªã pixel t·ª´ 0-1)
            tensor = face_equalized.reshape(1, 1, 64, 64).astype(np.float32)

            # T·∫£i model v√† d·ª± ƒëo√°n
            print("T·∫£i model DNN...")
            net = cv2.dnn.readNetFromONNX(model_path)
            net.setInput(tensor)
            output = net.forward()

            # T√≠nh x√°c su·∫•t v·ªõi softmax
            def softmax(x):
                exp_x = np.exp(x - np.max(x))
                return exp_x / exp_x.sum()

            probabilities = softmax(output[0])

            # Danh s√°ch c·∫£m x√∫c theo th·ª© t·ª± c·ªßa model FER+
            emotions = ['neutral', 'happy', 'surprise', 'sad', 'angry', 'disgust', 'fear']

            # T·∫°o t·ª´ ƒëi·ªÉn ƒëi·ªÉm s·ªë
            emotion_scores_dict = {emotion: float(prob) for emotion, prob in zip(emotions, probabilities)}
            # Th√™m contempt cho t∆∞∆°ng th√≠ch v·ªõi code g·ªëc
            emotion_scores_dict['contempt'] = 0.01
            emotion_scores_dict['focus'] = 0.01

            # X√°c ƒë·ªãnh c·∫£m x√∫c ch√≠nh
            dominant_emotion = max(emotion_scores_dict, key=emotion_scores_dict.get)
            dominant_score = emotion_scores_dict[dominant_emotion]

            print(f"DNN ph√°t hi·ªán c·∫£m x√∫c: {dominant_emotion}")
            print(f"ƒêi·ªÉm s·ªë c·∫£m x√∫c: {emotion_scores_dict}")

            # T√≠nh c∆∞·ªùng ƒë·ªô c·∫£m x√∫c
            emotion_intensity = min(0.95, max(0.2, dominant_score))

            # T·∫°o ·∫£nh debug
            debug_img = face_img.copy()

            # Hi·ªÉn th·ªã c·∫£m x√∫c ch√≠nh
            font = cv2.FONT_HERSHEY_SIMPLEX
            cv2.putText(debug_img, f"{dominant_emotion.upper()}: {dominant_score:.2f}",
                        (10, 30), font, 0.7, (0, 255, 0), 2)

            # Hi·ªÉn th·ªã ƒëi·ªÉm s·ªë c·∫£m x√∫c
            y_pos = 60
            sorted_emotions = sorted(emotion_scores_dict.items(), key=lambda x: x[1], reverse=True)

            for emotion, score in sorted_emotions:
                if emotion == 'contempt':  # B·ªè qua contempt v√¨ n√≥ ch·ªâ l√† gi√° tr·ªã gi·ªØ ch·ªó
                    continue

                bar_width = int(score * 200)
                bar_color = (0, 255, 0)  # Default: green

                # M√†u ri√™ng cho t·ª´ng c·∫£m x√∫c
                if emotion == 'happy':
                    bar_color = (0, 255, 255)  # Yellow
                elif emotion == 'sad':
                    bar_color = (255, 0, 0)  # Blue
                elif emotion == 'angry':
                    bar_color = (0, 0, 255)  # Red
                elif emotion == 'surprise':
                    bar_color = (255, 0, 255)  # Magenta

                cv2.rectangle(debug_img, (10, y_pos), (10 + bar_width, y_pos + 20),
                              bar_color, -1)
                cv2.putText(debug_img, f"{emotion}: {score:.2f}",
                            (15, y_pos + 15), font, 0.5, (255, 255, 255), 1)
                y_pos += 30

            # Hi·ªÉn th·ªã ·∫£nh ƒë√£ x·ª≠ l√Ω cho debugging
            h, w = face_equalized.shape
            display_face = cv2.resize(face_equalized, (w * 2, h * 2))
            display_face = cv2.cvtColor(display_face, cv2.COLOR_GRAY2BGR)

            # L∆∞u ·∫£nh debug
            emotion_debug_path = f"{debug_dir}/dnn_emotion.jpg"
            cv2.imwrite(emotion_debug_path, cv2.cvtColor(debug_img, cv2.COLOR_RGB2BGR))
            print(f"ƒê√£ l∆∞u ·∫£nh debug t·∫°i: {emotion_debug_path}")

        except Exception as e:
            print(f"DNN analysis failed: {str(e)}")
            print(f"Chi ti·∫øt: {traceback.format_exc()}")

            # GI·∫¢I PH√ÅP D·ª∞ PH√íNG: PH√ÇN T√çCH LBP + HOG FEATURES
            try:
                print("S·ª≠ d·ª•ng ph∆∞∆°ng ph√°p ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng LBP v√† HOG...")

                # Chuy·ªÉn ·∫£nh sang grayscale n·∫øu ch∆∞a
                if len(face_img.shape) == 3:
                    gray_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)
                else:
                    gray_img = face_img.copy()

                # ƒê·∫£m b·∫£o ·∫£nh c√≥ k√≠ch th∆∞·ªõc chu·∫©n
                gray_img = cv2.resize(gray_img, (96, 96))

                # 1. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng LBP
                def extract_lbp_features(image, radius=1, n_points=8):
                    lbp = np.zeros_like(image)
                    for i in range(radius, image.shape[0] - radius):
                        for j in range(radius, image.shape[1] - radius):
                            center = image[i, j]
                            binary_pattern = []

                            # So s√°nh t·ª´ng ƒëi·ªÉm l√¢n c·∫≠n
                            binary_pattern.append(1 if image[i - 1, j - 1] >= center else 0)
                            binary_pattern.append(1 if image[i - 1, j] >= center else 0)
                            binary_pattern.append(1 if image[i - 1, j + 1] >= center else 0)
                            binary_pattern.append(1 if image[i, j + 1] >= center else 0)
                            binary_pattern.append(1 if image[i + 1, j + 1] >= center else 0)
                            binary_pattern.append(1 if image[i + 1, j] >= center else 0)
                            binary_pattern.append(1 if image[i + 1, j - 1] >= center else 0)
                            binary_pattern.append(1 if image[i, j - 1] >= center else 0)

                            lbp_value = 0
                            for k, bit in enumerate(binary_pattern):
                                lbp_value += bit * (2 ** k)

                            lbp[i, j] = lbp_value

                    return lbp

                # 2. Tr√≠ch xu·∫•t ƒë·∫∑c tr∆∞ng gradient (ƒë∆°n gi·∫£n h√≥a HOG)
                def extract_gradient_features(image):
                    # T√≠nh gradient theo tr·ª•c x v√† y
                    gradient_x = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=3)
                    gradient_y = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=3)

                    # T√≠nh magnitude v√† g√≥c
                    gradient_magnitude = np.sqrt(gradient_x ** 2 + gradient_y ** 2)
                    gradient_angle = np.arctan2(gradient_y, gradient_x) * 180 / np.pi

                    return gradient_magnitude, gradient_angle

                # Tr√≠ch xu·∫•t LBP
                lbp_image = extract_lbp_features(gray_img)

                # Chia ·∫£nh th√†nh v√πng (3x3)
                h, w = lbp_image.shape
                cell_h, cell_w = h // 3, w // 3

                # T√≠nh histogram cho t·ª´ng v√πng
                lbp_features = []
                for i in range(3):
                    for j in range(3):
                        cell = lbp_image[i * cell_h:(i + 1) * cell_h, j * cell_w:(j + 1) * cell_w]
                        hist, _ = np.histogram(cell, bins=16, range=(0, 256))
                        lbp_features.extend(hist)

                # Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng
                lbp_features = np.array(lbp_features) / np.sum(lbp_features)

                # Tr√≠ch xu·∫•t gradient
                gradient_mag, gradient_angle = extract_gradient_features(gray_img)

                # T√≠nh c√°c v√πng m·∫∑t
                # V√πng m·∫Øt (1/3 tr√™n)
                eyes_region = gray_img[:h // 3, :]
                # V√πng m≈©i (1/3 gi·ªØa)
                nose_region = gray_img[h // 3:2 * h // 3, :]
                # V√πng mi·ªáng (1/3 d∆∞·ªõi)
                mouth_region = gray_img[2 * h // 3:, :]

                # T√≠nh c√°c ƒë·∫∑c tr∆∞ng th·ªëng k√™
                regions = {
                    'eyes': eyes_region,
                    'nose': nose_region,
                    'mouth': mouth_region
                }

                region_stats = {}
                for name, region in regions.items():
                    region_stats[name] = {
                        'mean': np.mean(region) / 255.0,
                        'std': np.std(region) / 255.0,
                        'gradient_mean': np.mean(gradient_mag[0:h // 3, :] if name == 'eyes' else
                                                 gradient_mag[h // 3:2 * h // 3, :] if name == 'nose' else
                                                 gradient_mag[2 * h // 3:, :]) / 255.0
                    }

                # H·ªá s·ªë cho c√°c ƒë·∫∑c tr∆∞ng
                emotion_scores_dict = {
                    'neutral': 0.2,
                    'happy': 0.1,
                    'sad': 0.1,
                    'surprise': 0.1,
                    'angry': 0.1,
                    'fear': 0.1,
                    'disgust': 0.1,
                    'contempt': 0.1,
                    'focus': 0.10  # Th√™m c·∫£m x√∫c focus
                }

                # Ph√¢n t√≠ch ƒë·∫∑c tr∆∞ng LBP
                # LBP patterns ƒë·∫∑c tr∆∞ng cho happy th∆∞·ªùng c√≥ nhi·ªÅu ƒëi·ªÉm s√°ng (do n·ª• c∆∞·ªùi)
                lbp_bright_pattern = np.sum(lbp_features[np.arange(16) * 9 + 8])  # Ki·ªÉm tra m·∫´u bit s√°ng

                # Ph√¢n t√≠ch c√°c v√πng
                eyes_bright = region_stats['eyes']['mean']
                mouth_bright = region_stats['mouth']['mean']
                mouth_contrast = region_stats['mouth']['std']
                eyes_gradient = region_stats['eyes']['gradient_mean']
                mouth_gradient = region_stats['mouth']['gradient_mean']

                # Quy t·∫Øc ph√¢n lo·∫°i
                # 1. Happy: Mi·ªáng s√°ng, ƒë·ªô t∆∞∆°ng ph·∫£n cao (n·ª• c∆∞·ªùi)
                if mouth_bright > 0.5 and mouth_contrast > 0.16:
                    emotion_scores_dict['happy'] += 0.4
                    emotion_scores_dict['neutral'] -= 0.1

                # 2. Sad: Mi·ªáng t·ªëi, m·∫Øt t·ªëi
                if mouth_bright < 0.35 and eyes_bright < 0.4:
                    emotion_scores_dict['sad'] += 0.4
                    emotion_scores_dict['neutral'] -= 0.1

                # 3. Surprise: Gradient m·∫Øt v√† mi·ªáng cao (m·∫Øt m·ªü to, mi·ªáng m·ªü)
                if eyes_gradient > 0.12 and mouth_gradient > 0.15:
                    emotion_scores_dict['surprise'] += 0.4
                    emotion_scores_dict['fear'] += 0.2
                    emotion_scores_dict['neutral'] -= 0.2

                # 4. Angry: Eyes gradient cao, mi·ªáng t·ªëi
                if eyes_gradient > 0.15 and mouth_bright < 0.4:
                    emotion_scores_dict['angry'] += 0.3
                    emotion_scores_dict['disgust'] += 0.1

                # 5. Neutral: √çt bi·∫øn ƒë·ªïi
                if abs(eyes_gradient - mouth_gradient) < 0.05 and 0.4 < mouth_bright < 0.6:
                    emotion_scores_dict['neutral'] += 0.3

                # 6. TƒÉng Happy d·ª±a tr√™n LBP pattern
                if lbp_bright_pattern > 0.15:
                    emotion_scores_dict['happy'] += 0.2
                    emotion_scores_dict['neutral'] -= 0.1

                # ƒê·∫£m b·∫£o kh√¥ng c√≥ gi√° tr·ªã √¢m
                emotion_scores_dict = {k: max(0.01, v) for k, v in emotion_scores_dict.items()}

                # Chu·∫©n h√≥a
                total = sum(emotion_scores_dict.values())
                emotion_scores_dict = {k: v / total for k, v in emotion_scores_dict.items()}

                # T√¨m c·∫£m x√∫c ch√≠nh
                dominant_emotion = max(emotion_scores_dict, key=emotion_scores_dict.get)
                dominant_score = emotion_scores_dict[dominant_emotion]

                print(f"LBP/HOG ph√°t hi·ªán c·∫£m x√∫c: {dominant_emotion}")
                print(f"ƒêi·ªÉm s·ªë c·∫£m x√∫c: {emotion_scores_dict}")

                # T√≠nh c∆∞·ªùng ƒë·ªô c·∫£m x√∫c
                emotion_intensity = min(0.95, max(0.2, dominant_score))

                # T·∫°o ·∫£nh debug
                debug_img = face_img.copy()

                # Hi·ªÉn th·ªã c·∫£m x√∫c ch√≠nh
                font = cv2.FONT_HERSHEY_SIMPLEX
                cv2.putText(debug_img, f"{dominant_emotion.upper()}: {dominant_score:.2f}",
                            (10, 30), font, 0.7, (0, 255, 0), 2)

                # Chia v√πng khu√¥n m·∫∑t ƒë·ªÉ ph√¢n t√≠ch
                h, w = debug_img.shape[:2] if len(debug_img.shape) == 2 else debug_img.shape[:2]
                cv2.line(debug_img, (0, h // 3), (w, h // 3), (0, 255, 0), 1)
                cv2.line(debug_img, (0, 2 * h // 3), (w, 2 * h // 3), (0, 255, 0), 1)

                # Hi·ªÉn th·ªã c√°c ƒë·∫∑c tr∆∞ng v√πng
                y_pos = 50
                cv2.putText(debug_img, f"Eyes brightness: {eyes_bright:.2f}", (10, y_pos), font, 0.4, (255, 255, 255),
                            1)
                y_pos += 20
                cv2.putText(debug_img, f"Mouth brightness: {mouth_bright:.2f}", (10, y_pos), font, 0.4, (255, 255, 255),
                            1)
                y_pos += 20
                cv2.putText(debug_img, f"Mouth contrast: {mouth_contrast:.2f}", (10, y_pos), font, 0.4, (255, 255, 255),
                            1)

                # Hi·ªÉn th·ªã ƒëi·ªÉm s·ªë c·∫£m x√∫c
                y_pos = h - 120
                for emotion, score in sorted(emotion_scores_dict.items(), key=lambda x: x[1], reverse=True):
                    bar_width = int(score * 150)
                    bar_color = (0, 255, 0)  # Default: green

                    if emotion == 'happy':
                        bar_color = (0, 255, 255)  # Yellow
                    elif emotion == 'sad':
                        bar_color = (255, 0, 0)  # Blue
                    elif emotion == 'angry':
                        bar_color = (0, 0, 255)  # Red

                    cv2.rectangle(debug_img, (10, y_pos), (10 + bar_width, y_pos + 15),
                                  bar_color, -1)
                    cv2.putText(debug_img, f"{emotion}: {score:.2f}",
                                (10 + bar_width + 5, y_pos + 12), font, 0.4, (255, 255, 255), 1)
                    y_pos += 20

                # L∆∞u ·∫£nh debug
                emotion_debug_path = f"{debug_dir}/lbp_hog_emotion.jpg"
                cv2.imwrite(emotion_debug_path, cv2.cvtColor(debug_img, cv2.COLOR_RGB2BGR))
                print(f"ƒê√£ l∆∞u ·∫£nh debug t·∫°i: {emotion_debug_path}")

            except Exception as e:
                print(f"LBP/HOG analysis failed: {str(e)}")
                print(f"Chi ti·∫øt: {traceback.format_exc()}")
                print("Falling back to basic emotion analysis...")

                # Ph√¢n t√≠ch ƒë∆°n gi·∫£n d·ª±a tr√™n ƒë·ªô t∆∞∆°ng ph·∫£n v√† ƒë·ªô s√°ng
                gray = cv2.cvtColor(face_img, cv2.COLOR_RGB2GRAY)
                brightness = np.mean(gray) / 255.0
                np.std(gray) / 128.0

                # T√≠nh gradient (texture)
                sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)
                sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)
                gradient_mean = np.mean(np.sqrt(sobelx ** 2 + sobely ** 2)) / 128.0

                # Chia ·∫£nh th√†nh v√πng
                h, w = gray.shape
                top = gray[:h // 3, :]  # V√πng m·∫Øt
                middle = gray[h // 3:2 * h // 3, :]  # V√πng m≈©i
                bottom = gray[2 * h // 3:, :]  # V√πng mi·ªáng

                # T√≠nh c√°c ƒë·∫∑c tr∆∞ng theo v√πng
                np.std(top) / 128.0
                np.std(middle) / 128.0
                bottom_contrast = np.std(bottom) / 128.0

                # Kh·ªüi t·∫°o ƒëi·ªÉm s·ªë c·∫£m x√∫c
                emotion_scores_dict = {
                    'neutral': 0.10,
                    'happy': 0.10,
                    'surprise': 0.10,
                    'sad': 0.10,
                    'angry': 0.10,
                    'fear': 0.10,
                    'disgust': 0.10,
                    'focus': 0.10
                }

                # Ph√¢n t√≠ch ƒë·ªô s√°ng & t∆∞∆°ng ph·∫£n
                if brightness > 0.6:  # S√°ng -> vui/t√≠ch c·ª±c
                    emotion_scores_dict['happy'] += 0.15
                    emotion_scores_dict['neutral'] -= 0.05
                elif brightness < 0.4:  # T·ªëi -> nghi√™m tr·ªçng/ti√™u c·ª±c
                    emotion_scores_dict['sad'] += 0.10
                    emotion_scores_dict['angry'] += 0.05
                    emotion_scores_dict['happy'] -= 0.05

                # Ph√¢n t√≠ch texture
                if gradient_mean > 0.25:  # Texture cao -> bi·ªÉu c·∫£m m·∫°nh
                    emotion_scores_dict['surprise'] += 0.15
                    emotion_scores_dict['happy'] += 0.05
                    emotion_scores_dict['neutral'] -= 0.10

                # Ph√¢n t√≠ch v√πng mi·ªáng
                if bottom_contrast > 0.20:
                    if brightness > 0.45:
                        emotion_scores_dict['happy'] += 0.25
                    else:
                        emotion_scores_dict['surprise'] += 0.20

                # ƒê·∫£m b·∫£o kh√¥ng c√≥ gi√° tr·ªã √¢m
                emotion_scores_dict = {k: max(0.01, v) for k, v in emotion_scores_dict.items()}

                # Chu·∫©n h√≥a
                total = sum(emotion_scores_dict.values())
                emotion_scores_dict = {k: v / total for k, v in emotion_scores_dict.items()}

                # L·∫•y c·∫£m x√∫c ch√≠nh
                dominant_emotion = max(emotion_scores_dict, key=emotion_scores_dict.get)
                dominant_score = emotion_scores_dict[dominant_emotion]

                # T√≠nh c∆∞·ªùng ƒë·ªô
                other_scores = [v for k, v in emotion_scores_dict.items() if k != dominant_emotion]
                avg_other = sum(other_scores) / len(other_scores) if other_scores else 0
                emotion_intensity = max(0.2, min(0.95, 0.5 + (dominant_score - avg_other)))

                print(f"Basic detected emotion: {dominant_emotion}")

        # 6. PH√ÇN T√çCH NG·ªÆ C·∫¢NH TH·ªÇ THAO
        # X√°c ƒë·ªãnh lo·∫°i th·ªÉ thao
        sport_type = "Running"
        if isinstance(sports_analysis, dict):
            if "composition_analysis" in sports_analysis:
                sport_type = sports_analysis["composition_analysis"].get("sport_type", "Unknown")

        # M·ª©c ƒë·ªô h√†nh ƒë·ªông
        action_level = 0
        if "action_analysis" in sports_analysis and "action_level" in sports_analysis["action_analysis"]:
            action_level = sports_analysis["action_analysis"]["action_level"]

        # ƒêi·ªÅu ch·ªânh bi·ªÉu c·∫£m d·ª±a tr√™n ng·ªØ c·∫£nh th·ªÉ thao
        contextual_emotions = emotion_scores_dict.copy()

        # C√°c m√¥n th·ªÉ thao ƒë·ªëi kh√°ng
        combat_sports = ['Boxing', 'Wrestling', 'Martial Arts']
        team_sports = ['Soccer', 'Basketball', 'Baseball', 'Football', 'Ball Sport']
        track_sports = ['Running', 'Track', 'Sprint', 'Athletics']

        # Ph√°t hi·ªán th·ªÉ thao ƒëi·ªÅn kinh t·ª´ ·∫£nh
        if any(name in str(sport_type).lower() for name in ['track', 'run', 'sprint', 'athlet']):
            print("Detected track and field sport from image")
            sport_type = 'Track and Field'

        # ƒêi·ªÅu ch·ªânh theo lo·∫°i th·ªÉ thao
        if sport_type in combat_sports:
            print(f"Adjusting emotions for combat sport: {sport_type}")
            # TƒÉng m·∫°nh c·∫£m x√∫c 'determination', 'angry', v.v. trong th·ªÉ thao ƒë·ªëi kh√°ng
            contextual_emotions['angry'] = contextual_emotions.get('angry', 0) * 1.3
            contextual_emotions['fear'] = contextual_emotions.get('fear', 0) * 1.2
            emotion_intensity = min(0.95, emotion_intensity * 1.2)

        elif sport_type in team_sports:
            print(f"Adjusting emotions for team sport: {sport_type}")
            # TƒÉng c·∫£m x√∫c vui m·ª´ng/th·∫•t v·ªçng trong th·ªÉ thao ƒë·ªìng ƒë·ªôi
            if action_level > 0.5:  # H√†nh ƒë·ªông cao
                contextual_emotions['happy'] = contextual_emotions.get('happy', 0) * 1.2
                contextual_emotions['surprise'] = contextual_emotions.get('surprise', 0) * 1.2

        # ƒêi·ªÅu ch·ªânh cho m√¥n ƒëi·ªÅn kinh v·ªõi c·∫£m x√∫c ph√π h·ª£p h∆°n
        elif sport_type in track_sports or 'Track and Field' in sport_type:
            print(f"Adjusting emotions for track sport")
            # Gi·∫£m happy v√† tƒÉng determination/effort
            contextual_emotions['happy'] = contextual_emotions.get('happy', 0) * 0.9  # Gi·∫£m happy

            # N·∫øu ph√°t hi·ªán angry ho·∫∑c neutral cao, ƒë·ªïi th√†nh determination
            if contextual_emotions.get('angry', 0) > 0.2 or contextual_emotions.get('neutral', 0) > 0.3:
                # T·∫°o c·∫£m x√∫c determination t·ª´ angry
                determination_score = contextual_emotions.get('angry', 0) * 1.8
                contextual_emotions['determination'] = determination_score

                # N·∫øu determination l√† c·∫£m x√∫c ch√≠nh
                if determination_score > max([v for k, v in contextual_emotions.items() if k != 'determination']):
                    dominant_emotion = 'determination'
                    print("Changed main emotion to 'determination' based on track sports context")

        # Chuy·ªÉn ƒë·ªïi "neutral" th√†nh "focus" trong b·ªëi c·∫£nh th·ªÉ thao
        if dominant_emotion == 'neutral' and emotion_intensity > 0.5:
            # Xem x√©t c√°c ƒëi·ªÅu ki·ªán ƒë·ªÉ x√°c ƒë·ªãnh c√≥ ph·∫£i ƒëang t·∫≠p trung hay kh√¥ng
            is_focus = False

            # ƒêi·ªÅu ki·ªán 1: ƒêang trong m√¥i tr∆∞·ªùng th·ªÉ thao v√† c√≥ m·ª©c ƒë·ªô h√†nh ƒë·ªông cao
            if action_level > 0.5:
                is_focus = True

            # ƒêi·ªÅu ki·ªán 2: Trong m√¥n th·ªÉ thao ƒë·ªìng ƒë·ªôi ho·∫∑c v·ªõi b√≥ng
            if sport_type in team_sports or "ball" in str(sport_type).lower():
                is_focus = True

            # ƒêi·ªÅu ki·ªán 3: Ki·ªÉm tra c∆∞·ªùng ƒë·ªô c·∫£m x√∫c v√† ƒë·ªô tin c·∫≠y
            if emotion_intensity > 0.7:
                is_focus = True

            # N·∫øu th·ªèa m√£n ƒëi·ªÅu ki·ªán, thay ƒë·ªïi neutral th√†nh focus
            if is_focus:
                dominant_emotion = 'focus'
                print("Changed 'neutral' to 'focus' based on sports context")

                # C·∫≠p nh·∫≠t ƒëi·ªÉm s·ªë c·∫£m x√∫c
                if 'neutral' in contextual_emotions:
                    contextual_emotions['focus'] = contextual_emotions.pop('neutral', dominant_score)
                else:
                    contextual_emotions['focus'] = dominant_score

                # C≈©ng c·∫≠p nh·∫≠t trong emotion_scores_dict g·ªëc n·∫øu c·∫ßn
                if 'neutral' in emotion_scores_dict:
                    emotion_scores_dict['focus'] = emotion_scores_dict.pop('neutral', dominant_score)
                else:
                    emotion_scores_dict['focus'] = dominant_score

        # 7. PH√ÇN T√çCH M·ª®C ƒê·ªò C·∫¢M X√öC
        emotional_value = 'Moderate'
        if emotion_intensity < 0.4:
            emotional_value = 'Low'
        elif emotion_intensity > 0.7:
            emotional_value = 'High'

        # Th√™m k·∫øt qu·∫£ v√†o expression_results
        expression_results['has_faces'] = True
        expression_results['dominant_emotion'] = dominant_emotion
        expression_results['emotion_intensity'] = float(emotion_intensity)
        expression_results['emotional_value'] = emotional_value
        expression_results['emotion_scores'] = emotion_scores_dict
        expression_results['contextual_scores'] = contextual_emotions
        expression_results['expressions'] = [{'emotion': dominant_emotion, 'score': float(dominant_score)}]
        expression_results['debug_info']['detected'] = True
        expression_results['face_path'] = best_face_path
        expression_results['face_coordinates'] = (fx, fy, fw, fh)
        expression_results['sport_context'] = sport_type
        expression_results['action_level'] = action_level

        # Th√™m th√¥ng tin ƒë·ªëi t∆∞·ª£ng ch√≠nh
        expression_results['main_subject_idx'] = main_subject_idx
        expression_results['main_subject_box'] = main_subject['box']

        print(f"Advanced facial expression analysis successful: {face_found}")
        return expression_results

    except Exception as e:
        print(f"Error in advanced facial expression analysis: {str(e)}")
        import traceback
        traceback.print_exc()
        return {'has_faces': False, 'error': str(e), 'debug_info': {'reason': f"Error: {str(e)}"}}

def visualize_emotion_results(face_img, emotion_analysis):
    """T·∫°o hi·ªÉn th·ªã chuy√™n nghi·ªáp cho ph√¢n t√≠ch bi·ªÉu c·∫£m khu√¥n m·∫∑t"""
    # N·∫øu kh√¥ng c√≥ ph√¢n t√≠ch c·∫£m x√∫c, t·∫°o h√¨nh ·∫£nh th√¥ng b√°o l·ªói
    if face_img is None or emotion_analysis is None or not emotion_analysis.get('has_faces', False):
        # T·∫°o h√¨nh ·∫£nh tr·ªëng v·ªõi th√¥ng b√°o
        fig = plt.figure(figsize=(6, 4), dpi=100)

        # Th√™m ti√™u ƒë·ªÅ
        error_message = "NO FACE DETECTED"
        detail_message = "Cannot analyze facial expression"

        # Truy t√¨m l√Ω do l·ªói chi ti·∫øt h∆°n
        if emotion_analysis:
            if 'error' in emotion_analysis:
                detail_message = emotion_analysis['error']

            if 'debug_info' in emotion_analysis and 'reason' in emotion_analysis['debug_info']:
                detail_message = emotion_analysis['debug_info']['reason']

        # T·∫°o h√¨nh ·∫£nh th√¥ng b√°o
        ax = fig.add_subplot(111)
        ax.text(0.5, 0.6, error_message,
                fontsize=16, color='red', fontweight='bold',
                ha='center', va='center')
        ax.text(0.5, 0.4, detail_message,
                fontsize=12, ha='center', va='center',
                wrap=True)
        ax.axis('off')

        # Chuy·ªÉn ƒë·ªïi figure th√†nh ·∫£nh
        fig.canvas.draw()
        img_data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
        img_data = img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))

        # ƒê√≥ng figure ƒë·ªÉ tr√°nh memory leak
        plt.close(fig)

        return img_data

    # ƒê·∫£m b·∫£o k√≠ch th∆∞·ªõc h·ª£p l√Ω cho ·∫£nh khu√¥n m·∫∑t
    h, w = face_img.shape[:2]
    display_width = 300
    display_height = int((h / w) * display_width) if w > 0 else 300

    # ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc ·∫£nh khu√¥n m·∫∑t cho ph√π h·ª£p
    face_display = cv2.resize(face_img, (display_width, display_height),
                              interpolation=cv2.INTER_AREA)

    # L·∫•y th√¥ng tin c·∫£m x√∫c
    emotion = emotion_analysis.get('dominant_emotion', 'unknown')
    intensity = emotion_analysis.get('emotion_intensity', 0)
    original_emotion = emotion_analysis.get('original_emotion', emotion)

    # ƒê·ªãnh nghƒ©a m√†u d·ª±a tr√™n lo·∫°i c·∫£m x√∫c (d·∫°ng RGB cho matplotlib)
    emotion_colors = {
        'happy': (0.0, 0.8, 0.2),  # Green
        'happiness': (0.0, 0.8, 0.2),  # Green
        'surprise': (1.0, 0.7, 0.0),  # Orange
        'sad': (0.0, 0.0, 0.8),  # Blue
        'sadness': (0.0, 0.0, 0.8),  # Blue
        'angry': (0.8, 0.0, 0.0),  # Red
        'anger': (0.8, 0.0, 0.0),  # Red
        'fear': (0.8, 0.0, 0.8),  # Purple
        'disgust': (0.5, 0.4, 0.0),  # Brown
        'neutral': (0.5, 0.5, 0.5),  # Gray
        'contempt': (0.6, 0.0, 0.6),  # Dark purple
        'unknown': (0.7, 0.7, 0.7)  # Light Gray
    }

    # M√†u d·ª±a tr√™n c·∫£m x√∫c ph√°t hi·ªán ƒë∆∞·ª£c
    color = emotion_colors.get(emotion.lower(), (0.7, 0.7, 0.7))

    # T·∫°o figure ƒë·ªÉ hi·ªÉn th·ªã v·ªõi k√≠ch th∆∞·ªõc h·ª£p l√Ω
    fig = plt.figure(figsize=(6, 8), dpi=100)

    # Th√™m ti√™u ƒë·ªÅ v·ªõi m√†u theo c·∫£m x√∫c
    emotion_title = f"Face: {emotion.upper()} ({intensity:.2f})"
    if original_emotion != emotion and original_emotion != "unknown":
        emotion_title += f"\nOriginal: {original_emotion.upper()}"

    fig.suptitle(emotion_title, fontsize=16, color=color, fontweight='bold')

    # Thi·∫øt l·∫≠p layout - 2 d√≤ng, 1 c·ªôt
    gs = plt.GridSpec(2, 1, height_ratios=[3, 1], figure=fig)

    # Hi·ªÉn th·ªã ·∫£nh khu√¥n m·∫∑t
    ax_face = fig.add_subplot(gs[0])
    ax_face.imshow(face_display)

    # Th√™m vi·ªÅn m√†u xung quanh khu√¥n m·∫∑t
    border_width = 5
    for spine in ax_face.spines.values():
        spine.set_linewidth(border_width)
        spine.set_color(color)

    ax_face.set_xticks([])
    ax_face.set_yticks([])

    # Th√™m bi·ªÉu ƒë·ªì c·∫£m x√∫c n·∫øu c√≥ ƒëi·ªÉm s·ªë
    scores = emotion_analysis.get('contextual_scores', emotion_analysis.get('emotion_scores', {}))
    if scores:
        ax_chart = fig.add_subplot(gs[1])

        # S·∫Øp x·∫øp c·∫£m x√∫c theo ƒëi·ªÉm s·ªë
        emotions = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
        values = [scores[e] for e in emotions]

        # R√∫t ng·∫Øn t√™n c·∫£m x√∫c ƒë·ªÉ hi·ªÉn th·ªã g·ªçn h∆°n
        display_labels = [e[:3].upper() if len(e) > 3 else e.upper() for e in emotions]

        # T·∫°o m√†u cho t·ª´ng c·∫£m x√∫c
        bar_colors = [emotion_colors.get(e.lower(), (0.7, 0.7, 0.7)) for e in emotions]

        # V·∫Ω bi·ªÉu ƒë·ªì thanh ngang ƒë·ªÉ d·ªÖ ƒë·ªçc h∆°n
        bars = ax_chart.barh(display_labels, values, color=bar_colors, alpha=0.7)

        # Th√™m gi√° tr·ªã l√™n m·ªói thanh
        for i, bar in enumerate(bars):
            width = bar.get_width()
            ax_chart.text(width + 0.01, bar.get_y() + bar.get_height() / 2,
                          f'{width:.2f}', ha='left', va='center', fontweight='bold')

        # ƒê·∫∑t gi·ªõi h·∫°n tr·ª•c x t·ª´ 0 ƒë·∫øn 1
        ax_chart.set_xlim(0, 1.0)

        # Ti√™u ƒë·ªÅ nh·ªè cho bi·ªÉu ƒë·ªì
        ax_chart.set_title('Emotion Scores', fontsize=12)

        # Th√™m l∆∞·ªõi ƒë·ªÉ d·ªÖ ƒë·ªçc
        ax_chart.grid(axis='x', linestyle='--', alpha=0.7)

    plt.tight_layout()

    # Chuy·ªÉn ƒë·ªïi figure th√†nh ·∫£nh
    fig.canvas.draw()
    img_data = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)
    img_data = img_data.reshape(fig.canvas.get_width_height()[::-1] + (3,))

    # ƒê√≥ng figure ƒë·ªÉ tr√°nh memory leak
    plt.close(fig)

    return img_data


def visualize_sports_results(img_data, detections, depth_map, sports_analysis, action_analysis, composition_analysis,
                             facial_analysis=None, caption=None):
    """Create sports-specific visualization with enhanced main subject highlighting, emotion analysis and caption"""
    # Th√™m debug ƒë·ªÉ x√°c ƒë·ªãnh ID c·ªßa bi·∫øn
    print(f"DEBUG D - ID c·ªßa sports_analysis trong visualize: {id(sports_analysis)}")
    print(
        f"DEBUG - sports_analysis keys trong visualize: {sports_analysis.keys() if isinstance(sports_analysis, dict) else type(sports_analysis)}")

    img = np.array(img_data['resized']).copy()
    height, width = img.shape[:2]

    # T√¨m ƒë·ªëi t∆∞·ª£ng ch√≠nh (ng∆∞·ªùi) t·ª´ key_subjects
    main_person = None
    main_person_idx = -1

    if "key_subjects" in sports_analysis and sports_analysis['key_subjects']:
        # L·∫•y ch√≠nh x√°c ƒë·ªëi t∆∞·ª£ng ƒë·∫ßu ti√™n n·∫øu l√† ng∆∞·ªùi
        if sports_analysis['key_subjects'][0]['class'] == 'person':
            main_person = sports_analysis['key_subjects'][0]
            main_person_idx = 0
        else:
            # N·∫øu kh√¥ng, t√¨m ng∆∞·ªùi ƒë·∫ßu ti√™n trong danh s√°ch
            for idx, subject in enumerate(sports_analysis['key_subjects']):
                if subject['class'] == 'person':
                    main_person = subject
                    main_person_idx = idx
                    break

    # T·∫°o visual cho detection v·ªõi sharpness
    det_viz = img.copy()

    # T·∫°o mask ƒë·ªÉ l√†m n·ªïi b·∫≠t ƒë·ªëi t∆∞·ª£ng ch√≠nh
    highlight_mask = np.zeros_like(img)
    main_obj_viz = img.copy()

    # L·∫•y ƒëi·ªÉm s·ªë s·∫Øc n√©t
    sharpness_scores = sports_analysis.get('sharpness_scores', [])

    # Draw bounding boxes
    for i, box in enumerate(detections['boxes']):
        x1, y1, x2, y2 = box
        label = detections['classes'][i]
        conf = detections['scores'][i]
        sharpness = sharpness_scores[i] if i < len(sharpness_scores) else 0
        object_id = i + 1

        # Determine if this is the main person
        is_main_person = False
        if main_person is not None and i == main_person_idx:
            is_main_person = True

        # Different colors for different classes
        if label == 'person':
            if is_main_person:
                # Highlight ƒë·ªëi t∆∞·ª£ng ch√≠nh v·ªõi m√†u s√°ng v√† ƒë·∫≠m h∆°n
                color = (0, 255, 255)  # V√†ng cho ng∆∞·ªùi ch√≠nh
                border_thickness = 4
                font_scale = 0.7

                # T·∫°o mask cho v√πng ƒë·ªëi t∆∞·ª£ng ch√≠nh
                highlight_mask[y1:y2, x1:x2] = img[y1:y2, x1:x2]

                # V·∫Ω spotlight effect xung quanh ng∆∞·ªùi ch√≠nh
                cv2.rectangle(main_obj_viz, (x1 - 5, y1 - 5), (x2 + 5, y2 + 5), (0, 255, 255), 8)

                # Th√™m t√™n nh√£n n·ªïi b·∫≠t h∆°n v·ªõi ID
                cv2.putText(main_obj_viz, f"ID:{object_id} MAIN SUBJECT", (x1, y1 - 25),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 255), 2)
            else:
                color = (0, 255, 0)  # Green for other people
                border_thickness = 2
                font_scale = 0.5
        elif 'ball' in label:
            color = (0, 0, 255)  # Red for balls
            border_thickness = 2
            font_scale = 0.5
        else:
            color = (255, 0, 0)  # Blue for other equipment
            border_thickness = 2
            font_scale = 0.5

        cv2.rectangle(det_viz, (x1, y1), (x2, y2), color, border_thickness)

        # Draw label with sharpness
        label_y = y1 - 10 if y1 > 20 else y1 + 20
        cv2.putText(det_viz, f"ID:{object_id} {label} {conf:.2f} S:{sharpness:.2f}", (x1, label_y),
                    cv2.FONT_HERSHEY_SIMPLEX, font_scale, color, 2)

    # T·∫°o hi·ªáu ·ª©ng l√†m m·ªù h√¨nh n·ªÅn v√† l√†m n·ªïi b·∫≠t ƒë·ªëi t∆∞·ª£ng ch√≠nh
    if main_person is not None:
        # Blur background
        blurred_bg = cv2.GaussianBlur(img, (25, 25), 0)

        # T·∫°o mask cho v√πng ng∆∞·ªùi ch√≠nh
        person_mask = np.zeros((height, width), dtype=np.uint8)

        # S·ª≠ d·ª•ng mask chi ti·∫øt t·ª´ segmentation n·∫øu c√≥
        if 'main_subject_mask' in sports_analysis and sports_analysis['main_subject_mask'] is not None:
            main_mask = sports_analysis['main_subject_mask']

            # Resize mask n·∫øu k√≠ch th∆∞·ªõc kh√°c v·ªõi ·∫£nh
            if main_mask.shape[:2] != (height, width):
                main_mask = cv2.resize(main_mask, (width, height))

            # Chuy·ªÉn mask v·ªÅ binary
            person_mask = (main_mask > 0.5).astype(np.uint8) * 255
        else:
            # S·ª≠ d·ª•ng bounding box n·∫øu kh√¥ng c√≥ mask
            x1, y1, x2, y2 = main_person['box']
            person_mask[y1:y2, x1:x2] = 255

        # Th√™m m·ªôt border tr∆°n m∆∞·ª£t
        kernel = np.ones((15, 15), np.uint8)
        dilated_mask = cv2.dilate(person_mask, kernel, iterations=1)
        blur_border = cv2.GaussianBlur(dilated_mask, (21, 21), 0)

        # Chuy·ªÉn ƒë·ªïi mask th√†nh 3 k√™nh
        person_mask_3ch = cv2.cvtColor(person_mask, cv2.COLOR_GRAY2BGR)
        blur_border_3ch = cv2.cvtColor(blur_border, cv2.COLOR_GRAY2BGR) / 255.0

        # K·∫øt h·ª£p h√¨nh n·ªÅn m·ªù v√† ƒë·ªëi t∆∞·ª£ng ch√≠nh s·∫Øc n√©t
        main_highlight = blurred_bg.copy()
        main_highlight = np.where(person_mask_3ch > 0, img, main_highlight)

        # Th√™m hi·ªáu ·ª©ng glow xung quanh ƒë·ªëi t∆∞·ª£ng ch√≠nh
        glow_effect = np.where(blur_border_3ch > 0,
                               img * blur_border_3ch + blurred_bg * (1 - blur_border_3ch),
                               blurred_bg)

        # ƒê√°nh d·∫•u box v√† th√™m nh√£n
        color = (0, 255, 255)  # Yellow for main person
        #cv2.rectangle(main_highlight, (x1, y1), (x2, y2), color, 3)
        #cv2.putText(main_highlight, "MAIN SUBJECT", (x1, y1 - 10),
        #            cv2.FONT_HERSHEY_SIMPLEX, 0.8, color, 2)
    else:
        main_highlight = img.copy()

    # Create composition analysis visualization
    comp_viz = img.copy()

    # Draw rule of thirds grid
    for i in range(1, 3):
        cv2.line(comp_viz, (0, int(height * i / 3)), (width, int(height * i / 3)), (255, 255, 255), 1)
        cv2.line(comp_viz, (int(width * i / 3), 0), (int(width * i / 3), height), (255, 255, 255), 1)

    # Draw key subjects with prominence
    if "key_subjects" in sports_analysis:
        for idx, subject in enumerate(sports_analysis['key_subjects']):
            box = subject['box']
            x1, y1, x2, y2 = box

            # X√°c ƒë·ªãnh n·∫øu l√† ƒë·ªëi t∆∞·ª£ng ch√≠nh (ng∆∞·ªùi)
            is_main_subject = (idx == main_person_idx)
            subject_id = idx + 1

            if is_main_subject:
                # Highlight ƒë·ªëi t∆∞·ª£ng ch√≠nh v·ªõi m√†u n·ªïi b·∫≠t
                color = (0, 255, 255)  # Yellow
                thickness = 3
            else:
                # Color based on prominence - more red = more important
                prominence = min(1.0, subject['prominence'] * 10)  # Scale for visibility
                color = (0, int(255 * (1 - prominence)), int(255 * prominence))
                thickness = 2

            cv2.rectangle(comp_viz, (x1, y1), (x2, y2), color, thickness)

            # Hi·ªÉn th·ªã ID, ƒëi·ªÉm s·∫Øc n√©t v√† ch·ªâ s·ªë prominence
            label_text = f"ID:{subject_id} P:{subject['prominence']:.2f} S:{subject.get('sharpness', 0):.2f}"
            if is_main_subject:
                label_text = f"ID:{subject_id} MAIN P:{subject['prominence']:.2f} S:{subject.get('sharpness', 0):.2f}"

            cv2.putText(comp_viz, label_text,
                        (x1, y2 + 15), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # T·∫°o heatmap ƒë·ªô s·∫Øc n√©t c·∫£i ti·∫øn
    try:
        # T·∫°o sharpness heatmap cho to√†n b·ªô ·∫£nh
        sharpness_overlay, sharpness_heatmap_raw = create_sharpness_heatmap(img)
        sharpness_viz = sharpness_overlay.copy()

        # S·ª≠ d·ª•ng colormap
        from matplotlib import cm
        jet_colormap = cm.get_cmap('jet')

        # V·∫Ω bounding boxes v·ªõi sharpness scores
        for i, box in enumerate(detections['boxes']):
            if i < len(sharpness_scores):
                x1, y1, x2, y2 = box
                sharpness = sharpness_scores[i]

                # M√†u d·ª±a tr√™n ƒë·ªô s·∫Øc n√©t
                color_rgba = jet_colormap(sharpness)
                color_rgb = tuple(int(255 * c) for c in color_rgba[:3])
                color = (color_rgb[2], color_rgb[1], color_rgb[0])  # BGR for OpenCV

                # V·∫Ω border d√†y h∆°n cho objects c√≥ sharpness cao
                border_thickness = max(2, int(sharpness * 5))
                cv2.rectangle(sharpness_viz, (x1, y1), (x2, y2), color, border_thickness)

                # Th√™m nh√£n v·ªõi background
                label_text = f"Sharp: {sharpness:.2f}"
                (label_w, label_h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)

                # Background cho text
                cv2.rectangle(sharpness_viz, (x1, y1 - label_h - 10), (x1 + label_w, y1), color, -1)
                cv2.putText(sharpness_viz, label_text, (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

        print("Sharpness heatmap created successfully")

    except Exception as e:
        print(f"Error creating sharpness heatmap: {e}")
        # Fallback to original method
        try:
            # T·∫°o sharpness heatmap cho to√†n b·ªô ·∫£nh
            sharpness_overlay, sharpness_heatmap_raw = create_sharpness_heatmap(img)
            sharpness_viz = sharpness_overlay.copy()

            # S·ª≠ d·ª•ng colormap
            from matplotlib import cm
            jet_colormap = cm.get_cmap('jet')

            # V·∫Ω bounding boxes v·ªõi sharpness scores
            for i, box in enumerate(detections['boxes']):
                if i < len(sharpness_scores):
                    x1, y1, x2, y2 = box
                    sharpness = sharpness_scores[i]

                    # M√†u d·ª±a tr√™n ƒë·ªô s·∫Øc n√©t
                    color_rgba = jet_colormap(sharpness)
                    color_rgb = tuple(int(255 * c) for c in color_rgba[:3])
                    color = (color_rgb[2], color_rgb[1], color_rgb[0])  # BGR for OpenCV

                    # V·∫Ω border d√†y h∆°n cho objects c√≥ sharpness cao
                    border_thickness = max(2, int(sharpness * 5))
                    cv2.rectangle(sharpness_viz, (x1, y1), (x2, y2), color, border_thickness)

                    # Th√™m nh√£n v·ªõi background
                    label_text = f"Sharp: {sharpness:.2f}"
                    (label_w, label_h), _ = cv2.getTextSize(label_text, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)

                    # Background cho text
                    cv2.rectangle(sharpness_viz, (x1, y1 - label_h - 10), (x1 + label_w, y1), color, -1)
                    cv2.putText(sharpness_viz, label_text, (x1, y1 - 5),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)

            print("Sharpness heatmap created successfully")

        except Exception as e:
            print(f"Error creating sharpness heatmap: {e}")
            # Fallback to original method
            sharpness_viz = img.copy()

            from matplotlib import cm
            jet_colormap = cm.get_cmap('jet')

            for i, box in enumerate(detections['boxes']):
                if i < len(sharpness_scores):
                    x1, y1, x2, y2 = box
                    sharpness = sharpness_scores[i]

                    color_rgba = jet_colormap(sharpness)
                    color_rgb = tuple(int(255 * c) for c in color_rgba[:3])
                    color = (color_rgb[2], color_rgb[1], color_rgb[0])

                    overlay = sharpness_viz.copy()
                    cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
                    alpha = 0.4 + 0.3 * sharpness
                    cv2.addWeighted(overlay, alpha, sharpness_viz, 1 - alpha, 0, sharpness_viz)

                    cv2.rectangle(sharpness_viz, (x1, y1), (x2, y2), color, 3)
                    cv2.putText(sharpness_viz, f"Sharp: {sharpness:.2f}",
                                (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

        from matplotlib import cm
        jet_colormap = cm.get_cmap('jet')

        for i, box in enumerate(detections['boxes']):
            if i < len(sharpness_scores):
                x1, y1, x2, y2 = box
                sharpness = sharpness_scores[i]

                color_rgba = jet_colormap(sharpness)
                color_rgb = tuple(int(255 * c) for c in color_rgba[:3])
                color = (color_rgb[2], color_rgb[1], color_rgb[0])

                overlay = sharpness_viz.copy()
                cv2.rectangle(overlay, (x1, y1), (x2, y2), color, -1)
                alpha = 0.4 + 0.3 * sharpness
                cv2.addWeighted(overlay, alpha, sharpness_viz, 1 - alpha, 0, sharpness_viz)

                cv2.rectangle(sharpness_viz, (x1, y1), (x2, y2), color, 3)
                cv2.putText(sharpness_viz, f"Sharp: {sharpness:.2f}",
                            (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

    # TH√äM M·ªöI: Hi·ªÉn th·ªã keypoints t·ª´ pose estimation n·∫øu c√≥
    pose_viz = img.copy()

    # Ki·ªÉm tra x√°c th·ª±c c·∫•u tr√∫c d·ªØ li·ªáu pose_analysis
    # Ki·ªÉm tra x√°c th·ª±c c·∫•u tr√∫c d·ªØ li·ªáu pose_analysis
    if 'pose_analysis' in sports_analysis and isinstance(sports_analysis['pose_analysis'], dict) and 'poses' in \
            sports_analysis['pose_analysis']:
        poses = sports_analysis['pose_analysis']['poses']

        # CH·ªà l·∫•y m·ªôt pose duy nh·∫•t - pose c·ªßa main subject
        main_subject_pose = None

        # N·∫øu c√≥ main subject mask th√¨ d√πng mask ƒë·ªÉ t√¨m pose
        main_subject_pose = None

        # Ki·ªÉm tra xem c√≥ main subject mask kh√¥ng
        if 'main_subject_mask' in sports_analysis and sports_analysis['main_subject_mask'] is not None:
            main_mask = sports_analysis['main_subject_mask']

            # ƒê·∫£m b·∫£o mask c√≥ k√≠ch th∆∞·ªõc ph√π h·ª£p v·ªõi ·∫£nh
            if main_mask.shape[:2] != (height, width):
                # Resize mask n·∫øu c·∫ßn
                main_mask = cv2.resize(main_mask, (width, height))

            best_pose = None
            max_in_mask = 0
            max_ratio = 0

            print(f"ƒêang ki·ªÉm tra {len(poses)} poses v·ªõi mask")

            # Duy·ªát qua t·ª´ng pose v√† ki·ªÉm tra keypoints trong mask
            for idx, pose in enumerate(poses):
                if 'keypoints' in pose and pose['keypoints']:
                    # ƒê·∫øm s·ªë keypoint n·∫±m trong mask
                    count_in_mask = 0
                    total_keypoints = len(pose['keypoints'])

                    for kp in pose['keypoints']:
                        if kp['confidence'] < 0.2:  # B·ªè qua keypoints c√≥ ƒë·ªô tin c·∫≠y th·∫•p
                            continue

                        x, y = int(kp['x']), int(kp['y'])

                        # Ki·ªÉm tra x,y c√≥ n·∫±m trong ph·∫°m vi mask kh√¥ng
                        if 0 <= x < width and 0 <= y < height:
                            # Ki·ªÉm tra keypoint c√≥ n·∫±m trong mask kh√¥ng
                            if main_mask[y, x] > 0.5:
                                count_in_mask += 1

                    # T√≠nh t·ª∑ l·ªá keypoints trong mask
                    ratio = count_in_mask / total_keypoints if total_keypoints > 0 else 0

                    print(f"Pose {idx}: {count_in_mask}/{total_keypoints} keypoints trong mask ({ratio * 100:.1f}%)")

                    # Ch·ªâ ch·ªçn pose c√≥ √≠t nh·∫•t 30% keypoints trong mask
                    if ratio > max_ratio and ratio > 0.3:
                        max_ratio = ratio
                        max_in_mask = count_in_mask
                        best_pose = pose
                        print(f"  -> Pose {idx} hi·ªán l√† pose t·ªët nh·∫•t v·ªõi {ratio * 100:.1f}% keypoints trong mask")

            if best_pose:
                print(f"ƒê√£ t√¨m th·∫•y pose ph√π h·ª£p v·ªõi mask: {max_in_mask} keypoints, {max_ratio * 100:.1f}%")
                main_subject_pose = best_pose
            else:
                print(f"Kh√¥ng t√¨m th·∫•y pose n√†o c√≥ ƒë·ªß keypoints trong mask (ng∆∞·ª°ng 30%)")

        # Backup: N·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c pose d·ª±a tr√™n mask, th·ª≠ d√πng IoU v·ªõi box
        if main_subject_pose is None and main_person is not None:
            print("Th·ª≠ t√¨m pose d·ª±a tr√™n IoU v·ªõi bounding box")
            main_x1, main_y1, main_x2, main_y2 = main_person['box']
            best_iou = 0

            for pose in poses:
                if 'bbox' in pose and pose['bbox']:
                    p_x1, p_y1, p_x2, p_y2 = pose['bbox']

                    # T√≠nh IoU (Intersection over Union)
                    x_left = max(main_x1, p_x1)
                    y_top = max(main_y1, p_y1)
                    x_right = min(main_x2, p_x2)
                    y_bottom = min(main_y2, p_y2)

                    if x_right <= x_left or y_bottom <= y_top:
                        continue

                    intersection = (x_right - x_left) * (y_bottom - y_top)
                    main_area = (main_x2 - main_x1) * (main_y2 - main_y1)
                    pose_area = (p_x2 - p_x1) * (p_y2 - p_y1)
                    union = main_area + pose_area - intersection

                    iou = intersection / union if union > 0 else 0
                    print(f"IoU v·ªõi pose bbox: {iou:.2f}")

                    if iou > best_iou and iou > 0.3:
                        best_iou = iou
                        main_subject_pose = pose

        # Backup: n·∫øu kh√¥ng t√¨m ƒë∆∞·ª£c b·∫±ng mask, d√πng bounding box
        if main_subject_pose is None and main_person is not None:
            main_x1, main_y1, main_x2, main_y2 = main_person['box']
            (main_x1 + main_x2) / 2
            (main_y1 + main_y2) / 2

            # T√¨m pose c√≥ bbox tr√πng nhi·ªÅu nh·∫•t v·ªõi main person box
            best_iou = 0
            for pose in poses:
                if 'bbox' in pose and pose['bbox']:
                    p_x1, p_y1, p_x2, p_y2 = pose['bbox']

                    # T√≠nh IoU
                    x_left = max(main_x1, p_x1)
                    y_top = max(main_y1, p_y1)
                    x_right = min(main_x2, p_x2)
                    y_bottom = min(main_y2, p_y2)

                    if x_right < x_left or y_bottom < y_top:
                        continue

                    intersection = (x_right - x_left) * (y_bottom - y_top)
                    main_area = (main_x2 - main_x1) * (main_y2 - main_y1)
                    pose_area = (p_x2 - p_x1) * (p_y2 - p_y1)
                    union = main_area + pose_area - intersection

                    iou = intersection / union if union > 0 else 0

                    if iou > best_iou:
                        best_iou = iou
                        main_subject_pose = pose

        # N·∫øu t√¨m ƒë∆∞·ª£c pose c·ªßa main subject, ch·ªâ x·ª≠ l√Ω pose ƒë√≥
        if main_subject_pose:
            poses = [main_subject_pose]  # Ch·ªâ x·ª≠ l√Ω pose c·ªßa main subject
        else:
            poses = []  # Kh√¥ng c√≥ pose n√†o kh·ªõp v·ªõi main subject

        # N·∫øu kh√¥ng t√¨m th·∫•y theo IoU, l·∫•y ng∆∞·ªùi c√≥ bbox l·ªõn nh·∫•t/·ªü gi·ªØa nh·∫•t
        if main_subject_pose is None and poses:
            largest_area = 0
            center_pose = None

            for pose in poses:
                if 'bbox' in pose and pose['bbox']:
                    p_x1, p_y1, p_x2, p_y2 = pose['bbox']
                    area = (p_x2 - p_x1) * (p_y2 - p_y1)

                    # ∆Øu ti√™n ng∆∞·ªùi ·ªü gi·ªØa
                    p_center_x = (p_x1 + p_x2) / 2
                    p_center_y = (p_y1 + p_y2) / 2
                    center_score = 1 - (abs(p_center_x - width / 2) / width + abs(p_center_y - height / 2) / height) / 2

                    # K·∫øt h·ª£p di·ªán t√≠ch v√† v·ªã tr√≠
                    score = area * center_score

                    if score > largest_area:
                        largest_area = score
                        center_pose = pose

            main_subject_pose = center_pose

        # X·ª≠ l√Ω ch·ªâ v·ªõi main_subject_pose
        if main_subject_pose:
            poses = [main_subject_pose]  # Ch·ªâ x·ª≠ l√Ω pose c·ªßa main subject

        # ƒê·ªãnh nghƒ©a c√°c c·∫∑p keypoint ƒë·ªÉ v·∫Ω khung x∆∞∆°ng
        skeleton = [
            (5, 7), (7, 9),  # Left arm
            (6, 8), (8, 10),  # Right arm
            (5, 6),  # Shoulders
            (5, 11), (6, 12),  # Body
            (11, 13), (13, 15),  # Left leg
            (12, 14), (14, 16),  # Right leg
            (11, 12)  # Hips
        ]

        print(f"S·ªë ng∆∞·ªùi ƒë∆∞·ª£c ph√°t hi·ªán pose: {len(poses)}")

        for person in poses:
            # Ki·ªÉm tra v√† debug th√¥ng tin keypoints
            print(f"S·ªë keypoints c·ªßa ng∆∞·ªùi: {len(person['keypoints'])}")

            # T·∫°o dict ƒë·ªÉ l∆∞u c√°c keypoint theo id
            keypoints = {kp['id']: (int(kp['x']), int(kp['y'])) for kp in person['keypoints']}

            # V·∫Ω c√°c keypoint
            for kp in person['keypoints']:
                if kp['confidence'] < 0.2:  # B·ªè qua c√°c ƒëi·ªÉm c√≥ ƒë·ªô tin c·∫≠y qu√° th·∫•p
                    continue

                x, y = int(kp['x']), int(kp['y'])

                # M√†u s·∫Øc cho c√°c keypoint kh√°c nhau
                if kp['id'] <= 4:  # V√πng ƒë·∫ßu
                    color = (255, 0, 0)  # Xanh d∆∞∆°ng
                elif 5 <= kp['id'] <= 10:  # V√πng tay
                    color = (0, 255, 255)  # V√†ng
                else:  # V√πng ch√¢n
                    color = (0, 255, 0)  # Xanh l√°

                # V·∫Ω ƒëi·ªÉm l·ªõn h∆°n, v·ªõi vi·ªÅn ƒëen ƒë·ªÉ d·ªÖ nh√¨n h∆°n
                cv2.circle(pose_viz, (x, y), 7, (0, 0, 0), -1)  # Vi·ªÅn ƒëen
                cv2.circle(pose_viz, (x, y), 5, color, -1)  # ƒêi·ªÉm m√†u

                # T·∫ÆT hi·ªÉn th·ªã t√™n keypoint ƒë·ªÉ tr√°nh r·ªëi m·∫Øt
                # Ch·ªâ hi·ªÉn th·ªã confidence b√™n c·∫°nh ƒëi·ªÉm
                # if kp['confidence'] > 0.6:  # Ch·ªâ hi·ªÉn th·ªã cho ƒëi·ªÉm c√≥ ƒë·ªô tin c·∫≠y cao
                #     cv2.putText(pose_viz, f"{kp['confidence']:.2f}", (x+5, y),
                #                 cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)

            # V·∫Ω skeleton
            for kp1_id, kp2_id in skeleton:
                # CH·ªà k·∫øt n·ªëi c√°c keypoint c√≥ confidence ƒë·ªß cao
                if kp1_id in keypoints and kp2_id in keypoints:
                    pt1 = keypoints[kp1_id]
                    pt2 = keypoints[kp2_id]

                    # Ki·ªÉm tra kho·∫£ng c√°ch ƒë·ªÉ tr√°nh v·∫Ω c√°c ƒë∆∞·ªùng qu√° d√†i - GI·∫¢M NG∆Ø·ª†NG XU·ªêNG
                    distance = np.sqrt(((pt1[0] - pt2[0]) ** 2) + ((pt1[1] - pt2[1]) ** 2))
                    # Kho·∫£ng c√°ch t·ªëi ƒëa h·ª£p l√Ω gi·ªØa c√°c keypoint (GI·∫¢M XU·ªêNG 30%)
                    max_distance = width * 0.3  # Gi·∫£m t·ª´ 50% xu·ªëng 30% chi·ªÅu r·ªông ·∫£nh

                    # TH√äM: Ki·ªÉm tra confidence c·ªßa c·∫£ hai ƒëi·ªÉm trong dict ban ƒë·∫ßu
                    kp1_conf = 0
                    kp2_conf = 0
                    for kp in person['keypoints']:
                        if kp['id'] == kp1_id:
                            kp1_conf = kp['confidence']
                        if kp['id'] == kp2_id:
                            kp2_conf = kp['confidence']

                    # CH·ªà v·∫Ω khi c·∫£ 2 ƒëi·ªÉm ƒë·ªÅu c√≥ confidence cao
                    if kp1_conf < 0.2 or kp2_conf < 0.2 or distance > max_distance:
                        continue  # B·ªè qua c√°c skeleton khi c√≥ ƒëi·ªÉm k√©m tin c·∫≠y ho·∫∑c qu√° d√†i

                    # S·ª≠ d·ª•ng m√†u kh√°c nhau cho c√°c ph·∫ßn kh√°c nhau c·ªßa c∆° th·ªÉ
                    if (kp1_id <= 4 and kp2_id <= 4):  # Ph·∫ßn ƒë·∫ßu
                        line_color = (255, 0, 0)
                    elif (5 <= kp1_id <= 10) or (5 <= kp2_id <= 10):  # Ph·∫ßn tay
                        line_color = (0, 255, 255)
                    else:  # Ph·∫ßn ch√¢n
                        line_color = (0, 255, 0)

                    # V·∫Ω ƒë∆∞·ªùng v·ªõi ƒë·ªô d√†y l·ªõn h∆°n
                    cv2.line(pose_viz, pt1, pt2, (0, 0, 0), 5)  # ƒê∆∞·ªùng vi·ªÅn ƒëen
                    cv2.line(pose_viz, pt1, pt2, line_color, 3)  # ƒê∆∞·ªùng m√†u
                    # HI·ªÇN TH·ªä ACTION DETECTION
                    if 'action_detection' in sports_analysis and sports_analysis['action_detection'].get(
                            'detected_actions'):
                        actions = sports_analysis['action_detection']['detected_actions']
                        y_offset = 30

                        # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ
                        cv2.putText(pose_viz, "DETECTED ACTIONS:", (10, y_offset),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)
                        y_offset += 25

                        # Hi·ªÉn th·ªã t·ª´ng action
                        for i, action in enumerate(actions[:3]):  # Hi·ªÉn th·ªã t·ªëi ƒëa 3 actions
                            action_text = f"{action['action'].upper()}: {action['confidence']:.2f}"
                            color = (0, 255, 255) if action['confidence'] > 0.8 else (0, 255, 0)

                            cv2.putText(pose_viz, action_text, (10, y_offset),
                                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)
                            y_offset += 20

                            # Hi·ªÉn th·ªã chi ti·∫øt ng·∫Øn g·ªçn
                            if len(action['details']) < 50:
                                cv2.putText(pose_viz, action['details'], (10, y_offset),
                                            cv2.FONT_HERSHEY_SIMPLEX, 0.4, (200, 200, 200), 1)
                                y_offset += 15

                        # Hi·ªÉn th·ªã body orientation
                        orientation = sports_analysis['action_detection'].get('body_orientation', 'unknown')
                        cv2.putText(pose_viz, f"View: {orientation}", (10, y_offset + 10),
                                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 200, 0), 2)
    else:
        print("Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu pose_analysis ho·∫∑c c·∫•u tr√∫c d·ªØ li·ªáu kh√¥ng ƒë√∫ng")
        print(
            f"sports_analysis keys: {sports_analysis.keys() if isinstance(sports_analysis, dict) else type(sports_analysis)}")
        if isinstance(sports_analysis, dict) and 'pose_analysis' in sports_analysis:
            print(f"pose_analysis keys: {sports_analysis['pose_analysis'].keys()}")

    # Hi·ªÉn th·ªã bi·ªÉu c·∫£m khu√¥n m·∫∑t
    face_emotion_viz = None
    if facial_analysis and facial_analysis.get('has_faces', False):
        try:
            # T√¨m ·∫£nh khu√¥n m·∫∑t
            face_img = None
            if 'face_path' in facial_analysis:
                face_path = facial_analysis['face_path']
                print(f"ƒê·ªçc ·∫£nh khu√¥n m·∫∑t t·ª´: {face_path}")
                if os.path.exists(face_path):
                    face_img = cv2.imread(face_path)
                    if face_img is not None:
                        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
                        print(f"ƒê√£ ƒë·ªçc ·∫£nh khu√¥n m·∫∑t th√†nh c√¥ng, k√≠ch th∆∞·ªõc: {face_img.shape}")
                    else:
                        print(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh khu√¥n m·∫∑t t·ª´ {face_path}")

            # N·∫øu kh√¥ng c√≥ face_path ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c, th·ª≠ t√¨m tr·ª±c ti·∫øp trong th∆∞ m·ª•c debug
            if face_img is None:
                fallback_path = "face_debug/best_face.jpg"
                if os.path.exists(fallback_path):
                    face_img = cv2.imread(fallback_path)
                    if face_img is not None:
                        face_img = cv2.cvtColor(face_img, cv2.COLOR_BGR2RGB)
                        print(f"ƒê√£ ƒë·ªçc ·∫£nh khu√¥n m·∫∑t t·ª´ ƒë∆∞·ªùng d·∫´n d·ª± ph√≤ng: {fallback_path}")
                    else:
                        print(f"Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh khu√¥n m·∫∑t t·ª´ ƒë∆∞·ªùng d·∫´n d·ª± ph√≤ng")
                else:
                    print(f"Kh√¥ng t√¨m th·∫•y file ·∫£nh khu√¥n m·∫∑t d·ª± ph√≤ng: {fallback_path}")

            # N·∫øu ƒë·ªçc ƒë∆∞·ª£c ·∫£nh khu√¥n m·∫∑t, t·∫°o hi·ªÉn th·ªã bi·ªÉu c·∫£m
            if face_img is not None:
                # ƒê·∫£m b·∫£o k√≠ch th∆∞·ªõc h·ª£p l√Ω cho ·∫£nh khu√¥n m·∫∑t
                h, w = face_img.shape[:2]
                display_width = 300
                display_height = int((h / w) * display_width) if w > 0 else 300

                # ƒêi·ªÅu ch·ªânh k√≠ch th∆∞·ªõc ·∫£nh khu√¥n m·∫∑t cho ph√π h·ª£p
                face_display = cv2.resize(face_img, (display_width, display_height),
                                          interpolation=cv2.INTER_AREA)

                # L·∫•y th√¥ng tin c·∫£m x√∫c
                emotion = facial_analysis.get('dominant_emotion', 'unknown')
                intensity = facial_analysis.get('emotion_intensity', 0)

                # T·∫°o h√¨nh ·∫£nh hi·ªÉn th·ªã bi·ªÉu c·∫£m
                face_emotion_viz = np.ones((400, 400, 3), dtype=np.uint8) * 255  # T·∫°o n·ªÅn tr·∫Øng

                # Hi·ªÉn th·ªã ·∫£nh khu√¥n m·∫∑t ·ªü gi·ªØa
                y_offset = 50
                x_offset = (400 - display_width) // 2
                face_emotion_viz[y_offset:y_offset + display_height, x_offset:x_offset + display_width] = face_display

                # Hi·ªÉn th·ªã th√¥ng tin c·∫£m x√∫c
                font = cv2.FONT_HERSHEY_SIMPLEX
                cv2.putText(face_emotion_viz, f"Emotion: {emotion.upper()}", (20, 30),
                            font, 0.7, (0, 0, 0), 2)
                cv2.putText(face_emotion_viz, f"Intensity: {intensity:.2f}", (20, y_offset + display_height + 30),
                            font, 0.7, (0, 0, 0), 2)

                print(f"ƒê√£ t·∫°o hi·ªÉn th·ªã bi·ªÉu c·∫£m khu√¥n m·∫∑t th√†nh c√¥ng")
            else:
                print("Kh√¥ng th·ªÉ ƒë·ªçc ·∫£nh khu√¥n m·∫∑t t·ª´ b·∫•t k·ª≥ ngu·ªìn n√†o")
        except Exception as e:
            import traceback
            print(f"L·ªói khi t·∫°o hi·ªÉn th·ªã bi·ªÉu c·∫£m: {str(e)}")
            print(traceback.format_exc())

    # L∆∞u c√°c th√†nh ph·∫ßn ri√™ng bi·ªát
    # Depth map
    plt.figure(figsize=(8, 6))
    plt.imshow(depth_map, cmap='plasma')
    plt.title("Depth Map")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("depth_map.png", dpi=150)
    plt.close()

    # Detections
    plt.figure(figsize=(8, 6))
    plt.imshow(det_viz)
    plt.title(f"Detections ({detections['athletes']} athletes)")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("detections.png", dpi=150)
    plt.close()

    # Main subject highlight
    plt.figure(figsize=(8, 6))
    plt.imshow(main_highlight)
    plt.title("Main Subject Highlight")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("main_subject_highlight.png", dpi=150)
    plt.close()

    # Sharpness heatmap
    plt.figure(figsize=(8, 6))
    plt.imshow(sharpness_viz)
    plt.title("Sharpness Heatmap")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("sharpness_heatmap.png", dpi=150)
    plt.close()

    # Composition analysis
    plt.figure(figsize=(8, 6))
    plt.imshow(comp_viz)
    plt.title("Composition Analysis")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("composition_analysis.png", dpi=150)
    plt.close()

    # TH√äM M·ªöI: L∆∞u Pose Estimation
    plt.figure(figsize=(8, 6))
    plt.imshow(pose_viz)
    plt.title("Pose Estimation")
    plt.axis('off')
    plt.tight_layout()
    plt.savefig("pose_estimation.png", dpi=150)
    plt.close()

    # Hi·ªÉn th·ªã v·ªõi b·ªë c·ª•c n√¢ng cao
    fig = plt.figure(figsize=(18, 12))

    # THAY ƒê·ªîI: C·∫≠p nh·∫≠t GridSpec ƒë·ªÉ th√™m pose estimation
    grid = plt.GridSpec(4, 4, hspace=0.3, wspace=0.3)

    # ·∫¢nh g·ªëc
    ax_orig = fig.add_subplot(grid[0, 0])
    ax_orig.imshow(img)
    ax_orig.set_title("Original Image")
    ax_orig.axis('off')

    # Main subject highlight (l·ªõn nh·∫•t - ·ªü gi·ªØa)
    ax_main = fig.add_subplot(grid[0:2, 1:3])
    ax_main.imshow(main_highlight)
    ax_main.set_title("Main Subject Highlight")
    ax_main.axis('off')

    # Detections
    ax_det = fig.add_subplot(grid[0, 3])
    ax_det.imshow(det_viz)
    ax_det.set_title(f"Detections ({detections['athletes']} athletes)")
    ax_det.axis('off')

    # Depth map
    ax_depth = fig.add_subplot(grid[1, 0])
    ax_depth.imshow(depth_map, cmap='plasma')
    ax_depth.set_title("Depth Map")
    ax_depth.axis('off')

    # Composition analysis
    ax_comp = fig.add_subplot(grid[1, 3])
    ax_comp.imshow(comp_viz)
    ax_comp.set_title("Composition Analysis")
    ax_comp.axis('off')

    # Sharpness heatmap
    ax_sharp = fig.add_subplot(grid[2, 0:2])
    ax_sharp.imshow(sharpness_viz)
    ax_sharp.set_title("Sharpness Heatmap")
    ax_sharp.axis('off')

    # TH√äM M·ªöI: Pose estimation visualization
    ax_pose = fig.add_subplot(grid[2:4, 2:4])  # ƒêI·ªÄU CH·ªàNH V·ªä TR√ç HI·ªÇN TH·ªä POSE
    ax_pose.imshow(pose_viz)
    ax_pose.set_title("Pose Estimation")
    ax_pose.axis('off')

    # Face analysis n√¢ng cao
    if face_emotion_viz is not None:
        ax_face = fig.add_subplot(grid[3, 0:2])  # ƒêI·ªÄU CH·ªàNH V·ªä TR√ç HI·ªÇN TH·ªä FACE
        ax_face.imshow(face_emotion_viz)

        # Hi·ªÉn th·ªã ti√™u ƒë·ªÅ chi ti·∫øt h∆°n
        emotion = facial_analysis.get('dominant_emotion', 'unknown')
        intensity = facial_analysis.get('emotion_intensity', 0)
        original_emotion = facial_analysis.get('original_emotion', emotion)

        if original_emotion != emotion:
            title = f"Face: {emotion.upper()} ({intensity:.2f}) [orig: {original_emotion}]"
        else:
            title = f"Face: {emotion.upper()} ({intensity:.2f})"

        ax_face.set_title(title)
        ax_face.axis('off')
    else:
        # Hi·ªÉn th·ªã th√¥ng b√°o r√µ r√†ng khi kh√¥ng ph√°t hi·ªán ƒë∆∞·ª£c khu√¥n m·∫∑t
        ax_info = fig.add_subplot(grid[3, 0:2])  # ƒêI·ªÄU CH·ªàNH V·ªä TR√ç HI·ªÇN TH·ªä L·ªñI FACE

        # T·∫°o th√¥ng b√°o "No face detected"
        if not facial_analysis or not facial_analysis.get('has_faces', False):
            ax_info.text(0.5, 0.5, "NO FACE DETECTED\nFacial analysis skipped",
                         fontsize=16, color='red', fontweight='bold',
                         horizontalalignment='center', verticalalignment='center')
        else:
            # Tr∆∞·ªùng h·ª£p c√≥ facial_analysis nh∆∞ng kh√¥ng c√≥ ·∫£nh
            emotion = facial_analysis.get('dominant_emotion', 'unknown')
            intensity = facial_analysis.get('emotion_intensity', 0)
            text = f"Face Emotion: {emotion}\nIntensity: {intensity:.2f}"
            if 'original_emotion' in facial_analysis and facial_analysis['original_emotion'] != emotion:
                text += f"\nOriginal: {facial_analysis['original_emotion']}"

            ax_info.text(0.5, 0.5, text, horizontalalignment='center',
                         verticalalignment='center', fontsize=14)

        ax_info.axis('off')
        ax_info.set_title("Facial Expression")

    plt.savefig("sports_analysis_results.png", dpi=150)
    plt.close()

    # Print detailed analysis
    print("\n==== SPORTS IMAGE ANALYSIS ====")
    print(
        f"Detected {detections['athletes']} athletes and {len(detections['classes']) - detections['athletes']} other objects")

    if "sport_type" in composition_analysis:
        print(f"\nSport type: {composition_analysis['sport_type']}")

        # Hi·ªÉn th·ªã logic detection
        if 'action_detection' in sports_analysis and sports_analysis['action_detection'].get('detected_actions'):
            best_action = max(sports_analysis['action_detection']['detected_actions'],
                            key=lambda x: x['confidence'])
            print(f"  -> Influenced by detected action: {best_action['action']} ({best_action['confidence']:.2f})")

        # Th√™m th√¥ng tin v·ªÅ m√¥i tr∆∞·ªùng n·∫øu c√≥
        if 'environment_analysis' in composition_analysis:
            env = composition_analysis['environment_analysis']
            if env['detected_environments']:
                print(f"Detected environment: {', '.join(env['detected_environments'])}")
            if env['surface_type'] != 'unknown':
                print(f"Surface type: {env['surface_type']}")
            if env['dominant_colors']:
                colors = [f"{color} ({ratio:.2f})" for color, ratio in env['dominant_colors']]
                print(f"Dominant colors: {', '.join(colors)}")

    if detections['athletes'] > 0:
        print("\nPlayer Analysis:")
        print(f"- Number of players: {detections['athletes']}")
        if detections['athletes'] > 1:
            print(f"- Player dispersion: {sports_analysis['player_dispersion']:.2f}")

    print("\nAction Analysis:")
    print(
        f"- Equipment detected: {', '.join(action_analysis['equipment_types']) if action_analysis['equipment_types'] else 'None'}")
    print(f"- Action level: {action_analysis['action_quality']} ({action_analysis['action_level']:.2f})")

    print("\nComposition Analysis:")
    print(f"- Framing quality: {composition_analysis['framing_quality']}")

    # Hi·ªÉn th·ªã chi ti·∫øt framing analysis n·∫øu c√≥
    if 'framing_analysis' in composition_analysis and 'overall_score' in composition_analysis['framing_analysis']:
        framing = composition_analysis['framing_analysis']
        print(f"  * Overall score: {framing['overall_score']:.3f}")
        print(f"  * Position score: {framing['position_score']:.3f}")
        print(f"  * Size score: {framing['size_score']:.3f}")
        print(f"  * Breathing room score: {framing['breathing_score']:.3f}")
        print(f"  * Subject size ratio: {framing['size_ratio']:.3f}")
        print(f"  * Min margin: {framing['min_margin']:.3f}")

    if composition_analysis['recommended_crop']:
        crop = composition_analysis['recommended_crop']
        direction_x = "right" if crop['shift_x'] < 0 else "left"
        direction_y = "down" if crop['shift_y'] < 0 else "up"
        print(
            f"- Recommended crop: Shift {abs(crop['shift_x']) * 100:.1f}% {direction_x} and {abs(crop['shift_y']) * 100:.1f}% {direction_y}")

    # Key subjects with sharpness
    if sports_analysis['key_subjects']:
        print("\nKey Subjects by Prominence:")
        for i, subject in enumerate(sports_analysis['key_subjects']):
            main_tag = " (MAIN SUBJECT)" if main_person_idx == i else ""
            print(
                f"{i + 1}. {subject['class']}{main_tag} (Prominence: {subject['prominence']:.2f}, Sharpness: {subject.get('sharpness', 0):.2f})")

            # Chi ti·∫øt v·ªÅ ƒë·ªô s·∫Øc n√©t n·∫øu c√≥
            if 'sharpness_details' in subject and subject['sharpness_details']:
                details = subject['sharpness_details']
                print(f"   - Laplacian Variance: {details['laplacian_var']:.2f}")
                print(f"   - Sobel Mean: {details['sobel_mean']:.2f}")

    # HI·ªÇN TH·ªä K·∫æT QU·∫¢ ACTION DETECTION
    if 'action_detection' in sports_analysis and sports_analysis['action_detection'].get('detected_actions'):
        print("\nSports Action Detection:")
        action_data = sports_analysis['action_detection']
        print(f"- Body orientation: {action_data.get('body_orientation', 'unknown')}")
        print(f"- Overall confidence: {action_data.get('confidence', 0):.2f}")
        print(f"- Keypoints analyzed: {action_data.get('keypoints_count', 0)}")

        for i, action in enumerate(action_data['detected_actions']):
            print(f"  {i + 1}. {action['action'].upper()} ({action['confidence']:.2f})")
            print(f"     - Body part: {action['body_part']}")
            print(f"     - Details: {action['details']}")
            print(f"     - View angle: {action['body_orientation']}")
    else:
        print("\nSports Action Detection: No specific actions detected")

    # HI·ªÇN TH·ªä PH√ÇN T√çCH BI·ªÇU C·∫¢M C·∫¢I TI·∫æN
    if facial_analysis and facial_analysis.get('has_faces', False):
        print("\nFacial Expression Analysis (Advanced):")
        print(f"- Dominant emotion: {facial_analysis['dominant_emotion']}")
        if 'original_emotion' in facial_analysis and facial_analysis['original_emotion'] != facial_analysis[
            'dominant_emotion']:
            print(f"- Original detected emotion: {facial_analysis['original_emotion']}")
        print(f"- Emotion intensity: {facial_analysis['emotion_intensity']:.2f}")
        print(f"- Emotional value: {facial_analysis['emotional_value']}")
        print(f"- Sport context: {facial_analysis.get('sport_context', 'Unknown')}")

        # Hi·ªÉn th·ªã chi ti·∫øt ƒëi·ªÉm s·ªë c·∫£m x√∫c n·∫øu c√≥
        if 'contextual_scores' in facial_analysis:
            print("\nDetailed Emotion Scores:")
            for emotion, score in facial_analysis['contextual_scores'].items():
                print(f"  - {emotion}: {score:.3f}")
    else:
        print("\nFacial Expression Analysis: No valid face detected")

    # Save analysis results
    with open("analysis_results.txt", "w") as f:
        f.write("\n==== SPORTS IMAGE ANALYSIS ====\n")
        f.write(
            f"Detected {detections['athletes']} athletes and {len(detections['classes']) - detections['athletes']} other objects\n")

        if "sport_type" in composition_analysis:
            f.write(f"\nSport type: {composition_analysis['sport_type']}\n")

        if detections['athletes'] > 0:
            f.write("\nPlayer Analysis:\n")
            f.write(f"- Number of players: {detections['athletes']}\n")
            if detections['athletes'] > 1:
                f.write(f"- Player dispersion: {sports_analysis['player_dispersion']:.2f}\n")

        f.write("\nAction Analysis:\n")
        f.write(
            f"- Equipment detected: {', '.join(action_analysis['equipment_types']) if action_analysis['equipment_types'] else 'None'}\n")
        f.write(f"- Action quality: {action_analysis['action_quality']} ({action_analysis['action_level']:.2f})\n")

        # Th√™m chi ti·∫øt framing analysis
        if 'framing_analysis' in composition_analysis and 'overall_score' in composition_analysis['framing_analysis']:
            framing = composition_analysis['framing_analysis']
            f.write(f"  * Overall score: {framing['overall_score']:.3f}\n")
            f.write(f"  * Position score: {framing['position_score']:.3f}\n")
            f.write(f"  * Size score: {framing['size_score']:.3f}\n")
            f.write(f"  * Breathing room score: {framing['breathing_score']:.3f}\n")
            f.write(f"  * Subject size ratio: {framing['size_ratio']:.3f}\n")
            f.write(f"  * Min margin: {framing['min_margin']:.3f}\n")

        if composition_analysis['recommended_crop']:
            crop = composition_analysis['recommended_crop']
            direction_x = "right" if crop['shift_x'] < 0 else "left"
            direction_y = "down" if crop['shift_y'] < 0 else "up"
            f.write(
                f"- Recommended crop: Shift {abs(crop['shift_x']) * 100:.1f}% {direction_x} and {abs(crop['shift_y']) * 100:.1f}% {direction_y}\n")

        # Th√™m th√¥ng tin ƒë·ªô s·∫Øc n√©t khi l∆∞u k·∫øt qu·∫£
        if sports_analysis['key_subjects']:
            f.write("\nKey Subjects by Prominence:\n")
            for i, subject in enumerate(sports_analysis['key_subjects']):
                main_tag = " (MAIN SUBJECT)" if main_person_idx == i else ""
                f.write(
                    f"{i + 1}. {subject['class']}{main_tag} (Prominence: {subject['prominence']:.2f}, Sharpness: {subject.get('sharpness', 0):.2f})\n")

        # L∆∞u ph√¢n t√≠ch bi·ªÉu c·∫£m n√¢ng cao
        if facial_analysis and facial_analysis.get('has_faces', False):
            f.write("\nFacial Expression Analysis (Advanced):\n")
            f.write(f"- Dominant emotion: {facial_analysis['dominant_emotion']}\n")
            if 'original_emotion' in facial_analysis and facial_analysis['original_emotion'] != facial_analysis[
                'dominant_emotion']:
                f.write(f"- Original detected emotion: {facial_analysis['original_emotion']}\n")
            f.write(f"- Emotion intensity: {facial_analysis['emotion_intensity']:.2f}\n")
            f.write(f"- Emotional value: {facial_analysis['emotional_value']}\n")
            f.write(f"- Sport context: {facial_analysis.get('sport_context', 'Unknown')}\n")

            # Chi ti·∫øt ƒëi·ªÉm s·ªë c·∫£m x√∫c
            if 'contextual_scores' in facial_analysis:
                f.write("\nDetailed Emotion Scores:\n")
                for emotion, score in facial_analysis['contextual_scores'].items():
                    f.write(f"  - {emotion}: {score:.3f}\n")
        else:
            f.write("\nFacial Expression Analysis: No valid face detected\n")

        # L∆∞u caption
        if caption:
            f.write("\nImage Caption:\n")
            f.write(caption + "\n")


def analyze_sports_image(file_path):
    """Main function to analyze sports images"""
    t_start = time.time()

    # Load models
    print("Loading models...")
    midas, yolo, yolo_seg, device = load_models()
    print("Models loaded successfully.")

    img_data = preprocess_image(file_path)
    print(f"Image preprocessed: {file_path}")

    # Step 1: Object detection with YOLO
    print("Detecting objects...")
    detections = detect_sports_objects(yolo, img_data)
    print(f"Found {len(detections['classes'])} objects, including {detections['athletes']} athletes")

    # Step 2: Generate depth map
    print("Generating depth map...")
    depth_map, depth_mask, depth_contour = generate_depth_map(midas, img_data)
    print("Depth map generated.")

    # Step 3: Analyze sports scene with sharpness detection
    print("Analyzing sports scene with sharpness detection...")
    sports_analysis = analyze_sports_scene(detections, depth_map, img_data, yolo_seg)

    # Step 4: Analyze action quality
    print("Analyzing action quality...")
    action_analysis = analyze_action_quality(detections, img_data)

    # Step 5: Sports composition analysis (QUAN TR·ªåNG: Ph·∫£i sau action detection)
    print("Analyzing composition...")
    composition_analysis = analyze_sports_composition(detections, {
        'sports_analysis': sports_analysis,  # Bao g·ªìm action_detection results
        'depth_map': depth_map
    }, img_data)

    # Step 6: Facial expression analysis v·ªõi phi√™n b·∫£n n√¢ng cao
    print("Starting advanced facial expression analysis...")
    try:
        facial_analysis = analyze_facial_expression_advanced(
            detections,
            img_data,
            depth_map=depth_map,
            sports_analysis={'sports_analysis': sports_analysis, 'composition_analysis': composition_analysis}
        )
        print("Advanced facial expression analysis successful:", facial_analysis.get('has_faces', False))
    except Exception as e:
        print(f"Error in facial expression analysis: {str(e)}")
        import traceback
        traceback.print_exc()
        facial_analysis = {'has_faces': False, 'error': str(e)}

    # Step 6.5: TH√äM PH√ÅT HI·ªÜN POSE
    print("Detecting human poses...")
    pose_results = detect_human_pose(img_data, conf_threshold=0.15)
    # C·∫≠p nh·∫≠t sports_analysis v·ªõi pose_results
    sports_analysis['pose_analysis'] = pose_results
    print(f"DEBUG B - sports_analysis sau khi g√°n pose: {sports_analysis.keys()}")

    # Step 6.6: PH√ÅT HI·ªÜN H√ÄNH ƒê·ªòNG TH·ªÇ THAO V·ªöI EQUIPMENT FILTERING
    print("Detecting sports actions with equipment filtering...")
    sport_type = composition_analysis.get('sport_type', 'Unknown')

    # L·∫•y equipment ƒë√£ detect
    detected_equipment = action_analysis.get('equipment_types', [])

    # L·∫•y environment sport analysis
    environment_sport = None
    if 'environment_analysis' in composition_analysis and 'sport_probabilities' in composition_analysis[
        'environment_analysis']:
        env_sports = composition_analysis['environment_analysis']['sport_probabilities']
        if env_sports:
            environment_sport = max(env_sports.items(), key=lambda x: x[1])[0]

    action_detection_results = detect_sports_actions(
        pose_results,
        sport_type,
        img_data['resized_array'].shape,
        detected_equipment=detected_equipment,
        environment_sport=environment_sport
    )

    # C·∫≠p nh·∫≠t sports_analysis v·ªõi action detection
    sports_analysis['action_detection'] = action_detection_results
    print(f"Detected actions: {[action['action'] for action in action_detection_results.get('detected_actions', [])]}")
    if action_detection_results.get('detected_actions'):
        for action in action_detection_results['detected_actions']:
            print(f"  - {action['action']}: {action['confidence']:.2f} ({action['details']})")

    # T·∫°o k·∫øt qu·∫£ ph√¢n t√≠ch cu·ªëi c√πng
    analysis_result = {
        'detections': detections,
        'sports_analysis': sports_analysis,  # sports_analysis ƒë√£ c√≥ pose_analysis
        'action_analysis': action_analysis,
        'composition_analysis': composition_analysis,
        'facial_analysis': facial_analysis
    }

    # Step 7: Visualize results v·ªõi hi·ªÉn th·ªã bi·ªÉu c·∫£m c·∫£i ti·∫øn
    print("Visualizing results...")
    print(f"DEBUG C - ID c·ªßa sports_analysis tr∆∞·ªõc khi visualize: {id(sports_analysis)}")
    visualize_sports_results(img_data, detections, depth_map,
                             sports_analysis, action_analysis, composition_analysis,
                             facial_analysis)

    t_end = time.time()
    print(f"\nAnalysis completed in {t_end - t_start:.2f} seconds")

    # T·∫°o caption t·ª´ k·∫øt qu·∫£ ph√¢n t√≠ch
    caption = generate_sports_caption(analysis_result)
    print(f"\nCaption: {caption}")

    # Th√™m caption v√†o k·∫øt qu·∫£ tr·∫£ v·ªÅ
    analysis_result['caption'] = caption

    return analysis_result


def generate_sports_caption(analysis_result):
    """
    Generates high-quality, natural-sounding English captions for sports images based on analysis results.

    Special handling for:
    - Groups of athletes (>6 athletes with >3 objects close together)
    - Main athlete highlight when standing alone or with minimal overlap

    Args:
        analysis_result: Dictionary containing sports image analysis data

    Returns:
        str: Well-crafted caption describing the sports image
    """
    # Extract key information from analysis results
    detections = analysis_result.get('detections', {})
    sports_analysis = analysis_result.get('sports_analysis', {})
    action_analysis = analysis_result.get('action_analysis', {})
    composition_analysis = analysis_result.get('composition_analysis', {})
    facial_analysis = analysis_result.get('facial_analysis', {})

    # Initialize caption component parts
    intro_phrases = []  # Opening sentence
    subject_phrases = []  # Main subject description
    action_phrases = []  # Action description
    emotion_phrases = []  # Emotional aspects
    detail_phrases = []  # Additional details
    closing_phrases = []  # Conclusion

    # ----------------- 1. IDENTIFY SPORT TYPE -----------------
    sport_type = composition_analysis.get('sport_type', 'Running').lower()
    sport_type_original = composition_analysis.get('sport_type', 'Running')
    athlete_count = detections.get('athletes', 0)
    action_level = action_analysis.get('action_level', 0)
    action_quality = action_analysis.get('action_quality', '')
    equipment = action_analysis.get('equipment_types', [])

    # Sport-specific terminology
    sport_specific_terms = {
        'soccer': ['match', 'pitch', 'soccer', 'football', 'kick', 'goal'],
        'football': ['stadium', 'team', 'touchdown', 'offense', 'quarterback'],
        'basketball': ['court', 'shot', 'hoop', 'dunk', 'basket'],
        'tennis': ['court', 'player', 'serve', 'set', 'stroke', 'match point'],
        'baseball': ['field', 'hit', 'home run', 'pitcher', 'batter'],
        'swimming': ['pool', 'lane', 'swimmer', 'stroke', 'race'],
        'volleyball': ['net', 'court', 'spike', 'serve'],
        'track': ['track', 'race', 'sprinter', 'athlete'],
        'running': ['race', 'track', 'runner', 'sprint'],
        'boxing': ['ring', 'boxer', 'punch', 'match', 'bout'],
        'skiing': ['snow', 'slope', 'skier', 'mountain'],
        'skating': ['ice', 'skater', 'performance', 'rink'],
        'surfing': ['wave', 'beach', 'surfer', 'ocean'],
        'skateboarding': ['skate park', 'skateboarder', 'trick', 'jump'],
        'golf': ['course', 'club', 'swing', 'golfer', 'hole'],
        'rugby': ['field', 'tackle', 'player', 'scrum'],
        'martial arts': ['mat', 'fighter', 'technique', 'match']
    }

    # Determine sport type based on name and equipment
    detected_sport = 'unknown'

    for sport_name, terms in sport_specific_terms.items():
        # Check sport name
        if sport_name in sport_type.lower():
            detected_sport = sport_name
            break

        # Check equipment
        for eq in equipment:
            eq_lower = eq.lower()
            if sport_name in eq_lower or any(term.lower() in eq_lower for term in terms):
                detected_sport = sport_name
                break

    # If no specific sport found but "sports ball" is detected
    if detected_sport == 'unknown' and any('ball' in eq.lower() for eq in equipment):
        detected_sport = 'ball sport'

    # ----------------- 2. ANALYZE SCENE COMPLEXITY -----------------
    # Check if it's a crowded scene (many athletes close together)
    is_crowded_scene = False
    has_clear_main_subject = False

    # Determine if scene is crowded (>6 athletes and >3 objects close together)
    if athlete_count > 6 and len(detections.get('boxes', [])) > 3:
        # Check proximity of objects if we have position data
        if 'key_subjects' in sports_analysis and len(sports_analysis['key_subjects']) > 3:
            # Count objects that are close to each other
            overlapping_count = 0
            boxes = [subj['box'] for subj in sports_analysis['key_subjects'] if 'box' in subj]

            # Simple overlap detection by checking box proximity
            for i in range(len(boxes)):
                for j in range(i + 1, len(boxes)):
                    # Calculate centers
                    x1_center = (boxes[i][0] + boxes[i][2]) / 2
                    y1_center = (boxes[i][1] + boxes[i][3]) / 2
                    x2_center = (boxes[j][0] + boxes[j][2]) / 2
                    y2_center = (boxes[j][1] + boxes[j][3]) / 2

                    # Calculate distance between centers
                    distance = ((x1_center - x2_center) ** 2 + (y1_center - y2_center) ** 2) ** 0.5

                    # Define "close" as less than average box width/height
                    avg_width = ((boxes[i][2] - boxes[i][0]) + (boxes[j][2] - boxes[j][0])) / 2
                    avg_height = ((boxes[i][3] - boxes[i][1]) + (boxes[j][3] - boxes[j][1])) / 2
                    avg_size = (avg_width + avg_height) / 2

                    if distance < avg_size:
                        overlapping_count += 1

            # If many overlaps detected, consider it a crowded scene
            is_crowded_scene = overlapping_count > 3

    # Determine if there's a clear main subject
    if 'key_subjects' in sports_analysis and sports_analysis['key_subjects']:
        main_subject = sports_analysis['key_subjects'][0]
        # Check if the main subject is significantly more prominent
        if len(sports_analysis['key_subjects']) > 1:
            second_subject = sports_analysis['key_subjects'][1]
            if main_subject.get('prominence', 0) > second_subject.get('prominence', 0) * 1.5:
                has_clear_main_subject = True
        else:
            has_clear_main_subject = True

    # ----------------- 3. CREATE INTRO PHRASES -----------------
    import random

    if action_level > 0.7:  # High action
        if detected_sport != 'unknown':
            sport_name = sport_type_original
            intro_options = [
                f"A dramatic moment in {sport_name}",
                f"An intense action shot from {sport_name}",
                f"A thrilling {sport_name} action capture",
                f"A peak moment in {sport_name} competition",
                f"A powerful {sport_name} action frame"
            ]
        else:
            intro_options = [
                "A stunning sports action moment",
                "An energetic capture from a sporting event",
                "A dynamic shot showcasing athletic prowess",
                "An impressive display of sports action",
                "A high-intensity athletic moment"
            ]
    elif action_level > 0.3:  # Medium action
        if detected_sport != 'unknown':
            sport_name = sport_type_original
            intro_options = [
                f"An engaging {sport_name} moment",
                f"A skillful display in {sport_name}",
                f"An active {sport_name} sequence",
                f"A competitive {sport_name} scene",
                f"A focused moment during {sport_name} play"
            ]
        else:
            intro_options = [
                "A captivating sports moment",
                "An active scene from a sporting event",
                "A dynamic athletic display",
                "A moment of sporting engagement",
                "A vibrant athletic performance"
            ]
    else:  # Low action
        if detected_sport != 'unknown':
            sport_name = sport_type_original
            intro_options = [
                f"A composed moment in {sport_name}",
                f"A strategic pause during {sport_name}",
                f"A reflective scene from {sport_name}",
                f"A calm moment in {sport_name} activity",
                f"A preparation phase for {sport_name} action"
            ]
        else:
            intro_options = [
                "A contemplative sports moment",
                "A composed athletic scene",
                "A moment of focus in sports",
                "A strategic pause in athletic activity",
                "A calm moment in sports performance"
            ]

    intro_phrases.append(random.choice(intro_options))

    # ----------------- 4. DESCRIBE SUBJECTS -----------------
    # Consider the special cases for athlete descriptions
    if is_crowded_scene:
        # For crowded scenes with many athletes and objects
        subject_options = [
            "featuring a group of athletes in close formation",
            "showcasing a cluster of competitors in action",
            "capturing a tight group of players in motion",
            "highlighting the coordinated movement of multiple athletes",
            "depicting a formation of athletes in synchronized action"
        ]
        subject_phrases.append(random.choice(subject_options))
    elif has_clear_main_subject and athlete_count > 1:
        # For scenes with a clear main subject and other athletes
        if 'key_subjects' in sports_analysis and sports_analysis['key_subjects']:
            main_class = sports_analysis['key_subjects'][0].get('class', 'athlete')
            if main_class.lower() == 'person':
                main_desc = "athlete"
            else:
                main_desc = main_class

            subject_options = [
                f"featuring a standout {main_desc} among {athlete_count - 1} other competitors",
                f"focusing on a primary {main_desc} with {athlete_count - 1} other athletes visible",
                f"highlighting the main performer against a backdrop of {athlete_count - 1} others",
                f"showcasing a central {main_desc} with supporting participants",
                f"centered on the primary athlete with additional performers in frame"
            ]
            subject_phrases.append(random.choice(subject_options))
    elif athlete_count > 0:
        # Standard description based on athlete count
        if athlete_count == 1:
            subject_options = [
                "featuring a solo athlete",
                "showcasing an individual competitor",
                "capturing a lone performer",
                "highlighting a single athlete's form",
                "focusing on an individual's athletic prowess"
            ]
        elif athlete_count == 2:
            subject_options = [
                f"featuring a pair of athletes in competition",
                f"showcasing two competitors facing off",
                f"capturing the dynamic between two athletes",
                f"highlighting the interaction of two competitors"
            ]
        else:
            subject_options = [
                f"featuring {athlete_count} athletes in action",
                f"showcasing a team of {athlete_count} competitors",
                f"capturing the coordination of {athlete_count} athletes",
                f"highlighting multiple performers in synchronized motion"
            ]

        subject_phrases.append(random.choice(subject_options))

    # ----------------- 5. DESCRIBE ACTION -----------------
    if action_quality:
        if action_quality == 'High':
            if detected_sport in ['soccer', 'football', 'basketball', 'volleyball']:
                action_options = [
                    "during an intensely competitive play",
                    "at a crucial moment in the match",
                    "in a high-stakes game situation",
                    "executing an advanced technical maneuver",
                    "during a powerful offensive drive"
                ]
            elif detected_sport in ['tennis', 'baseball', 'golf']:
                action_options = [
                    "during a perfectly executed swing",
                    "demonstrating exceptional technique",
                    "at the critical moment of impact",
                    "showcasing masterful control",
                    "with professional athletic form"
                ]
            elif detected_sport in ['running', 'track', 'swimming']:
                action_options = [
                    "at the moment of breakthrough acceleration",
                    "displaying extraordinary effort",
                    "at a decisive point in the race",
                    "during a powerful acceleration phase",
                    "with remarkable concentration and form"
                ]
            else:
                action_options = [
                    "at the peak moment of performance",
                    "with impressive competitive intensity",
                    "during a critical action sequence",
                    "demonstrating professional-level skill",
                    "in a dynamic display of athleticism"
                ]
        elif action_quality == 'Medium':
            action_options = [
                "during active competition",
                "with focused engagement in the event",
                "amidst the flow of the game",
                "demonstrating solid technique",
                "in a noteworthy moment of play"
            ]
        else:
            action_options = [
                "during a moment of calculated preparation",
                "in a strategic positioning phase",
                "during a brief respite in the action",
                "with focused pre-action concentration",
                "before initiating the next movement"
            ]

        action_phrases.append(random.choice(action_options))

    # ----------------- 6. DESCRIBE EQUIPMENT -----------------
    if equipment:
        eq_list = []
        for eq in equipment:
            if eq.lower() == "sports ball":
                if detected_sport == "soccer":
                    eq_list.append("a soccer ball")
                elif detected_sport == "basketball":
                    eq_list.append("a basketball")
                elif detected_sport == "volleyball":
                    eq_list.append("a volleyball")
                elif detected_sport == "tennis":
                    eq_list.append("a tennis ball")
                else:
                    eq_list.append("a sports ball")
            elif eq.lower() == "tennis racket":
                eq_list.append("a tennis racket")
            else:
                eq_list.append(eq)

        if len(eq_list) == 1:
            detail_phrases.append(f"with {eq_list[0]}")
        elif len(eq_list) == 2:
            detail_phrases.append(f"with {eq_list[0]} and {eq_list[1]}")
        elif len(eq_list) > 2:
            detail_phrases.append(f"with {', '.join(eq_list[:-1])}, and {eq_list[-1]}")

    # ----------------- 6.5. DESCRIBE DETECTED ACTIONS -----------------
    detected_actions = []
    if 'action_detection' in sports_analysis and sports_analysis['action_detection'].get('detected_actions'):
        actions = sports_analysis['action_detection']['detected_actions']
        high_confidence_actions = [a for a in actions if a['confidence'] > 0.7]

        if high_confidence_actions:
            action_names = [action['action'] for action in high_confidence_actions]

            # T·∫°o m√¥ t·∫£ h√†nh ƒë·ªông t·ª± nhi√™n
            action_descriptions = {
                'shooting': ['taking a shot', 'in a shooting motion', 'preparing to shoot'],
                'pre_kick_stance': ['positioning for a kick', 'in pre-kick stance', 'preparing to strike the ball'],
                'approach_run': ['approaching the ball', 'in run-up motion', 'building momentum for the kick'],
                'dribbling': ['dribbling the ball', 'controlling the ball', 'maneuvering with the ball'],
                'serving': ['serving the ball', 'in a serving position', 'executing a serve'],
                'forehand': ['executing a forehand stroke', 'in a forehand swing'],
                'backhand': ['performing a backhand shot', 'in a backhand position'],
                'classic_spike': ['spiking the ball powerfully', 'in classic attack position'],
                'power_spike_prep': ['preparing for a power spike', 'winding up for attack'],
                'double_hand_spike': ['executing a two-handed spike', 'in powerful attack stance'],
                'quick_attack': ['performing a quick attack', 'in rapid strike position'],
                'double_block': ['blocking at the net', 'in defensive block position'],
                'single_block': ['single-hand blocking', 'defending with one arm'],
                'soft_block': ['soft blocking technique', 'angled defensive position'],
                'setting': ['setting up teammates', 'in setting position'],
                'digging': ['digging the ball', 'in defensive receive position'],
                'sprinting': ['in full sprint', 'sprinting at high speed', 'running dynamically'],
                'running': ['running', 'in motion', 'moving forward'],
                # Boxing/Martial Arts
                'straight_punch': ['throwing a straight punch', 'executing a jab', 'in punching stance'],
                'left_hook': ['delivering a left hook', 'swinging a hook punch'],
                'right_hook': ['executing a right hook', 'throwing a power hook'],
                'uppercut': ['launching an uppercut', 'delivering an uppercut punch'],
                'body_shot': ['targeting the body', 'executing a body punch'],
                'defensive_guard': ['in defensive stance', 'maintaining guard position'],
                'high_kick': ['executing a high kick', 'performing a head kick'],
                'mid_kick': ['delivering a body kick', 'executing a mid-level kick'],

                # Badminton
                'badminton_smash': ['smashing the shuttlecock', 'executing a powerful smash'],
                'clear_shot': ['playing a clear shot', 'hitting a defensive clear'],
                'drop_shot': ['executing a drop shot', 'playing a delicate drop'],
                'defensive_ready': ['in defensive ready position', 'prepared for opponent attack'],

                # Golf
                'golf_backswing': ['in backswing motion', 'preparing the swing'],
                'golf_impact': ['at impact with the ball', 'striking the ball'],
                'golf_follow_through': ['completing the swing', 'in follow-through motion'],
                'putting': ['putting on the green', 'lining up the putt'],
                'freestyle_stroke': ['performing a freestyle stroke', 'swimming freestyle']
            }

            for action_name in action_names:
                if action_name in action_descriptions:
                    action_phrase = random.choice(action_descriptions[action_name])
                    detected_actions.append(action_phrase)

            if detected_actions:
                if len(detected_actions) == 1:
                    action_phrases.append(detected_actions[0])
                else:
                    # K·∫øt h·ª£p nhi·ªÅu h√†nh ƒë·ªông
                    action_phrases.append(f"{detected_actions[0]} and {detected_actions[1]}")

    # ----------------- 7. DESCRIBE EMOTION -----------------
    if facial_analysis and facial_analysis.get('has_faces', False):
        emotion = facial_analysis.get('dominant_emotion', '').lower()
        intensity = facial_analysis.get('emotion_intensity', 0)

        if intensity > 0.5:  # Clear emotion
            if emotion == 'happy' or emotion == 'happiness':
                emotion_options = [
                    "displaying evident joy on their face",
                    "with a confident smile radiating triumph",
                    "revealing excitement through their expression",
                    "with a remarkably upbeat expression"
                ]
            elif emotion == 'neutral':
                emotion_options = [
                    "maintaining intense concentration",
                    "showing steely determination in their gaze",
                    "keeping a professionally composed expression",
                    "with unwavering focus visible in their eyes"
                ]
            elif emotion == 'sad' or emotion == 'sadness':
                emotion_options = [
                    "with an expression revealing disappointment",
                    "showing signs of frustration at the challenge",
                    "with concern evident during a difficult moment",
                    "visibly processing the emotional weight of competition"
                ]
            elif emotion == 'angry' or emotion == 'anger':
                emotion_options = [
                    "with fierce intensity in their eyes",
                    "showing the powerful determination of an elite competitor",
                    "with an expression reflecting competitive fire",
                    "channeling powerful emotion into performance"
                ]
            elif emotion == 'surprise':
                emotion_options = [
                    "with astonishment at the unfolding situation",
                    "showing surprise at the unexpected development",
                    "with a notable expression of shock",
                    "displaying visible reaction to the surprising turn of events"
                ]
            elif emotion == 'determination':
                emotion_options = [
                    "with resolute determination etched on their face",
                    "showing unyielding commitment in their expression",
                    "with the steadfast focus of an elite athlete",
                    "displaying the mental fortitude required at this level"
                ]
            else:
                emotion_options = [
                    "with clearly visible emotion",
                    "displaying the powerful feelings of competition",
                    "with an expressive reaction to the moment",
                    "showing the emotional intensity of athletics"
                ]

            emotion_phrases.append(random.choice(emotion_options))

    # ----------------- 8. CLOSING PHRASES -----------------
    if action_level > 0.7:
        closing_options = [
            "The image perfectly captures this remarkable athletic moment.",
            "This photograph brilliantly conveys the intensity and skill of competitive sports.",
            "This powerful image freezes a memorable moment of athletic excellence.",
            "The photograph vividly showcases the peak of sporting achievement."
        ]
    elif action_level > 0.4:
        closing_options = [
            "The image provides an authentic glimpse into the nature of this sport.",
            "This photograph effectively highlights the technical aspects of athletic performance.",
            "The image captures the essence of this sporting discipline.",
            "This compelling viewpoint reveals the nuanced dynamics of the competition."
        ]
    else:
        closing_options = [
            "The image reveals a rarely seen calm moment in this intense sport.",
            "This photograph offers a different perspective on the athletic world.",
            "The image captures a moment of preparation that precedes athletic action.",
            "This thoughtful composition shows the mental aspect of sports competition."
        ]

    closing_phrases.append(random.choice(closing_options))

    # ----------------- 9. COMBINE ALL COMPONENTS -----------------
    all_parts = []

    # Add intro
    if intro_phrases:
        all_parts.append(intro_phrases[0])

    # Add subject
    if subject_phrases:
        all_parts.append(subject_phrases[0])

    # Add action
    if action_phrases:
        all_parts.append(action_phrases[0])

    # Add details
    if detail_phrases:
        all_parts.append(detail_phrases[0])

    # Add emotion
    if emotion_phrases:
        all_parts.append(emotion_phrases[0])

    # Combine main parts
    main_caption = ' '.join(all_parts)

    # Add closing
    if closing_phrases:
        caption = main_caption + '. ' + closing_phrases[0]
    else:
        caption = main_caption + '.'

    # Ensure proper capitalization and punctuation
    caption = caption.strip()
    if not caption.endswith('.'):
        caption += '.'

    # Fix spacing issues
    caption = caption.replace('  ', ' ')
    caption = caption.replace(' ,', ',')
    caption = caption.replace(' .', '.')

    return caption


def generate_smart_suggestion(analysis_result):
    """
    T·∫°o 1 c√¢u g·ª£i √Ω th√¥ng minh d·ª±a tr√™n k·∫øt qu·∫£ ph√¢n t√≠ch
    """
    # L·∫•y c√°c th√¥ng s·ªë t·ª´ k·∫øt qu·∫£ ph√¢n t√≠ch
    action_level = analysis_result.get('action_analysis', {}).get('action_level', 0)
    framing_quality = analysis_result.get('composition_analysis', {}).get('framing_quality', 'Unknown')
    athletes_count = analysis_result.get('detections', {}).get('athletes', 0)
    sport_type = analysis_result.get('composition_analysis', {}).get('sport_type', 'Unknown')

    # T√≠nh ƒëi·ªÉm sharpness trung b√¨nh n·∫øu c√≥
    avg_sharpness = 0
    if 'sports_analysis' in analysis_result and 'sharpness_scores' in analysis_result['sports_analysis']:
        sharpness_scores = analysis_result['sports_analysis']['sharpness_scores']
        if sharpness_scores:
            avg_sharpness = sum(sharpness_scores) / len(sharpness_scores)

    # Ki·ªÉm tra c√≥ emotion kh√¥ng
    has_emotion = analysis_result.get('facial_analysis', {}).get('has_faces', False)
    emotion_intensity = analysis_result.get('facial_analysis', {}).get('emotion_intensity', 0) if has_emotion else 0

    # ∆Øu ti√™n theo m·ª©c ƒë·ªô quan tr·ªçng
    # 1. Action level th·∫•p
    if action_level < 0.4:
        return "Try capturing during peak action moments for more dynamic sports photography."

    # 2. Framing k√©m
    if framing_quality in ['Poor', 'Could be improved', 'Fair']:
        return "Apply the rule of thirds and ensure subjects are well-positioned in the frame."

    # 3. Sharpness th·∫•p
    if avg_sharpness < 0.5:
        return "Use faster shutter speed and proper focus to achieve sharper subject details."

    # 4. Kh√¥ng c√≥ emotion
    if not has_emotion and athletes_count > 0:
        return "Consider angles that capture athlete expressions for more engaging storytelling."

    # 5. Emotion t·ªët
    if has_emotion and emotion_intensity > 0.7:
        return "Excellent emotional capture! This adds great storytelling value to your sports photo."

    # 6. Action t·ªët nh∆∞ng c√≥ th·ªÉ c·∫£i thi·ªán kh√°c
    if action_level > 0.7 and avg_sharpness > 0.6:
        return "Great action shot! Consider varying angles or including more context for visual variety."

    # 7. Suggestion chung theo m√¥n th·ªÉ thao
    if sport_type.lower() in ['soccer', 'football', 'basketball']:
        return "For team sports, try capturing player interactions and tactical moments."
    elif sport_type.lower() in ['tennis', 'golf', 'athletics']:
        return "Focus on technique and form - these sports offer great opportunities for skill showcase."

    # 8. Default suggestion
    return "Solid sports photography! Experiment with different perspectives to add creative flair."


def main():
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Sports Image Analysis')
    parser.add_argument('--image', type=str, help='Path to image file')
    args = parser.parse_args()

    # Check dependencies first
    print("Checking and installing dependencies...")
    check_dependencies()

    # Either use the provided image path or ask for one
    if args.image:
        image_path = args.image
    else:
        image_path = input("Please enter the path to the sports image: ")

    if not os.path.exists(image_path):
        print(f"Error: File {image_path} not found")
        return

    # Analyze image
    print(f"Analyzing image: {image_path}")
    analysis = analyze_sports_image(image_path)
    print("Analysis complete. Results saved to sports_analysis_results.png and analysis_results.txt")
    return analysis




if __name__ == "__main__":
    main()